{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import copy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from itertools import tee\n",
    "import wikipedia\n",
    "import pylcs\n",
    "import string\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATIONS = set(string.punctuation)\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*|\\(|\\)|-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362367\n"
     ]
    }
   ],
   "source": [
    "img_meta = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data/img_metadata-Copy1.json\", \"r\"))\n",
    "print(len(img_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25467\n"
     ]
    }
   ],
   "source": [
    "img_dataset = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/img_dataset_J_0623-Copy1.json\", \"r\"))\n",
    "print(len(img_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['NUM', 'NOUN', 'ADJ', 'PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(A, B):\n",
    "    intersection = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    return round(intersection / (union+1e-7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PUNCT', '\"'),\n",
       " ('DET', 'Which'),\n",
       " ('NOUN', 'road'),\n",
       " ('AUX', 'is'),\n",
       " ('ADJ', 'thinner'),\n",
       " ('PUNCT', ','),\n",
       " ('DET', 'the'),\n",
       " ('ADJ', 'Main'),\n",
       " ('NOUN', 'road'),\n",
       " ('PROPN', 'Shanischara'),\n",
       " ('PROPN', 'Temple'),\n",
       " ('PUNCT', ','),\n",
       " ('PROPN', 'Morena'),\n",
       " ('CCONJ', 'or'),\n",
       " ('DET', 'the'),\n",
       " ('ADJ', 'main'),\n",
       " ('NOUN', 'road'),\n",
       " ('ADP', 'of'),\n",
       " ('PROPN', 'Armenia'),\n",
       " ('PUNCT', ','),\n",
       " ('PROPN', 'Gagarin'),\n",
       " ('PUNCT', '?'),\n",
       " ('PUNCT', '\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.pos_, t.text) for t in nlp('\"Which road is thinner, the Main road Shanischara Temple, Morena or the main road of Armenia, Gagarin?\"')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_from_page_for_img_data(title, page, keywords, answerwords):\n",
    "    try: \n",
    "        content = wikipedia.page(title, auto_suggest=False, redirect=True).content\n",
    "        paragraphs = content[:content.find('== References ==')].split('\\n')\n",
    "        \n",
    "    except: return {}\n",
    "    #records = []\n",
    "    sen2score = {}\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            #records.append(-999)\n",
    "            doc = nlp(p)\n",
    "            for s in doc.sents:\n",
    "                if len(s) < 10: \n",
    "                    continue\n",
    "                nouns_in_s = [t.text for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A > 0.06:\n",
    "                    sen2score[s.text] = {'scores': (IoU_Q, IoU_A, IoU_Q - IoU_A), 'link': page, 'title': title}\n",
    "                #records.append(round(IoU_Q, 2))\n",
    "    #print(records)\n",
    "\n",
    "    #records = []\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            #records.append(-999)\n",
    "            doc = nlp(p)\n",
    "            it1, it2 = tee(doc.sents)\n",
    "            next(it2, None)\n",
    "            for s1, s2 in zip(it1, it2):\n",
    "                if len(s1) < 5 or len(s2) < 5 or len(s1)+len(s2) > 70 or len(s1)+len(s2) < 10: \n",
    "                    continue \n",
    "                nouns_in_s = [t.text for s in [s1, s2] for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A >= 0.06:\n",
    "                    sen2score[\" \".join([s1.text, s2.text])] = {'scores': (IoU_Q, IoU_A, IoU_Q - IoU_A), 'link': page, 'title': title}\n",
    "                    #print(s)\n",
    "                #records.append(round(IoU_Q, 2))\n",
    "    #print(records)\n",
    "    #print(len(sen2score))\n",
    "    return sen2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_img_sample(k):\n",
    "    Q = img_dataset[str(k)]['Q'].replace('\"', '')\n",
    "    doc = nlp(Q)\n",
    "    keywords = set([t.text for s in doc.sents for t in s if t.pos_ in ['NUM', 'PROPN', 'ADJ', 'NOUN'] or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    keywords = keywords - PUNCTUATIONS\n",
    "    \n",
    "    ### Extract noun chunks\n",
    "    keywords = [t.text for s in doc.sents for t in s if t.pos_ in ['NUM', 'PROPN', 'ADJ'] or ((not t.is_sent_start) and t.text[0].isupper())]\n",
    "    chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([n in keywords for n in chunk.text.split()]):\n",
    "            chunks.append(chunk.text)\n",
    "    if not chunks: chunks.extend([t.text for s in doc.sents for t in s if t.pos_ == 'PROPN' or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    \n",
    "    A = img_dataset[str(k)]['A'].replace('\"', '')\n",
    "    doc = nlp(A)\n",
    "    answerwords = set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    answerwords = answerwords - PUNCTUATIONS\n",
    "    \n",
    "    \n",
    "    return keywords, answerwords, Q, A, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0: 53,\n",
       "         1: 6,\n",
       "         2: 4,\n",
       "         3: 2,\n",
       "         4: 5,\n",
       "         5: 8,\n",
       "         6: 4,\n",
       "         7: 3,\n",
       "         8: 4,\n",
       "         9: 1,\n",
       "         10: 351,\n",
       "         11: 9,\n",
       "         12: 14,\n",
       "         13: 8,\n",
       "         14: 10,\n",
       "         15: 10,\n",
       "         16: 6,\n",
       "         17: 8,\n",
       "         18: 6,\n",
       "         19: 3,\n",
       "         20: 520,\n",
       "         21: 8,\n",
       "         22: 7,\n",
       "         23: 6,\n",
       "         24: 4,\n",
       "         25: 15,\n",
       "         26: 6,\n",
       "         27: 5,\n",
       "         28: 3,\n",
       "         29: 2,\n",
       "         30: 476,\n",
       "         31: 2,\n",
       "         32: 4,\n",
       "         33: 1,\n",
       "         34: 1,\n",
       "         35: 1,\n",
       "         36: 3,\n",
       "         37: 1,\n",
       "         38: 1,\n",
       "         39: 1,\n",
       "         40: 199,\n",
       "         41: 7,\n",
       "         43: 1,\n",
       "         44: 4,\n",
       "         45: 3,\n",
       "         47: 1,\n",
       "         48: 1,\n",
       "         49: 2,\n",
       "         50: 107,\n",
       "         51: 4,\n",
       "         52: 1,\n",
       "         57: 1,\n",
       "         58: 1,\n",
       "         59: 1,\n",
       "         60: 42,\n",
       "         63: 1,\n",
       "         64: 1,\n",
       "         68: 1,\n",
       "         70: 16,\n",
       "         72: 1,\n",
       "         74: 1,\n",
       "         80: 13,\n",
       "         90: 4,\n",
       "         100: 3,\n",
       "         110: 1,\n",
       "         114: 1})"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for k in random.sample(range(22467), 2000):\n",
    "    x.append(len(noun_chunk2candidate_page(k)))\n",
    "    if len(x)%200 == 0: print(len(x))\n",
    "Counter(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6c066fc050>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.sum(np.array(x) == 0) / len(x))\n",
    "sns.distplot(x, kde=False, bins=30)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given img_dataset indx & title, find sentences with word overlap with the question\n",
    "def find_sentences_from_indx_for_img(k, keywords, answerwords, chunks):\n",
    "    sen2score = {}\n",
    "    candidate_pages = noun_chunk2candidate_page(chunks)\n",
    "    print(\"num of candidate pages = {}\\n\".format(len(candidate_pages)))\n",
    "    for title in candidate_pages:\n",
    "        page = \"https://en.wikipedia.org/wiki/\" + \"_\".join(title.split())\n",
    "        sen2score.update(find_sentences_from_page_for_img_data(title, page, keywords, answerwords))\n",
    "    sen2score = dict(sorted(sen2score.items(), key=lambda x: x[1]['scores'][0], reverse=True))\n",
    "    return sen2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q =  What color did Croatia's goalkeeper wear in the 2018 FIFA World Cup Match 64, France v Croatia?\n",
      "Keywords = {'Cup', 'color', 'Match', 'World', 'France', 'wear', '2018', 'goalkeeper', 'Croatia', 'FIFA', '64'}\n",
      "Gold img: Croatia_WC2018_final - \n",
      "A =  \"Green\"\n",
      "answerwords = {'Green'}\n",
      " \n",
      "keywords used to find noun chunks:  ['Croatia', '2018', 'FIFA', 'World', 'Cup', 'Match', '64', 'France', 'Croatia']\n",
      "noun chunk :  What color\n",
      "noun chunk :  Croatia's goalkeeper wear\n",
      "noun chunk :  the 2018 FIFA World Cup Match\n",
      "noun chunk :  France\n",
      "noun chunk :  Croatia\n",
      "num of candidate pages = 30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France won the 2018 FIFA World Cup, defeating Croatia 4–2 in the final match on 15 July 2018.\n",
      "{'scores': (0.37, 0.0, 0.37), 'link': 'https://en.wikipedia.org/wiki/France_national_football_team', 'title': 'France national football team'}\n",
      " \n",
      "In 2018, France defeated Croatia 4–2 in the final match and won the World Cup for the second time.\n",
      "{'scores': (0.31, 0.0, 0.31), 'link': 'https://en.wikipedia.org/wiki/France_national_football_team', 'title': 'France national football team'}\n",
      " \n",
      "Before 2018, France's only World Cup victory was in 1998 – though they had also reached the final in 2006 – while Croatia were playing in their first World Cup final.\n",
      "{'scores': (0.29, 0.0, 0.29), 'link': 'https://en.wikipedia.org/wiki/2018_FIFA_World_Cup_Final', 'title': '2018 FIFA World Cup Final'}\n",
      " \n",
      "The winners of each group qualified for the 2018 FIFA World Cup.\n",
      "{'scores': (0.29, 0.0, 0.29), 'link': 'https://en.wikipedia.org/wiki/2018_FIFA_World_Cup_qualification', 'title': '2018 FIFA World Cup qualification'}\n",
      " \n",
      "The four winners qualified for the 2018 FIFA World Cup.\n",
      "{'scores': (0.29, 0.0, 0.29), 'link': 'https://en.wikipedia.org/wiki/2018_FIFA_World_Cup_qualification', 'title': '2018 FIFA World Cup qualification'}\n",
      " \n",
      "Afterwards, it was renovated in preparation for the 2017 FIFA Confederations Cup and the 2018 World Cup.\n",
      "{'scores': (0.29, 0.0, 0.29), 'link': 'https://en.wikipedia.org/wiki/2018_FIFA_World_Cup', 'title': '2018 FIFA World Cup'}\n",
      " \n",
      "The following article concerns the performance of Brazil at the 2018 FIFA World Cup.\n",
      "{'scores': (0.29, 0.0, 0.29), 'link': 'https://en.wikipedia.org/wiki/Brazil_at_the_2018_FIFA_World_Cup', 'title': 'Brazil at the 2018 FIFA World Cup'}\n",
      " \n",
      "In July 1930, France appeared in the inaugural FIFA World Cup, held in Uruguay.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/France_national_football_team', 'title': 'France national football team'}\n",
      " \n",
      "France won the 2018 FIFA World Cup, defeating Croatia 4–2 in the final match on 15 July 2018. This was the second time they had won the tournament after winning it on home soil in 1998.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/France_national_football_team', 'title': 'France national football team'}\n",
      " \n",
      "Croatia was undefeated for the first round of 2018 World Cup qualification matches.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/Croatia_national_football_team', 'title': 'Croatia national football team'}\n",
      " \n",
      " ------------------------------------------------------------ \n",
      "\n",
      "Q =  Is the dog park in Piedmont Park surrounded by trees and greenery?\n",
      "Keywords = {'park', 'Piedmont', 'greenery', 'Park', 'trees', 'dog'}\n",
      "Gold img: Dog_park_in_Piedmont_Park.JPG - \n",
      "A =  \"yes\"\n",
      "answerwords = set()\n",
      " \n",
      "keywords used to find noun chunks:  ['Piedmont', 'Park']\n",
      "noun chunk :  the dog park\n",
      "noun chunk :  Piedmont Park\n",
      "noun chunk :  trees\n",
      "noun chunk :  greenery\n",
      "num of candidate pages = 10\n",
      "\n",
      "The city bought the park for $98,000 in 1904, incorporating Piedmont Park into Atlanta's city limits.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "Atlanta was a rapidly growing city in the years before Piedmont Park.\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "Over $2 million was spent on the transformation of Piedmont Park.\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "=== Friends of Piedmont Park v. The Piedmont Park Conservancy =\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/Non-profit_organizations_and_access_to_public_information', 'title': 'Non-profit organizations and access to public information'}\n",
      " \n",
      "The Piedmont Park Conservancy (a nonprofit) was responsible for the restoration of the Piedmont Park Dog Parks, and still manages the dog parks today.\n",
      "{'scores': (0.21, 0.0, 0.21), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "Piedmont Park has picnic shelters located just to the East of the north soccer field. There are also various picnic tables and benches throughout the park.\n",
      "{'scores': (0.21, 0.0, 0.21), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "Piedmont Park is a popular place for organized sports.\n",
      "{'scores': (0.2, 0.0, 0.2), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "The Conservancy was founded in 1989 to revitalize the rapidly deteriorating park. Since then, it has raised and invested $66 million in the restoration and enhancement of Piedmont Park and making it, once again, the most visited green space in Atlanta.\n",
      "{'scores': (0.2, 0.0, 0.2), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "The first grill in Piedmont Park was erected for the 1895 Cotton States Exposition where the administrative offices now sit. There are 22 grills throughout the park.\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Piedmont_Park', 'title': 'Piedmont Park'}\n",
      " \n",
      "The Piedmont Park Conservancy is a private non-profit that oversees and manages Piedmont Park.\n",
      "{'scores': (0.18, 0.0, 0.18), 'link': 'https://en.wikipedia.org/wiki/Non-profit_organizations_and_access_to_public_information', 'title': 'Non-profit organizations and access to public information'}\n",
      " \n",
      " ------------------------------------------------------------ \n",
      "\n",
      "Q =  Do the Starbucks Coffee at Penn Station in New York and at the 81st Street Theatre have the same color font in their signage?\n",
      "Keywords = {'color', 'New', 'Penn', 'font', 'Street', '81st', 'same', 'Starbucks', 'Coffee', 'York', 'signage', 'Theatre', 'Station'}\n",
      "Gold img: RKO_81st_Street_Theatre_jeh - \n",
      "Gold img: Starbucks_Coffee_Penn_Station_New_York_ - \n",
      "A =  \"Yes\"\n",
      "answerwords = {'Yes'}\n",
      " \n",
      "keywords used to find noun chunks:  ['Starbucks', 'Coffee', 'Penn', 'Station', 'New', 'York', '81st', 'Street', 'Theatre', 'same']\n",
      "noun chunk :  the Starbucks Coffee\n",
      "noun chunk :  Penn Station\n",
      "noun chunk :  New York\n",
      "noun chunk :  the 81st Street Theatre\n",
      "noun chunk :  the same color font\n",
      "noun chunk :  their signage\n",
      "num of candidate pages = 50\n",
      "\n",
      "Occasionally used by Amtrak trains to New York Penn Station.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(Newark)', 'title': 'Pennsylvania Station (Newark)'}\n",
      " \n",
      "New York Penn Station was officially declared complete on August 29, 1910.\n",
      "{'scores': (0.24, 0.0, 0.24), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(1910–1963)', 'title': 'Pennsylvania Station (1910–1963)'}\n",
      " \n",
      "34th Street–Penn Station is an express station on the IRT Broadway–Seventh Avenue Line of the New York City Subway.\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/34th_Street–Penn_Station_(IRT_Broadway–Seventh_Avenue_Line)', 'title': '34th Street–Penn Station (IRT Broadway–Seventh Avenue Line)'}\n",
      " \n",
      "Philadelphia's Pennsylvania Station-30th Street became Penn Central Station-30th Street, while Baltimore's Pennsylvania Station, Michigan Central Station in Detroit, Michigan, New York Central Railroad's (NYC) Buffalo Central Station, and Pittsburgh's Pennsylvania Station became simply Penn Central Station.\n",
      "{'scores': (0.21, 0.0, 0.21), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station', 'title': 'Pennsylvania Station'}\n",
      " \n",
      "As the terminal shared its name with several stations in other cities, it was sometimes called New York Pennsylvania Station, or Penn Station for short.\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(1910–1963)', 'title': 'Pennsylvania Station (1910–1963)'}\n",
      " \n",
      "At the time of Penn Station's completion, The New York Times called it \"the largest building in the world ever built at one time\".\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(1910–1963)', 'title': 'Pennsylvania Station (1910–1963)'}\n",
      " \n",
      "The project was to include New York Penn Station; the North River Tunnels, crossing the Hudson River to the west; and the East River Tunnels, crossing the East River to the east.\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(1910–1963)', 'title': 'Pennsylvania Station (1910–1963)'}\n",
      " \n",
      "The Pennsylvania Railroad merged with longtime rival New York Central Railroad in 1968 to form Penn Central Railroad, but Newark kept the name \"Penn Station.\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(Newark)', 'title': 'Pennsylvania Station (Newark)'}\n",
      " \n",
      "Track 1 is normally used by New Jersey Transit trains to New York Penn Station and is served by an island platform shared with Track M.\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Pennsylvania_Station_(Newark)', 'title': 'Pennsylvania Station (Newark)'}\n",
      " \n",
      "Penn Station Access is a public works project planned by the Metropolitan Transportation Authority in New York City.\n",
      "{'scores': (0.19, 0.0, 0.19), 'link': 'https://en.wikipedia.org/wiki/Penn_Station_Access', 'title': 'Penn Station Access'}\n",
      " \n",
      " ------------------------------------------------------------ \n",
      "\n",
      "Q =  Do both the floor and the dome interior of San Francisco City Hall reflect light?\n",
      "Keywords = {'City', 'light', 'Francisco', 'interior', 'San', 'Hall', 'dome', 'floor'}\n",
      "Gold img: San_Francisco_City_Hall_people_and_dome.JPG - \n",
      "A =  \"Yes\"\n",
      "answerwords = {'Yes'}\n",
      " \n",
      "keywords used to find noun chunks:  ['San', 'Francisco', 'City', 'Hall']\n",
      "noun chunk :  both the floor\n",
      "noun chunk :  the dome interior\n",
      "noun chunk :  San Francisco City Hall\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun chunk :  light\n",
      "num of candidate pages = 10\n",
      "\n",
      "San Francisco City Hall is the seat of government for the City and County of San Francisco, California.\n",
      "{'scores': (0.33, 0.0, 0.33), 'link': 'https://en.wikipedia.org/wiki/San_Francisco_City_Hall', 'title': 'San Francisco City Hall'}\n",
      " \n",
      "The beauty of City Hall has not been lost on filmmakers working in San Francisco; many films have shot scenes in and around the building.\n",
      "{'scores': (0.29, 0.0, 0.29), 'link': 'https://en.wikipedia.org/wiki/San_Francisco_City_Hall', 'title': 'San Francisco City Hall'}\n",
      " \n",
      "The flag of the City and County of San Francisco is a flag representing San Francisco, California.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/Flag_of_San_Francisco', 'title': 'Flag of San Francisco'}\n",
      " \n",
      "San Francisco International Airport, though located in San Mateo County, is owned and operated by the City and County of San Francisco.\n",
      "{'scores': (0.25, 0.0, 0.25), 'link': 'https://en.wikipedia.org/wiki/San_Francisco', 'title': 'San Francisco'}\n",
      " \n",
      "In February 1948, the name was changed to City College of San Francisco.\n",
      "{'scores': (0.25, 0.0, 0.25), 'link': 'https://en.wikipedia.org/wiki/City_College_of_San_Francisco', 'title': 'City College of San Francisco'}\n",
      " \n",
      "The City of San Francisco has detailed maps of each district available on its website.\n",
      "{'scores': (0.23, 0.0, 0.23), 'link': 'https://en.wikipedia.org/wiki/San_Francisco_Board_of_Supervisors', 'title': 'San Francisco Board of Supervisors'}\n",
      " \n",
      "Belying the name, South San Francisco does not touch on San Francisco, with either Brisbane, Colma, or Daly City lying between them.\n",
      "{'scores': (0.23, 0.0, 0.23), 'link': 'https://en.wikipedia.org/wiki/South_San_Francisco,_California', 'title': 'South San Francisco, California'}\n",
      " \n",
      "City College of San Francisco is located in an urban environment and has the associated crime rate.\n",
      "{'scores': (0.23, 0.0, 0.23), 'link': 'https://en.wikipedia.org/wiki/City_College_of_San_Francisco', 'title': 'City College of San Francisco'}\n",
      " \n",
      "City Hall is located at 400 Grand Avenue (37.656°N 122.413°W﻿ / 37.656; -122.413﻿ (South San Francisco City Hall)).\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/South_San_Francisco,_California', 'title': 'South San Francisco, California'}\n",
      " \n",
      "Colloquial nicknames for San Francisco include SF,  San Fran, The City, and Frisco.\n",
      "{'scores': (0.21, 0.0, 0.21), 'link': 'https://en.wikipedia.org/wiki/San_Francisco', 'title': 'San Francisco'}\n",
      " \n",
      " ------------------------------------------------------------ \n",
      "\n",
      "Q =  Are the ornaments of the Temple Church Organ in Birmingham, UK golden or silver colored?\n",
      "Keywords = {'Church', 'Organ', 'Birmingham', 'golden', 'UK', 'silver', 'ornaments', 'Temple'}\n",
      "Gold img: Temple_Church_Organ_(14142918741) - \n",
      "A =  \"Golden\"\n",
      "answerwords = {'Golden'}\n",
      " \n",
      "keywords used to find noun chunks:  ['Temple', 'Church', 'Organ', 'Birmingham', 'UK', 'golden']\n",
      "noun chunk :  the ornaments\n",
      "noun chunk :  the Temple Church Organ\n",
      "noun chunk :  Birmingham\n",
      "noun chunk :  UK\n",
      "num of candidate pages = 30\n",
      "\n",
      "The area around the Temple Church is known as the Temple.\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "Birmingham is the fourth-most visited city in the UK by foreign visitors.\n",
      "{'scores': (0.18, 0.0, 0.18), 'link': 'https://en.wikipedia.org/wiki/Birmingham', 'title': 'Birmingham'}\n",
      " \n",
      "The Temple Church serves both the Inner Temple and the Middle Temple as a private chapel.\n",
      "{'scores': (0.17, 0.0, 0.17), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "Temple Church houses one of the most magnificent organs in the world.\"\n",
      "{'scores': (0.17, 0.0, 0.17), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "The choir continues to record, broadcast and perform, in addition to its regular services at the Temple Church.\n",
      "{'scores': (0.17, 0.0, 0.17), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "Birmingham became the first UK university to offer a sports degree.\n",
      "{'scores': (0.17, 0.0, 0.17), 'link': 'https://en.wikipedia.org/wiki/University_of_Birmingham', 'title': 'University of Birmingham'}\n",
      " \n",
      "Zimmer is quoted saying that \"Setting foot into Temple Church is like stepping into profound history. ...\n",
      "{'scores': (0.15, 0.0, 0.15), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "The Temple Church has always been a Peculiar (but not a Royal Peculiar), due to which the choristers have the privilege of wearing scarlet cassocks.\n",
      "{'scores': (0.14, 0.0, 0.14), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "Relations with the Bishop of London are very good and she regularly attends events and services at the Temple Church.\n",
      "{'scores': (0.14, 0.0, 0.14), 'link': 'https://en.wikipedia.org/wiki/Temple_Church', 'title': 'Temple Church'}\n",
      " \n",
      "The original temple site proper is now owned by the Church of Christ (Temple Lot).\n",
      "{'scores': (0.14, 0.0, 0.14), 'link': 'https://en.wikipedia.org/wiki/Independence_Temple', 'title': 'Independence Temple'}\n",
      " \n",
      " ------------------------------------------------------------ \n",
      "\n",
      "Q =  Where is the sign for the Galeries Lafayette department store located at on the front of the building?\n",
      "Keywords = {'Galeries', 'department', 'Lafayette', 'sign', 'store', 'front', 'building'}\n",
      "Gold img: Galeries_lafayette_xmas_night - \n",
      "A =  \"near the top\"\n",
      "answerwords = {'top'}\n",
      " \n",
      "keywords used to find noun chunks:  ['Galeries', 'Lafayette']\n",
      "noun chunk :  the sign\n",
      "noun chunk :  the Galeries Lafayette department store\n",
      "noun chunk :  the front\n",
      "noun chunk :  the building\n",
      "num of candidate pages = 10\n",
      "\n",
      "He also contributed to the reconstruction of the Galeries Lafayette department store in 1932–1836.\n",
      "{'scores': (0.44, 0.0, 0.44), 'link': 'https://en.wikipedia.org/wiki/Art_Deco_in_Paris', 'title': 'Art Deco in Paris'}\n",
      " \n",
      "Galeries Lafayette Berlin – Berlin branch of the French department store\n",
      "{'scores': (0.4, 0.0, 0.4), 'link': 'https://en.wikipedia.org/wiki/List_of_department_stores_by_country', 'title': 'List of department stores by country'}\n",
      " \n",
      "The Galeries Lafayette  (French pronunciation: ​[ɡalʁi lafajɛt]) is an upmarket French department store chain.\n",
      "{'scores': (0.31, 0.0, 0.31), 'link': 'https://en.wikipedia.org/wiki/Galeries_Lafayette', 'title': 'Galeries Lafayette'}\n",
      " \n",
      "Galeries Lafayette previously operated a store in Casablanca from the 1920s through the early 1970s.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/Galeries_Lafayette', 'title': 'Galeries Lafayette'}\n",
      " \n",
      "Maurice Dufrêne (1876–1955) was a French decorative artist who headed the  Maîtrise workshop of the Galeries Lafayette department store.\n",
      "{'scores': (0.27, 0.0, 0.27), 'link': 'https://en.wikipedia.org/wiki/Maurice_Dufrêne', 'title': 'Maurice Dufrêne'}\n",
      " \n",
      "The most famous example is the Galeries Lafayette department store on Boulevard Haussmann, built by architect Georges Chedanne and his pupil Ferdinand Chanut.\n",
      "{'scores': (0.24, 0.0, 0.24), 'link': 'https://en.wikipedia.org/wiki/Art_Nouveau_in_Paris', 'title': 'Art Nouveau in Paris'}\n",
      " \n",
      "The kimono store changed to a department store in 1910.\n",
      "{'scores': (0.22, 0.0, 0.22), 'link': 'https://en.wikipedia.org/wiki/Department_store', 'title': 'Department store'}\n",
      " \n",
      "Dubai, UAE - A Galeries Lafayette store opened in Dubai Mall on 18 May 2009.\n",
      "{'scores': (0.21, 0.0, 0.21), 'link': 'https://en.wikipedia.org/wiki/Galeries_Lafayette', 'title': 'Galeries Lafayette'}\n",
      " \n",
      "The most famous example is the Galeries Lafayette department store on Boulevard Haussmann, built by architect Georges Chedanne and his pupil Ferdinand Chanut. The building was begun in 1895, and the central dome and Art Nouveau staircases accessing it we're completed in 1912.\n",
      "{'scores': (0.21, 0.0, 0.21), 'link': 'https://en.wikipedia.org/wiki/Art_Nouveau_in_Paris', 'title': 'Art Nouveau in Paris'}\n",
      " \n",
      "It is a part of the company Groupe Galeries Lafayette and has been a member of the International Association of department stores since 1960.\n",
      "{'scores': (0.2, 0.0, 0.2), 'link': 'https://en.wikipedia.org/wiki/Galeries_Lafayette', 'title': 'Galeries Lafayette'}\n",
      " \n",
      " ------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [345, 678, 456, 1297, 9099, 4738]:\n",
    "    d, word_lists = find_sentences_from_indx_for_img(k)\n",
    "    for k in list(d.keys())[:10]:\n",
    "    \n",
    "        print(k)\n",
    "        print(d[k])\n",
    "        print(' ')\n",
    "    print(' ------------------------------------------------------------ \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_html_row_x_distractor_for_img(k, sen2score, word_lists, chunks, colors=[\"(205, 245, 252)\", \"(255, 214, 222)\"]):\n",
    "    html = '<tr><td>{}.</td>'.format(k)\n",
    "    Q = img_dataset[str(k)]['Q'].replace('\"', '')\n",
    "    html += '<td>Q: {}<br><br>'.format(highlight_words(word_lists, chunks, colors, Q))\n",
    "    A = img_dataset[str(k)]['A'].replace('\"', '')\n",
    "    for gid in img_dataset[str(k)]['GoldIds']:\n",
    "        img = img_meta[str(int(gid))]\n",
    "        html += '<a href=\"{}\" target=\"_blank\"><img style=\"display:block; max-height:300px; max-width:100%;\" src = \"{}\"></a>'.format(img['page'], img['src'])\n",
    "        html += '<br>Title = {}<br>Description = {}<br><br>'.format(highlight_words(word_lists, [], colors, img['name'].replace(\"_\", \" \")), highlight_words(word_lists, [], colors, img['description'].replace(\"_\", \" \")))\n",
    "    html += 'A: {}<br>'.format(highlight_words(word_lists, [], colors, A))\n",
    "    html += '</td><td>'\n",
    "    \n",
    "    for s in list(sen2score.keys())[:10]:\n",
    "        html += '{} --- {} '.format(highlight_words(word_lists, [], colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"  target=\"_blank\"> {}</a><br><br>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "    \n",
    "    html += '</td></tr>'\n",
    "    html += '<tr><td colspan=3><hr></td></tr>'\n",
    "    return html.encode('ascii', 'xmlcharrefreplace').decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21090\n",
      "Q =  Are the men in Vincent Van Gogh's paintings Self-portrait (1887) and The Smoker (Le Fumeur) (1888) wearing hats?\n",
      "Keywords = ['Vincent', 'Van', 'Gogh', 'Self', '1887', 'The', 'Smoker', 'Le', 'Fumeur', '1888']\n",
      "A =  Yes\n",
      "answerwords = set()\n",
      "\n",
      "Noun chunks:  [\"Vincent Van Gogh's paintings\", 'The Smoker', '(Le Fumeur']\n",
      " \n",
      "num of candidate pages = 30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "html = '<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">'\n",
    "html += '<script src=\"https://code.jquery.com/jquery-3.2.1.min.js\" integrity=\"sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=\" crossorigin=\"anonymous\"></script>'\n",
    "html += '<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>'\n",
    "html += '<!DOCTYPE html><html><head><meta http-equiv=\"content-type\" content=\"text/html; chatset=\"UTF-8\"><body>'\n",
    "html += '<script>$(\"img\").on(\"error\", function(){console.log($(this).attr(\"src\"));});</script>'\n",
    "html += '<style>table {border-collapse: separate;border-spacing: 10px;}\\n'\n",
    "html += '.chunk {text-decoration: underline solid rgb(227, 123, 253) 3px;}</style>'\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=lightblue style=\"text-align: center;\"><td width=5%>Index</td><td width=35%>Q & Pos Facts</td><td width=60%>Neg Facts</td></tr>'\n",
    "x = []\n",
    "for k in random.sample(range(25467), 20):\n",
    "    print(k)\n",
    "    keywords, answerwords, Q, A, chunks = get_keywords_from_img_sample(k)\n",
    "    print(\"Q = \", Q)\n",
    "    print(\"Keywords = {}\".format(keywords))\n",
    "    print(\"A = \", A)\n",
    "    print(\"answerwords = {}\".format(answerwords))\n",
    "    print(\"\\nNoun chunks: \", chunks)\n",
    "    print(' ')\n",
    "    d = find_sentences_from_indx_for_img(k, keywords, answerwords, chunks)\n",
    "    x.append(len(d))\n",
    "    \n",
    "    word_lists = [keywords, answerwords]\n",
    "    html += add_html_row_x_distractor_for_img(k, d, word_lists, chunks, colors=[\"(193, 239, 253)\", \"(255, 214, 222)\"])\n",
    "    o = open('x_distractor_for_img_demo.html', 'wt')\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_img_demo.html', 'wt')\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID-19 pandemic in Nevada',\n",
       " 'Curative (company)',\n",
       " 'Calvin Ball III',\n",
       " 'Panasonic',\n",
       " 'Value-added tax',\n",
       " 'COVID-19 pandemic in Turkey',\n",
       " 'Toronto Transit Commission incidents',\n",
       " 'Shrikant Shinde',\n",
       " 'Beetle',\n",
       " 'List of waste management acronyms']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search('the PPE vending machines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words(word_lists, chunks, colors, sentence):\n",
    "    s = copy.deepcopy(sentence)\n",
    "    if \"\".join(chunks):\n",
    "        s = re.sub(r'\\s*(' + r'|'.join([re.escape(c) for c in chunks]) + r')\\s*', lambda m: '<span class=\"chunk\">{}</span>'.format(m.group()), s)\n",
    "    for word_list, color in zip(word_lists, colors):\n",
    "        if \"\".join(word_list): s = re.sub(r'\\b(' + r'|'.join(word_list) + r')\\b', lambda m: '<span style=\"background-color:rgb{}\">{}</span>'.format(color, m.group()), s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunk2candidate_page(chunks):\n",
    "    pages = []\n",
    "    for chunk in chunks:\n",
    "        pages.extend(wikipedia.search(chunk))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "Does the T & T Supermarket in Richmond have a fire extinguisher that is at the front entrance to the left of large, red Chinese letters on a silver wall?\n",
      "['T', 'T', 'Supermarket', 'Richmond', 'front', 'large', 'red', 'Chinese', 'silver']\n",
      "noun_chunk:  the T & T Supermarket\n",
      "['T & T Supermarket', 'T&T', '99 Ranch Market', 'History of AT&T', 'T. R. Knight', 'Supermarket', 'AT&T Center', 'List of supermarket chains in Canada', 'Food City (K-VA-T)', 'Asian supermarket']\n",
      "noun_chunk:  Richmond\n",
      "['Richmond', 'Richmond, Virginia', 'Duke of Richmond', 'Richmond, California', 'Cedric Richmond', 'Jeff Richmond', 'Branscombe Richmond', 'Richmond Football Club', 'Richmond, London', 'Lady Margaret Beaufort']\n",
      "noun_chunk:  the front entrance\n",
      "['Entryway', 'Entrance pupil', 'AEC Routemaster', 'Ross Castle', 'Porch', 'Dulce et decorum est pro patria mori', '5, The Grove', 'Wardour Castle', 'Parliament Building (Quebec)', 'Cornelius Vanderbilt II House']\n",
      "noun_chunk:  large, red Chinese letters\n",
      "['Vermilion', 'Traditional Chinese marriage', 'Seal (East Asia)', 'Chinese characters', 'Quotations from Chairman Mao Tse-tung', 'Cultural Revolution', 'Pinyin', 'Mahjong tiles', 'Red fox', 'Chinese New Year']\n",
      "noun_chunk:  a silver wall\n",
      "['Silver', 'The Silver Scream', 'WALL-E', 'Silver Lake (investment firm)', 'Silver Surfer', 'American Silver Eagle', 'Louisa Wall', 'Walls of Constantinople', 'Fantastic Four: Rise of the Silver Surfer', 'Ice Nine Kills']\n",
      " ------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in random.sample(list(img_dataset.keys()), 1):\n",
    "    k=123\n",
    "    print(k)\n",
    "    Q = img_dataset[str(k)]['Q'].replace('\"', '')\n",
    "    print(Q)\n",
    "    doc = nlp(Q)\n",
    "    keywords = [t.text for s in doc.sents for t in s if t.pos_ in ['NUM', 'PROPN', 'ADJ'] or ((not t.is_sent_start) and t.text[0].isupper())]\n",
    "    print(keywords)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([n in keywords for n in chunk.text.split()]):\n",
    "            print(\"noun_chunk: \", chunk.text)\n",
    "            print(wikipedia.search(chunk.text))\n",
    "    print(' ------------------------------------------- \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntemp = {}\\ncount = 0\\nfor i in new_txt_data:\\n    temp[count] = copy.deepcopy(i)\\n    temp[count][\\'Q\\'] = temp[count][\\'Question\\']\\n    temp[count][\\'A\\'] = temp[count][\\'Answer\\']\\n    del temp[count][\\'Question\\']\\n    del temp[count][\\'Answer\\']\\n    count += 1\\njson.dump(temp, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/output_mine_all_schema.json\", \"w\"), indent=4)\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_txt_data = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/output_mine_all_schema.json\", \"r\"))\n",
    "print(len(new_txt_data))\n",
    "'''\n",
    "temp = {}\n",
    "count = 0\n",
    "for i in new_txt_data:\n",
    "    temp[count] = copy.deepcopy(i)\n",
    "    temp[count]['Q'] = temp[count]['Question']\n",
    "    temp[count]['A'] = temp[count]['Answer']\n",
    "    del temp[count]['Question']\n",
    "    del temp[count]['Answer']\n",
    "    count += 1\n",
    "json.dump(temp, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/output_mine_all_schema.json\", \"w\"), indent=4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2keywords(k):\n",
    "    Q = new_txt_data[str(k)]['Q']\n",
    "    doc = nlp(Q)\n",
    "    keywords = set([t.text for s in doc.sents for t in s if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "\n",
    "    keywords = keywords - PUNCTUATIONS        \n",
    "\n",
    "    A = new_txt_data[str(k)]['A']\n",
    "    doc = nlp(A)\n",
    "    answerwords = set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    answerwords = answerwords - PUNCTUATIONS\n",
    "\n",
    "    goldfactwords = set()\n",
    "    titlewords = set()\n",
    "    Q_A_words = keywords.union(answerwords)\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        doc = nlp(f['fact'])\n",
    "        goldfactwords = goldfactwords.union(set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())]) - Q_A_words)\n",
    "        fact_title_raw = ' '.join(urllib.parse.unquote(f['url']).split('/')[-1].split('_'))\n",
    "        fact_title = pattern.sub('', fact_title_raw)\n",
    "        titlewords = titlewords.union(fact_title.split())\n",
    "    goldfactwords = goldfactwords - PUNCTUATIONS \n",
    "    titlewords = titlewords - PUNCTUATIONS\n",
    "\n",
    "    word_lists = [titlewords, keywords, goldfactwords, answerwords]\n",
    "    return word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_html_row_vis_ori_dataset(k, word_lists, colors = [\"(223, 255, 238)\", \"(193, 239, 253)\", \"(253, 252, 152)\", \"(255, 214, 222)\"]):\n",
    "    html = \"\"\n",
    "    html += '<tr><td>{}.</td><td>Q: {}<br>'.format(k, highlight_words(word_lists, colors, new_txt_data[str(k)]['Q']))\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        html += '<br><br>&nbsp;&nbsp;{}'.format(highlight_words(word_lists, colors, f['fact']))\n",
    "        html += '<a href=\"{}\"> link</a>'.format(f['url'])\n",
    "    html += '<br><br>A: {}<br>'.format(highlight_words(word_lists, colors, new_txt_data[str(k)]['A']))\n",
    "    html += '</td><td>'\n",
    "    \n",
    "    for f in new_txt_data[str(k)]['DistractorFacts']:            \n",
    "            html += highlight_words(word_lists, colors, f['fact'])\n",
    "            html += '<a href=\"{}\"> {}</a><br><br>'.format(f['url'], f['title'])\n",
    "    html += '</td></tr>'\n",
    "    html += '<tr><td colspan=3><hr></td></tr>'\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indx = random.sample(range(7921), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"<html><body>\"\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=gray><td width=5%>Index</td><td width=35%>Q & Pos Facts</td><td width=60%>Neg Facts</td></tr>'\n",
    "count = 0\n",
    "for k in sampled_indx:\n",
    "    count += 1\n",
    "    word_lists = sample2keywords(k)[:2]\n",
    "    html += add_html_row_vis_ori_dataset(k, word_lists)\n",
    "    o = open('txt_dataset_vis.html', 'wt')\n",
    "\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('txt_dataset_vis.html', 'wt')\n",
    "\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What  <span style=\"background-color:rgb(193, 239, 253)\">kind </span> of  <span style=\"background-color:rgb(193, 239, 253)\">effect </span> are  <span style=\"background-color:rgb(193, 239, 253)\">artificial </span>  <span style=\"background-color:rgb(193, 239, 253)\">harmonics </span> and  <span style=\"background-color:rgb(193, 239, 253)\">vibrato </span> ?'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [\"(223, 255, 238)\", \"(193, 239, 253)\", \"(253, 252, 152)\", \"(255, 214, 222)\"]\n",
    "highlight_words(sample2keywords(3816), colors, 'What kind of effect are artificial harmonics and vibrato ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRON', 'What'),\n",
       " ('NOUN', 'kind'),\n",
       " ('ADP', 'of'),\n",
       " ('NOUN', 'effect'),\n",
       " ('AUX', 'are'),\n",
       " ('ADJ', 'artificial'),\n",
       " ('NOUN', 'harmonics'),\n",
       " ('CCONJ', 'and'),\n",
       " ('NOUN', 'vibrato'),\n",
       " ('PUNCT', '?')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.pos_, t.text) for t in nlp('What kind of effect are artificial harmonics and vibrato ?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Csárdás', 'Monti', 'Vibrato'},\n",
       " {'artificial', 'effect', 'harmonics', 'kind', 'vibrato'},\n",
       " {'24',\n",
       "  '5',\n",
       "  'Czardas',\n",
       "  'Italian',\n",
       "  'Meno',\n",
       "  'Monti',\n",
       "  'Vibrato',\n",
       "  'change',\n",
       "  'e',\n",
       "  'expression',\n",
       "  'finger',\n",
       "  'help',\n",
       "  'higher',\n",
       "  'instrumental',\n",
       "  'lento',\n",
       "  'media',\n",
       "  'music',\n",
       "  'musical',\n",
       "  'note',\n",
       "  'octaves',\n",
       "  'participle',\n",
       "  'past',\n",
       "  'pianoforte',\n",
       "  'pitch',\n",
       "  'quasi',\n",
       "  'regular',\n",
       "  'section',\n",
       "  'semitones',\n",
       "  'string',\n",
       "  'two',\n",
       "  'vibrare',\n",
       "  'violin',\n",
       "  'violinist',\n",
       "  'violino',\n",
       "  'vocal'},\n",
       " {'Musical', 'effects'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2keywords(3816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
