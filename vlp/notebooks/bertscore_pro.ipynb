{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json, time, copy\n",
    "import math\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from word2number import w2n\n",
    "import string, re\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\",\"textcat\",\"parser\"])\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/yingshac/CYS/WebQnA/VLP/BARTScore\")\n",
    "from bart_score import BARTScorer\n",
    "#bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_scorer_ParaBank = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "bart_scorer_ParaBank.load(path='/home/yingshac/CYS/WebQnA/VLP/BARTScore/bart.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "def normalize_text_for_bart(x):\n",
    "    return \" \".join(x.translate(TABLE).split())\n",
    "#def compute_bartscore(c, a, switch=False):\n",
    "    #if switch: score = np.exp(bart_scorer.score(c, a))\n",
    "    #else: score = np.exp(bart_scorer.score(a, c))\n",
    "    #return score\n",
    "def compute_bartscore_ParaBank(c, a, switch=False):\n",
    "    c_removepunc = [normalize_text_for_bart(x) for x in c]\n",
    "    a_removepunc = [normalize_text_for_bart(x) for x in a]\n",
    "    if switch: score = np.exp(bart_scorer_ParaBank.score(c_removepunc, a_removepunc))\n",
    "    else: score = np.exp(bart_scorer_ParaBank.score(a_removepunc, c_removepunc))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectNum(l):\n",
    "    result = []\n",
    "    for w in l:\n",
    "        try: result.append(str(int(w)))\n",
    "        except: pass\n",
    "    return result\n",
    "def toNum(word):\n",
    "    if word == 'point': return word\n",
    "    try: return w2n.word_to_num(word)\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text): # additional: converting numbers to digit form\n",
    "        return \" \".join([str(toNum(w)) for w in text.split()])\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation) - set(['.'])\n",
    "        text1 = \"\".join(ch for ch in text if ch not in exclude)\n",
    "        return re.sub(r\"\\.(?!\\d)\", \"\", text1) # remove '.' if it's not a decimal point\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def lemmatization(text):\n",
    "        return \" \".join([token.lemma_ for token in nlp(text)])\n",
    "\n",
    "    if len(s.strip()) == 1:\n",
    "        # accept article and punc if input is a single char\n",
    "        return white_space_fix(lower(s))\n",
    "    elif len(s.strip().split()) == 1: \n",
    "        # accept article if input is a single word\n",
    "        return lemmatization(white_space_fix(remove_punc(lower(s))))\n",
    "\n",
    "    return lemmatization(white_space_fix(remove_articles(remove_punc(lower(s)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA Eval (SQuAD style EM, F1)\n",
    "def compute_vqa_metrics(cands, a, exclude=\"\", domain=None):\n",
    "    if len(cands) == 0: return (0,0,0)\n",
    "    bow_a = normalize_text(a).split()\n",
    "    F1 = []\n",
    "    EM = 0\n",
    "    RE = []\n",
    "    PR = []\n",
    "    e = normalize_text(exclude).split()\n",
    "    for c in cands:\n",
    "        bow_c = [w for w in normalize_text(c).split() if not w in e]\n",
    "        if domain == {\"NUMBER\"}: bow_c = detectNum(bow_c)\n",
    "        elif domain is not None: \n",
    "            bow_c = list(domain.intersection(bow_c))\n",
    "            bow_a = list(domain.intersection(bow_a))\n",
    "        \n",
    "        #print(bow_c)\n",
    "        #print(bow_a)\n",
    "        if bow_c == bow_a:\n",
    "            EM = 1\n",
    "        common = Counter(bow_a) & Counter(bow_c)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return (0,0,0,0,0)\n",
    "        precision = 1.0 * num_same / len(bow_c)\n",
    "        recall = 1.0 * num_same / len(bow_a)\n",
    "        RE.append(recall)\n",
    "        PR.append(precision)\n",
    "\n",
    "        f1 = 2*precision*recall / (precision + recall + 1e-5)\n",
    "        F1.append(f1)\n",
    "    \n",
    "    PR_avg = np.mean(PR)\n",
    "    RE_avg = np.mean(RE)\n",
    "    F1_avg = np.mean(F1)\n",
    "    F1_max = np.max(F1)\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_set= {'orangebrown', 'spot', 'yellow', 'blue', 'rainbow', 'ivory', 'brown', 'gray', 'teal', 'bluewhite', 'orangepurple', 'black', 'white', 'gold', 'redorange', 'pink', 'blonde', 'tan', 'turquoise', 'grey', 'beige', 'golden', 'orange', 'bronze', 'maroon', 'purple', 'bluere', 'red', 'rust', 'violet', 'transparent', 'yes', 'silver', 'chrome', 'green', 'aqua'}\n",
    "shape_set = {'globular', 'octogon', 'ring', 'hoop', 'octagon', 'concave', 'flat', 'wavy', 'shamrock', 'cross', 'cylinder', 'cylindrical', 'pentagon', 'point', 'pyramidal', 'crescent', 'rectangular', 'hook', 'tube', 'cone', 'bell', 'spiral', 'ball', 'convex', 'square', 'arch', 'h', 'cuboid', 'step', 'rectangle', 'dot', 'oval', 'circle', 'star', 'crosse', 'crest', 'octagonal', 'cube', 'triangle', 'semicircle', 'domeshape', 'obelisk', 'corkscrew', 'curve', 'circular', 'xs', 'slope', 'pyramid', 'round', 'bow', 'straight', 'triangular', 'heart', 'fork', 'teardrop', 'fold', 'curl', 'spherical', 'diamond', 'keyhole', 'conical', 'dome', 'sphere', 'bellshaped', 'rounded', 'hexagon', 'flower', 'globe', 'torus'}\n",
    "yesno_set = {'yes', 'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'train': 16448, 'test': 3464, 'val': 2511})\n",
      "Counter({'YesNo': 7430, 'Others': 5823, 'choose': 4693, 'number': 2084, 'color': 1832, 'shape': 561})\n",
      "Counter({'Others': 1058, 'choose': 981, 'YesNo': 935, 'color': 228, 'number': 200, 'shape': 62})\n",
      "22423\n"
     ]
    }
   ],
   "source": [
    "img_dataset = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/img_dataset_0823_clean_te.json\", \"r\"))\n",
    "\n",
    "print(Counter([img_dataset[k]['split'] for k in img_dataset]))\n",
    "print(Counter([img_dataset[k]['Qcate'] for k in img_dataset]))\n",
    "print(Counter([img_dataset[k]['Qcate'] for k in img_dataset if img_dataset[k]['split'] == 'test']))\n",
    "print(len(set([img_dataset[k]['Guid'] for k in img_dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1579, 5: 780, 4: 703, 3: 402})\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for k in img_dataset:\n",
    "    if 'test' in img_dataset[k]['split']:\n",
    "        x.append(len(img_dataset[k]['A']))\n",
    "print(Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n",
      "3464\n",
      "defaultdict(<class 'int'>, {})\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "### With normalization, img_data\n",
    "bleu4_img_clean = {}\n",
    "RE_img_clean = {}\n",
    "F1_img_clean = {}\n",
    "mul_img_clean = {}\n",
    "acc_img_clean = {}\n",
    "\n",
    "drop = defaultdict(int)\n",
    "for k in img_dataset:\n",
    "    if not 'test' in img_dataset[k]['split']: continue\n",
    "    bleu4_img_clean[k] = []\n",
    "    RE_img_clean[k] = []\n",
    "    mul_img_clean[k] = []\n",
    "    F1_img_clean[k] = []\n",
    "    acc_img_clean[k] = []\n",
    "    datum = img_dataset[k]\n",
    "    Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    Qcate = img_dataset[k]['Qcate']\n",
    "    scores = np.zeros((len(all_A), len(all_A)))\n",
    "    c_list = []\n",
    "    a_list = []\n",
    "    for i in range(len(all_A)):\n",
    "        for j in range(len(all_A)):\n",
    "            c_list.append(all_A[i])\n",
    "            a_list.append(all_A[j])\n",
    "    scores = compute_bartscore_ParaBank(c_list, a_list).reshape((len(all_A), len(all_A)))\n",
    "    for i in range(len(all_A)):\n",
    "        Q = datum['Q'].replace('\"', \"\")\n",
    "        C = [all_A[i]]\n",
    "        \n",
    "        \n",
    "        score = np.max([min(1.0, scores[i][j]/scores[j][j]) for j in range(len(all_A)) if not i==j])\n",
    "        #print(scores)\n",
    "        \n",
    "        if Qcate == 'color': F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A, \"\", color_set)\n",
    "        elif Qcate == 'shape': F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A, \"\", shape_set)\n",
    "        elif Qcate == 'YesNo': F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A, \"\", yesno_set)\n",
    "        elif Qcate == 'number': F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A, \"\", {\"NUMBER\"})\n",
    "        else: F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A)\n",
    "        bleu4_img_clean[k].append(score)\n",
    "        RE_img_clean[k].append(RE_avg)\n",
    "        F1_img_clean[k].append(F1_avg)\n",
    "        if Qcate in ['color', 'shape', 'number', 'YesNo']: \n",
    "            acc_img_clean[k].append(F1_avg)\n",
    "            mul_img_clean[k].append(F1_avg * score)\n",
    "        else: \n",
    "            acc_img_clean[k].append(RE_avg)\n",
    "            mul_img_clean[k].append(RE_avg * score)\n",
    "        \n",
    "    if len(RE_img_clean) % 500 == 499: print(len(RE_img_clean))\n",
    "assert len(RE_img_clean) == len(mul_img_clean) == len(bleu4_img_clean) == len(acc_img_clean) == len(F1_img_clean)\n",
    "print(len(RE_img_clean))\n",
    "print(drop)\n",
    "print(np.sum(list(drop.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1579, 5: 780, 4: 703, 3: 402})\n",
      "mean RE:  0.9520481520712423\n",
      "mean F1:  0.5651965763804441\n",
      "mean accuracy:  0.9468847284155235\n",
      "mean fluency:  0.630391549716049\n",
      "mean mul:  0.6025058408240133\n",
      "\n",
      " choose\n",
      "mean RE:  0.9681807354975492\n",
      "mean F1:  0.30618078530459153\n",
      "mean accuracy:  0.9681807354975492\n",
      "mean fluency:  0.6839234503683429\n",
      "mean mul:  0.6665757875949492\n",
      "\n",
      " YesNo\n",
      "mean RE:  1.0\n",
      "mean F1:  0.9996384943803229\n",
      "mean accuracy:  0.9996384943803229\n",
      "mean fluency:  0.5743393083148296\n",
      "mean mul:  0.5741045999547485\n",
      "\n",
      " Others\n",
      "mean RE:  0.8780125472868293\n",
      "mean F1:  0.24214694472518142\n",
      "mean accuracy:  0.8780125472868293\n",
      "mean fluency:  0.6177855181773121\n",
      "mean mul:  0.5554930805609485\n",
      "\n",
      " color\n",
      "mean RE:  0.9836866471734893\n",
      "mean F1:  0.958286424484597\n",
      "mean accuracy:  0.958286424484597\n",
      "mean fluency:  0.6244174217447634\n",
      "mean mul:  0.6059464311760433\n",
      "\n",
      " shape\n",
      "mean RE:  0.9821236559139784\n",
      "mean F1:  0.9475757166031306\n",
      "mean accuracy:  0.9475757166031306\n",
      "mean fluency:  0.5919534839302913\n",
      "mean mul:  0.5639819434622342\n",
      "\n",
      " number\n",
      "mean RE:  0.995\n",
      "mean F1:  0.9469256561066419\n",
      "mean accuracy:  0.9469256561066419\n",
      "mean fluency:  0.7152740186880182\n",
      "mean mul:  0.6777361899488347\n"
     ]
    }
   ],
   "source": [
    "print(Counter([len(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "Qcate = ['choose', 'YesNo', 'Others', 'color', 'shape', 'number']\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_img_clean[k]) for k in RE_img_clean if img_dataset[k]['Qcate'] in Qcate]))\n",
    "print(\"mean F1: \", np.mean([np.mean(F1_img_clean[k]) for k in F1_img_clean if img_dataset[k]['Qcate'] in Qcate]))\n",
    "print(\"mean accuracy: \", np.mean([np.mean(acc_img_clean[k]) for k in acc_img_clean if img_dataset[k]['Qcate'] in Qcate]))\n",
    "print(\"mean fluency: \", np.mean([np.mean(bleu4_img_clean[k]) for k in bleu4_img_clean if img_dataset[k]['Qcate'] in Qcate]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_img_clean[k]) for k in mul_img_clean if img_dataset[k]['Qcate'] in Qcate]))\n",
    "\n",
    "Qcate = ['choose', 'YesNo', 'Others', 'color', 'shape', 'number']\n",
    "for cate in Qcate:\n",
    "    print(\"\\n\", cate)\n",
    "    print(\"mean RE: \", np.mean([np.mean(RE_img_clean[k]) for k in RE_img_clean if img_dataset[k]['Qcate'] == cate]))\n",
    "    print(\"mean F1: \", np.mean([np.mean(F1_img_clean[k]) for k in F1_img_clean if img_dataset[k]['Qcate'] == cate]))\n",
    "    print(\"mean accuracy: \", np.mean([np.mean(acc_img_clean[k]) for k in acc_img_clean if img_dataset[k]['Qcate'] == cate]))\n",
    "    print(\"mean fluency: \", np.mean([np.mean(bleu4_img_clean[k]) for k in bleu4_img_clean if img_dataset[k]['Qcate'] == cate]))\n",
    "    print(\"mean mul: \", np.mean([np.mean(mul_img_clean[k]) for k in mul_img_clean if img_dataset[k]['Qcate'] == cate]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n"
     ]
    }
   ],
   "source": [
    "# Cache normalization, img data\n",
    "img_dataset_0825_bartscorePB_te = copy.deepcopy(img_dataset)\n",
    "count = 0\n",
    "for k in img_dataset_0825_bartscorePB_te:\n",
    "    if not img_dataset_0825_bartscorePB_te[k]['split'] == 'test': continue\n",
    "    a_list = [a.replace('\"', \"\") for a in img_dataset_0825_bartscorePB_te[k]['A']]\n",
    "    scores = compute_bartscore_ParaBank(a_list, a_list)\n",
    "    img_dataset_0825_bartscorePB_te[k]['bartscore_normalizer'] = json.dumps(list(scores))\n",
    "    count += 1\n",
    "    if count % 500 == 499: print(count)\n",
    "#json.dump(img_dataset_0825_bartscorePB_te, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/img_dataset_0825_bartscorePB_te.json\", \"w\"),indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"bertscore\")\n",
    "def compute_bertscore(cands, a, model_type):\n",
    "    metric.add_batch(predictions = cands, references = [a]*len(cands))\n",
    "    score = metric.compute(model_type=model_type)\n",
    "    return np.mean(score['f1']), np.max(score['f1'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deno =  [0.5113 0.512  0.5519 0.5394 0.5117]\n",
      "[0.0049 0.0054 0.0074 0.0063 0.0052]\n",
      "[0.0005 0.0007 0.001  0.0008 0.0006]\n"
     ]
    }
   ],
   "source": [
    "cands = [\"5 years.\"]*5\n",
    "a = [\"The last direct Capetian died some 577 years before the coat of arms of France was created.\",\n",
    "    \"The last direct Capetian died 577 years before the coat of arms of France was created.\",\n",
    "    \"The coat of arms of France was created 577 years after the last direct Capetian died.\",\n",
    "    \"The last Capetian died 572 years before the coat of arms was created.\",\n",
    "    \"The last direct Capetian died about 577 years before the coat of arms of France was created.\"]\n",
    "deno = compute_bartscore_ParaBank(a, a)\n",
    "print(\"deno = \", deno)\n",
    "print(compute_bartscore_ParaBank(a, cands)/deno)\n",
    "print(compute_bartscore_ParaBank(cands, a)/deno)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BARTScore",
   "language": "python",
   "name": "bartscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
