{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, time\n",
    "import math\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "from word2number import w2n\n",
    "import string, re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\",\"textcat\",\"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNum(word):\n",
    "    try: return w2n.word_to_num(word)\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text): # additional: converting numbers to digit form\n",
    "        return \" \".join([str(toNum(w)) for w in text.split()])\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation) - set(['.'])\n",
    "        text1 = \"\".join(ch for ch in text if ch not in exclude)\n",
    "        return re.sub(r\"\\.(?!\\d)\", \"\", text1) # remove '.' if it's not a decimal point\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def lemmatization(text):\n",
    "        return \" \".join([token.lemma_ for token in nlp(text)])\n",
    "\n",
    "    if len(s.strip()) == 1:\n",
    "        # accept article and punc if input is a single char\n",
    "        return white_space_fix(lower(s))\n",
    "    elif len(s.strip().split()) == 1: \n",
    "        # accept article if input is a single word\n",
    "        return lemmatization(white_space_fix(remove_punc(lower(s))))\n",
    "\n",
    "    return lemmatization(white_space_fix(remove_articles(remove_punc(lower(s)))))\n",
    "\n",
    "# Language eval with Caption metrics\n",
    "class Evaluate(object):\n",
    "    def __init__(self):\n",
    "        self.scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            #(Meteor(), \"METEOR\"),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            #(Cider(), \"CIDEr\"),\n",
    "            #(Spice(), \"Spice\")\n",
    "        ]\n",
    "    \n",
    "    def score(self, ref, hypo):\n",
    "        final_scores = {}\n",
    "        for scorer, method in self.scorers:\n",
    "            if type(method) == list: score, scores = scorer.compute_score(ref, hypo, verbose=0)\n",
    "            else: score, scores = scorer.compute_score(ref, hypo)\n",
    "            if type(score) == list:\n",
    "                for m, s in zip(method, score):\n",
    "                    #print(m)\n",
    "                    final_scores[m] = s\n",
    "            else:\n",
    "                #print(method)\n",
    "                final_scores[method] = score\n",
    "        return final_scores\n",
    "\n",
    "    def evaluate(self, return_scores=False, **kwargs):\n",
    "        ans = kwargs.pop('ref', {}) # support a list of references\n",
    "        cand = kwargs.pop('cand', {}) # only support one cand per sample, but the input cand has size batch_size x K\n",
    "\n",
    "        hypo = {}\n",
    "        ref = {}\n",
    "        i = 0\n",
    "        for i in range(len(cand)):\n",
    "            hypo[i] = [cand[i][0]]\n",
    "            ref[i] = ans[i]\n",
    "        \n",
    "        final_scores = self.score(ref, hypo)\n",
    "        #print ('Bleu_1:\\t', final_scores['Bleu_1'])\n",
    "        #print ('Bleu_2:\\t', final_scores['Bleu_2'])\n",
    "        #print ('Bleu_3:\\t', final_scores['Bleu_3'])\n",
    "        #print ('Bleu_4:\\t', final_scores['Bleu_4'])\n",
    "        #print ('METEOR:\\t', final_scores['METEOR'])\n",
    "        #print ('ROUGE_L:', final_scores['ROUGE_L'])\n",
    "        #print ('CIDEr:\\t', final_scores['CIDEr'])\n",
    "        #print ('Spice:\\t', final_scores['Spice'])\n",
    "\n",
    "        if return_scores:\n",
    "            return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9.99999998000001e-16,\n",
       "  3.162277655424966e-11,\n",
       "  9.999999986666689e-10,\n",
       "  5.62341324487423e-09],\n",
       " [[9.99999998000001e-16],\n",
       "  [3.162277655424966e-11],\n",
       "  [9.999999986666689e-10],\n",
       "  [5.62341324487423e-09]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = Bleu(4)\n",
    "B.compute_score({1:['eee']}, {1: ['seef']}, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA Eval (SQuAD style EM, F1)\n",
    "def compute_vqa_metrics(cands, a):\n",
    "    if len(cands) == 0: return (0,0,0)\n",
    "    bow_a = normalize_text(a).split()\n",
    "    F1 = []\n",
    "    EM = 0\n",
    "    RE = []\n",
    "    PR = []\n",
    "    for c in cands:\n",
    "        bow_c = normalize_text(c).split()\n",
    "        if bow_c == bow_a:\n",
    "            EM = 1\n",
    "        common = Counter(bow_a) & Counter(bow_c)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return (0,0,0,0,0)\n",
    "        precision = 1.0 * num_same / len(bow_c)\n",
    "        recall = 1.0 * num_same / len(bow_a)\n",
    "        RE.append(recall)\n",
    "        PR.append(precision)\n",
    "\n",
    "        f1 = 2*precision*recall / (precision + recall + 1e-5)\n",
    "        F1.append(f1)\n",
    "    \n",
    "    PR_avg = np.mean(PR)\n",
    "    RE_avg = np.mean(RE)\n",
    "    F1_avg = np.mean(F1)\n",
    "    F1_max = np.max(F1)\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'train': 17812, 'test': 4695, 'val': 2455})\n",
      "24962\n",
      "Counter({'train': 16448, 'ood_test': 3948, 'val': 2511, 'ind_test': 2485})\n",
      "Counter({'YesNo': 8410, 'Others': 6689, 'choose': 5226, 'number': 2337, 'color': 2068, 'shape': 662})\n",
      "25392\n"
     ]
    }
   ],
   "source": [
    "txt_dataset = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/txt_dataset_0820_addKA.json\", \"r\"))\n",
    "img_dataset = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/img_dataset_0819_16neg.json\", \"r\"))\n",
    "\n",
    "print(Counter([txt_dataset[k]['split'] for k in txt_dataset]))\n",
    "print(len(set([txt_dataset[k]['Guid'] for k in txt_dataset])))\n",
    "\n",
    "print(Counter([img_dataset[k]['split'] for k in img_dataset]))\n",
    "print(Counter([img_dataset[k]['Qcate'] for k in img_dataset]))\n",
    "print(len(set([img_dataset[k]['Guid'] for k in img_dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 2476, 5: 1500, 4: 527, 3: 167, 2: 25})\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for k in txt_dataset:\n",
    "    if txt_dataset[k]['split'] == 'test':\n",
    "        x.append(len(txt_dataset[k]['A']))\n",
    "print(Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 6242, 5: 139, 4: 36, 3: 15, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for k in img_dataset:\n",
    "    if 'test' in img_dataset[k]['split']:\n",
    "        x.append(len(img_dataset[k]['A']))\n",
    "print(Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38338\n"
     ]
    }
   ],
   "source": [
    "Q = []\n",
    "A_list = []\n",
    "C = []\n",
    "Keywords_A = []\n",
    "for k in img_dataset:\n",
    "    if not 'test' in img_dataset[k]['split']: continue\n",
    "    datum = img_dataset[k]\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    for i in range(len(all_A)):\n",
    "        Q.append(datum['Q'].replace('\"', \"\"))\n",
    "        C.append([all_A[i]])\n",
    "        A_list.append(all_A[:i] + all_A[i+1:])\n",
    "        Keywords_A.append(datum['Keywords_A'].replace('\"', \"\"))\n",
    "assert len(C) == len(Q) == len(A_list) == len(Keywords_A)\n",
    "print(len(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "399\n",
      "599\n",
      "799\n",
      "999\n",
      "1199\n",
      "1399\n",
      "1599\n",
      "1799\n",
      "1999\n",
      "2199\n",
      "2399\n",
      "2599\n",
      "2799\n",
      "2999\n",
      "3199\n",
      "3399\n",
      "3599\n",
      "3799\n",
      "3999\n",
      "4199\n",
      "4399\n",
      "4599\n",
      "4799\n",
      "4999\n",
      "5199\n",
      "5399\n",
      "5599\n",
      "5799\n",
      "5999\n",
      "6199\n",
      "6399\n",
      "6433\n"
     ]
    }
   ],
   "source": [
    "## 同一个sample单独，最后取avg/max\n",
    "eval_f = Evaluate()\n",
    "bleu4 = {}\n",
    "RE = {}\n",
    "mul = {}\n",
    "for k in img_dataset:\n",
    "    if not 'test' in img_dataset[k]['split']: continue\n",
    "    bleu4[k] = []\n",
    "    RE[k] = []\n",
    "    mul[k] = []\n",
    "    datum = img_dataset[k]\n",
    "    Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    for i in range(len(all_A)):\n",
    "        Q = datum['Q'].replace('\"', \"\")\n",
    "        C = [all_A[i]]\n",
    "        A_list = all_A[:i] + all_A[i+1:]\n",
    "        scores = eval_f.evaluate(cand=[C], ref=[A_list], return_scores=True)\n",
    "        #print(scores)\n",
    "        bleu4[k].append(scores['Bleu_4'])\n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A)\n",
    "        RE[k].append(RE_avg)\n",
    "        mul[k].append(RE_avg * scores['Bleu_4'])\n",
    "    if len(RE) % 200 == 199: print(len(RE))\n",
    "assert len(RE) == len(mul) == len(bleu4)\n",
    "print(len(RE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_human_scores = {'RE': RE, 'mul': mul, 'bleu4': bleu4}\n",
    "pickle.dump(img_human_scores, open(\"./img_human_scores.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_human_scores = pickle.load(open(\"./img_human_scores.pkl\", \"rb\"))\n",
    "RE = img_human_scores['RE']\n",
    "mul = img_human_scores['mul']\n",
    "bleu4 = img_human_scores['bleu4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max RE:  0.9828157800008271\n",
      "max mul:  0.8684240369906326\n",
      "max bleu4:  0.9320046073960947\n",
      "mean RE:  0.7364894343993178\n",
      "mean mul:  0.48238871302412323\n",
      "mean bleu4:  0.6526889064849104\n"
     ]
    }
   ],
   "source": [
    "print(\"max RE: \", np.mean([max(RE[k]) for k in RE]))\n",
    "print(\"max mul: \", np.mean([max(mul[k]) for k in mul]))\n",
    "print(\"max bleu4: \", np.mean([max(bleu4[k]) for k in bleu4]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE[k]) for k in RE]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul[k]) for k in mul]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4[k]) for k in bleu4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7076\n",
      "[1.0, 0.42857142857142855, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855]\n",
      "[0.7739321540095442, 0.2863070881368244, 0.7561289225233876, 0.6687403048963458, 5.233846518568202e-05, 0.5114432342517753]\n",
      "\n",
      "7986\n",
      "[1.0, 0, 0.2, 0.4, 0, 0]\n",
      "[0.4692470063653536, 0.5410822689681074, 0.47987820661783703, 0.32466791540375595, 0.5329462626443542, 0.5372849657937071]\n",
      "\n",
      "3774\n",
      "[1.0, 0, 0.25, 0, 0.5, 0.25]\n",
      "[0.7138957846600729, 0.8313539763197327, 0.7765453554362727, 0.36336981878206925, 0.7307717332985799, 4.887406509299109e-05]\n",
      "\n",
      "4418\n",
      "[0.3333333333333333, 0.3333333333333333, 0, 1.0, 0.6666666666666666, 0.6666666666666666]\n",
      "[0.38141656158453924, 1.0294994182935423e-08, 0.8817122475196995, 1.5352597835010118e-12, 0.4952330115902502, 0.904431377538006]\n",
      "\n",
      "5173\n",
      "[0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666]\n",
      "[0.7473021918305992, 3.137143608036445e-05, 0.7138099644123547, 0.7598356855903131, 5.406149860844445e-09, 0.32885804547866293]\n",
      "\n",
      "5911\n",
      "[1.0, 0.6, 0.3, 0.6, 0.6, 0.2]\n",
      "[0.4792365811093905, 0.3869431775754755, 0.4760116547320126, 0.21446539595604802, 0.6505339954086, 0.7311104455702009]\n",
      "\n",
      "66\n",
      "[1.0, 0.75, 0, 0.5, 0.25, 1.0]\n",
      "[0.5452469119105582, 0.7856293016283906, 0.5169731538105412, 0.5988059641008373, 0.9036020033535557, 0.5109955810746272]\n",
      "\n",
      "336\n",
      "[1.0, 0.2, 0.6, 0.6, 0.6, 0.2]\n",
      "[0.8531413605907252, 0.9574533679820988, 0.8971148504200592, 0.8524094630119715, 0.06614057939049886, 0.7349031579858321]\n",
      "\n",
      "1353\n",
      "[1.0, 0, 0.3333333333333333, 0, 1.0, 0]\n",
      "[0.7479996328179238, 0.5898019930403656, 0.8289657838931372, 0.8524419935996841, 0.5035337886952919, 0.8694417437520626]\n",
      "\n",
      "1624\n",
      "[1.0, 0.25, 0.25, 0, 0, 0.25]\n",
      "[5.39059484752114e-09, 8.025716722742473e-09, 0.32523403427534525, 0.5329462626443542, 1.0445522729063402e-12, 0.48109772904027326]\n",
      "\n",
      "15135\n",
      "[1.0, 1.0, 1.0, 0.375, 1.0, 1.0]\n",
      "[0.9999999998957473, 0.8656030551684151, 0.9457416089045797, 0.30960799209910794, 0.8137489370168771, 0.9999999998957473]\n",
      "\n",
      "15587\n",
      "[1.0, 0.3333333333333333, 0, 0.3333333333333333, 0.3333333333333333, 0]\n",
      "[0.37991784278030427, 0.7476743904255464, 0.5169731538105413, 0.9036020033861858, 0.6147881527990924, 0.6914415690877681]\n",
      "\n",
      "12091\n",
      "[0.8888888888888888, 0.5555555555555556, 0.4444444444444444, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111]\n",
      "[0.5008718428095981, 0.6100034456009603, 0.4958271734289548, 0.5109955810746272, 0.2061477352038093, 0.6156286978237718]\n",
      "\n",
      "14044\n",
      "[1.0, 0, 0, 0.2, 0.2, 0.2]\n",
      "[0.7703484635762631, 0.5698363773141187, 0.8230724649295764, 0.935133483563592, 0.5101544332697462, 0.8694417437520626]\n",
      "\n",
      "10151\n",
      "[1.0, 0.6, 0.4, 0.2, 0.2, 0.2]\n",
      "[0.5844356470186963, 7.001843276184207e-13, 0.6591827715423145, 0.48245960455907727, 0.5169731539213033, 2.89785676216564e-05]\n",
      "\n",
      "10524\n",
      "[1.0, 0.5, 0.75, 1.0, 1.0, 0.25]\n",
      "[0.9999999998635914, 0.44683107169538017, 0.8938651487919365, 0.5475235664703343, 0.999999999883558, 2.235208016311594e-05]\n",
      "\n",
      "19579\n",
      "[1.0, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.42857142857142855]\n",
      "[2.839730284068095e-05, 0.6446561883292008, 0.6168005796387003, 0.6792310722130407, 0.740637500749382, 0.82053222162547]\n",
      "\n",
      "19898\n",
      "[0.75, 0.25, 0.25, 0.16666666666666666, 0.16666666666666666, 0.25]\n",
      "[0.4212246619015133, 0.6899302125135413, 0.6590955739181753, 9.19322715022905e-09, 7.3860999543075715e-09, 0.7348889200398049]\n",
      "\n",
      "19960\n",
      "[0.5714285714285714, 0.42857142857142855, 0.14285714285714285, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857]\n",
      "[0.6389431041697643, 0.7419446626572949, 0.5462757643294353, 0.4887164515321722, 0.4408231875199723, 7.730551755471572e-09]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in RE:\n",
    "    for i in RE[k]:\n",
    "        if i<0.5 and i>0.0:\n",
    "            if random.random()>0.05: break\n",
    "            print(k)\n",
    "            print(RE[k])\n",
    "            #print(sorted(mul[k], reverse=True))\n",
    "            print(bleu4[k])\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['\"Military and ordinary working people are carved on the building that '\n",
      "       'is part of the Quezon City Hall complex.\"',\n",
      "       '\"Figures are carved on the building.\"',\n",
      "       '\"A mural is carved on the building that is part of the Quezon City '\n",
      "       'Hall complex.\"',\n",
      "       '\"Images of people are carved on the building that is part of the '\n",
      "       'Quezon City Hall complex.\"',\n",
      "       '\"A group of people are gathered with a flag in the background, which '\n",
      "       'is what is carved on the building that is part of the Quezon City Hall '\n",
      "       'complex.\"',\n",
      "       '\"Figures of people are carved on the building.\"'],\n",
      " 'Guid': 'dc63e242d46f11ebba07a504a20e7724',\n",
      " 'Keywords_A': '\"A military and ordinary working people.\"',\n",
      " 'Q': '\"What is carved on the building that is part of the Quezon City Hall '\n",
      "      'complex?\"',\n",
      " 'Qcate': 'Others',\n",
      " 'Qtype': 'img-Singlehop',\n",
      " 'img_negFacts': [{'caption': 'City centre 05',\n",
      "                   'image_id': '10239772',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/f/f9/City_centre_05.jpg',\n",
      "                   'overlap_scores': '(0.1429, 0.0909, 0.3333, 0.0641, 0.3571, '\n",
      "                                     '0.0909)',\n",
      "                   'title': 'City centre 05',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:City_centre_05.jpg'},\n",
      "                  {'caption': 'EDSA  Heavy traffic on the EDSA in Makati City.',\n",
      "                   'image_id': '10239777',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/EDSA.JPG/800px-EDSA.JPG',\n",
      "                   'overlap_scores': '(0.0909, 0.0909, 0.0909, 0.1026, 0.1702, '\n",
      "                                     '0.0809)',\n",
      "                   'title': 'EDSA',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:EDSA.JPG'},\n",
      "                  {'caption': 'Quezon Memorial Circle - top shot from City '\n",
      "                              'Hall (Diliman, Quezon City)(2018-02-07) '\n",
      "                              'Overview of Quezon Memorial Circle in Diliman, '\n",
      "                              'Quezon City',\n",
      "                   'image_id': '10239784',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Quezon_Memorial_Circle_-_top_shot_from_City_Hall_%28Diliman%2C_Quezon_City%29%282018-02-07%29.jpg/800px-Quezon_Memorial_Circle_-_top_shot_from_City_Hall_%28Diliman%2C_Quezon_City%29%282018-02-07%29.jpg',\n",
      "                   'overlap_scores': '(0.1429, 0.2727, 0.0968, 0.1538, 0.0845, '\n",
      "                                     '0.0627)',\n",
      "                   'title': 'Quezon Memorial Circle - top shot from City Hall '\n",
      "                            '(Diliman, Quezon City)(2018-02-07)',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Quezon_Memorial_Circle_-_top_shot_from_City_Hall_(Diliman%2C_Quezon_City)(2018-02-07).jpg'},\n",
      "                  {'caption': 'Commonwealth Avenue - Iglesia Central near '\n",
      "                              'Tandang Sora (New Era, Quezon City)(2018-02-07)',\n",
      "                   'image_id': '10239775',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Commonwealth_Avenue_-_Iglesia_Central_near_Tandang_Sora_(New_Era%2C_Quezon_City)(2018-02-07).jpg/1024px-Commonwealth_Avenue_-_Iglesia_Central_near_Tandang_Sora_(New_Era%2C_Quezon_City)(2018-02-07).jpg',\n",
      "                   'overlap_scores': '(0.1053, 0.1818, 0.0741, 0.1538, 0.1333, '\n",
      "                                     '0.0118)',\n",
      "                   'title': 'Commonwealth Avenue - Iglesia Central near '\n",
      "                            'Tandang Sora (New Era, Quezon City)(2018-02-07)',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Commonwealth_Avenue_-_Iglesia_Central_near_Tandang_Sora_(New_Era%2C_Quezon_City)(2018-02-07).jpg'},\n",
      "                  {'caption': 'Metro Manila skyline - with Cubao area (Quezon '\n",
      "                              'City)(2017-09-06) Metro Manila skyline from '\n",
      "                              'Katipunan, Quezon City',\n",
      "                   'image_id': '10239785',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Metro_Manila_skyline_-_with_Cubao_area_%28Quezon_City%29%282017-09-06%29.jpg/800px-Metro_Manila_skyline_-_with_Cubao_area_%28Quezon_City%29%282017-09-06%29.jpg',\n",
      "                   'overlap_scores': '(0.1026, 0.1818, 0.0714, 0.1538, 0.1062, '\n",
      "                                     '0.0018)',\n",
      "                   'title': 'Metro Manila skyline - with Cubao area (Quezon '\n",
      "                            'City)(2017-09-06)',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Metro_Manila_skyline_-_with_Cubao_area_(Quezon_City)(2017-09-06).jpg'},\n",
      "                  {'caption': 'Tokyo metro road 311-2006-01-28',\n",
      "                   'image_id': '10239780',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Tokyo_metro_road_311-2006-01-28.jpg/800px-Tokyo_metro_road_311-2006-01-28.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.0645, 0.0)',\n",
      "                   'title': 'Tokyo metro road 311-2006-01-28',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Tokyo_metro_road_311-2006-01-28.jpg'},\n",
      "                  {'caption': 'Tegucigalpa from La Leona',\n",
      "                   'image_id': '10239793',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/3/3e/Tegucigalpa_from_La_Leona.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.08, 0.0)',\n",
      "                   'title': 'Tegucigalpa from La Leona',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Tegucigalpa_from_La_Leona.jpg'},\n",
      "                  {'caption': 'NQS a la altura del Campín',\n",
      "                   'image_id': '10239786',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a8/NQS_a_la_altura_del_Camp%C3%ADn.JPG/1200px-NQS_a_la_altura_del_Camp%C3%ADn.JPG',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.0769, 0.0)',\n",
      "                   'title': 'NQS a la altura del Campín',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:NQS_a_la_altura_del_Camp%C3%ADn.JPG'},\n",
      "                  {'caption': 'Bertrix',\n",
      "                   'image_id': '10239782',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/b/b1/Bertrix.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.2857, 0.0)',\n",
      "                   'title': 'Bertrix',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Bertrix.jpg'},\n",
      "                  {'caption': 'Downtown iligan Downtown Iligan',\n",
      "                   'image_id': '10239789',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/7/71/Downtown_iligan.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.0645, 0.0)',\n",
      "                   'title': 'Downtown iligan',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Downtown_iligan.jpg'},\n",
      "                  {'caption': 'USA LosAngeles DowntownLA from GettyCenter',\n",
      "                   'image_id': '10239790',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/3/3e/USA_LosAngeles_DowntownLA_from_GettyCenter.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.0476, 0.0)',\n",
      "                   'title': 'USA LosAngeles DowntownLA from GettyCenter',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:USA_LosAngeles_DowntownLA_from_GettyCenter.jpg'},\n",
      "                  {'caption': 'Puerto Valpo',\n",
      "                   'image_id': '10239776',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d1/Puerto_Valpo.jpg/800px-Puerto_Valpo.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.1667, 0.0)',\n",
      "                   'title': 'Puerto Valpo',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Puerto_Valpo.jpg'},\n",
      "                  {'caption': 'Arakuchi Elementary school2',\n",
      "                   'image_id': '10239773',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/f/f3/Arakuchi_Elementary_school2.JPG',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.0741, 0.0)',\n",
      "                   'title': 'Arakuchi Elementary school2',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Arakuchi_Elementary_school2.JPG'},\n",
      "                  {'caption': 'Navi Mumbai, Mahape',\n",
      "                   'image_id': '10239781',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Navi_Mumbai%2C_Mahape.jpg/800px-Navi_Mumbai%2C_Mahape.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.1053, 0.0)',\n",
      "                   'title': 'Navi Mumbai, Mahape',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Navi_Mumbai%2C_Mahape.jpg'},\n",
      "                  {'caption': 'Frankfurt (4707119744)',\n",
      "                   'image_id': '10239787',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Frankfurt_(4707119744).jpg/1024px-Frankfurt_(4707119744).jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0385, 0.1364, 0.0)',\n",
      "                   'title': 'Frankfurt (4707119744)',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:Frankfurt_(4707119744).jpg'},\n",
      "                  {'caption': '2015 Khamovniki district',\n",
      "                   'image_id': '10239788',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/2015_Khamovniki_district.jpg/800px-2015_Khamovniki_district.jpg',\n",
      "                   'overlap_scores': '(0, 0, 0, 0.0256, 0.0833, 0.0)',\n",
      "                   'title': '2015 Khamovniki district',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:2015_Khamovniki_district.jpg'}],\n",
      " 'img_posFacts': [{'caption': '7795jfQuezon City Hall Metro Manilafvf 37',\n",
      "                   'image_id': '00014854',\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/7795jfQuezon_City_Hall_Metro_Manilafvf_37.JPG/800px-7795jfQuezon_City_Hall_Metro_Manilafvf_37.JPG',\n",
      "                   'title': '7795jfQuezon City Hall Metro Manilafvf 37',\n",
      "                   'url': 'https://commons.wikimedia.org/wiki/File:7795jfQuezon_City_Hall_Metro_Manilafvf_37.JPG'}],\n",
      " 'split': 'ind_test',\n",
      " 'topic': 'civic center',\n",
      " 'txt_negFacts': [{'fact': \"The Main Library's location has moved or \"\n",
      "                           'reassigned for several times throughout the '\n",
      "                           'history of the Quezon City Public Library. The '\n",
      "                           'library is currently housed within a three-story '\n",
      "                           'building within the Quezon City Hall Complex which '\n",
      "                           'was inaugurated and opened by Mayor Herbert '\n",
      "                           'Bautista along with City Librarian Emelita '\n",
      "                           'Villanueva on February 6, 2017.',\n",
      "                   'overlap_scores': '(0.1667, 0.4545, 0.102, 0.2821, 0.0629, '\n",
      "                                     '0.5745)',\n",
      "                   'scores': '(0.16, 0.0, 0.16)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'The library is currently housed within a '\n",
      "                           'three-story building within the Quezon City Hall '\n",
      "                           'Complex which was inaugurated and opened by Mayor '\n",
      "                           'Herbert Bautista along with City Librarian Emelita '\n",
      "                           'Villanueva on February 6, 2017.',\n",
      "                   'overlap_scores': '(0.2273, 0.4545, 0.1515, 0.2821, 0.0991, '\n",
      "                                     '0.4545)',\n",
      "                   'scores': '(0.21, 0.0, 0.21)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'Close landmarks include the Ninoy Aquino Parks and '\n",
      "                           'Wildlife Center, Department of Agriculture '\n",
      "                           'Building, Quezon City Hall, Department of Agrarian '\n",
      "                           'Reform Building, the National Housing Authority '\n",
      "                           'and PTV Complex.',\n",
      "                   'overlap_scores': '(0.2439, 0.4545, 0.1667, 0.2179, 0.0813, '\n",
      "                                     '0.4545)',\n",
      "                   'scores': '(0.19, 0.0, 0.19)',\n",
      "                   'title': 'Quezon Memorial station',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_Memorial_station'},\n",
      "                  {'fact': 'The library is currently housed within a '\n",
      "                           'three-story building within the Quezon City Hall '\n",
      "                           'Complex which was inaugurated and opened by Mayor '\n",
      "                           'Herbert Bautista along with City Librarian Emelita '\n",
      "                           'Villanueva on February 6, 2017. The building for '\n",
      "                           'the main library has been planned since 2010.',\n",
      "                   'overlap_scores': '(0.1923, 0.4545, 0.122, 0.2821, 0.0772, '\n",
      "                                     '0.4545)',\n",
      "                   'scores': '(0.19, 0.0, 0.19)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'It marked Quezon City as a Rotary Peace City as '\n",
      "                           'part of the \"Community Peace Cities/Towns\" concept '\n",
      "                           'conceived by the Rotary Club of Wagga Wagga, '\n",
      "                           'Australia.',\n",
      "                   'overlap_scores': '(0.2703, 0.4545, 0.1923, 0.1795, 0.0909, '\n",
      "                                     '0.4545)',\n",
      "                   'scores': '(0.14, 0.0, 0.14)',\n",
      "                   'title': 'Quezon Memorial Circle',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_Memorial_Circle'},\n",
      "                  {'fact': 'The Quezon City Public Library (QCPL, Filipino: '\n",
      "                           'Aklatan ng Lungsod Quezon) is a public library of '\n",
      "                           'Quezon City, Metro Manila, Philippines. The main '\n",
      "                           'library is situated within the Quezon City Hall '\n",
      "                           'complex while the library also maintain smaller '\n",
      "                           'branches within the city.',\n",
      "                   'overlap_scores': '(0.1403, 0.3636, 0.087, 0.3718, 0.1082, '\n",
      "                                     '0.4236)',\n",
      "                   'scores': '(0.19, 0.0, 0.19)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'The main library moved to the New Quezon City Hall '\n",
      "                           \"Building's ground floor where it operated for ten \"\n",
      "                           'years before transferring to a dedicated building. '\n",
      "                           'An inauguration ceremony held on August 6, 1984 by '\n",
      "                           'Pacencia J. Buendia, former City Superintendent of '\n",
      "                           'Libraries, was held to mark the move.',\n",
      "                   'overlap_scores': '(0.1455, 0.3636, 0.0909, 0.2308, 0.0619, '\n",
      "                                     '0.3836)',\n",
      "                   'scores': '(0.16, 0.0, 0.16)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'The Quezon City Public Library (QCPL) operates 20 '\n",
      "                           'branches throughout the city, with its Main '\n",
      "                           'Library located within the Quezon City Hall '\n",
      "                           'Complex.',\n",
      "                   'overlap_scores': '(0.2286, 0.3636, 0.1667, 0.2821, 0.1507, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.28, 0.0, 0.28)',\n",
      "                   'title': 'Quezon City',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City'},\n",
      "                  {'fact': 'The main library vacated the old city hall '\n",
      "                           \"building and moved to the Lion's International \"\n",
      "                           'Building at Bernardo Park which stands in an area '\n",
      "                           \"donated by the Lion's International to the city \"\n",
      "                           'government. The main library moved to the New '\n",
      "                           \"Quezon City Hall Building's ground floor where it \"\n",
      "                           'operated for ten years before transferring to a '\n",
      "                           'dedicated building.',\n",
      "                   'overlap_scores': '(0.1481, 0.3636, 0.093, 0.2308, 0.0513, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.26, 0.0, 0.26)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'The main library moved to the New Quezon City Hall '\n",
      "                           \"Building's ground floor where it operated for ten \"\n",
      "                           'years before transferring to a dedicated building.',\n",
      "                   'overlap_scores': '(0.2667, 0.3636, 0.2105, 0.2308, 0.1192, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.25, 0.0, 0.25)',\n",
      "                   'title': 'Quezon City Public Library',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City_Public_Library'},\n",
      "                  {'fact': 'The Quezon City Science Interactive Center is '\n",
      "                           'regarded as the first of its kind science '\n",
      "                           'interactive center in the Philippines. The Quezon '\n",
      "                           'City Public Library (QCPL) operates 20 branches '\n",
      "                           'throughout the city, with its Main Library located '\n",
      "                           'within the Quezon City Hall Complex.',\n",
      "                   'overlap_scores': '(0.1633, 0.3636, 0.1053, 0.2821, 0.0806, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.19, 0.0, 0.19)',\n",
      "                   'title': 'Quezon City',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City'},\n",
      "                  {'fact': 'District Headquarters are located inside the '\n",
      "                           'Quezon City Hall Complex. The Armed Forces of the '\n",
      "                           \"Philippines' General Headquarters is in Camp \"\n",
      "                           'Emilio Aguinaldo in Murphy, Quezon City.',\n",
      "                   'overlap_scores': '(0.2162, 0.3636, 0.1538, 0.2821, 0.1222, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.18, 0.0, 0.18)',\n",
      "                   'title': 'Quezon City',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City'},\n",
      "                  {'fact': 'Peace and order, which includes traffic management '\n",
      "                           'of the city is administered by the Quezon City '\n",
      "                           'Department of Public Order and Safety, whose '\n",
      "                           'offices are found inside the Quezon City Hall '\n",
      "                           'Complex, is headed by retired QCPD District '\n",
      "                           'Director – Police Chief Superintendent Elmo San '\n",
      "                           'Diego.',\n",
      "                   'overlap_scores': '(0.16, 0.3636, 0.1026, 0.2821, 0.0767, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.17, 0.0, 0.17)',\n",
      "                   'title': 'Quezon City',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_City'},\n",
      "                  {'fact': 'It is also home to the Quezon City Hall Complex '\n",
      "                           \"located on the avenue's junction with Elliptical \"\n",
      "                           'Road. The entire avenue is designated as National '\n",
      "                           'Route 174 (N174) of the Philippine highway '\n",
      "                           'network.',\n",
      "                   'overlap_scores': '(0.1951, 0.3636, 0.1333, 0.2821, 0.1111, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.17, 0.0, 0.17)',\n",
      "                   'title': 'East Avenue, Quezon City',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/East_Avenue,_Quezon_City'},\n",
      "                  {'fact': '60 percent of the original house in Gilmore was '\n",
      "                           'used for the reconstruction of the house building '\n",
      "                           'now located at the Quezon Memorial Circle. An '\n",
      "                           'adjacent rectangular social hall was present in '\n",
      "                           'the Quezon house complex.',\n",
      "                   'overlap_scores': '(0.2286, 0.3636, 0.1667, 0.1538, 0.0553, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.17, 0.0, 0.17)',\n",
      "                   'title': 'Quezon Heritage House',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Quezon_Heritage_House'},\n",
      "                  {'fact': \"The street is located in Quezon City's government \"\n",
      "                           'area, known for different national and local '\n",
      "                           'government institutions, offices, and hospitals. '\n",
      "                           'It is also home to the Quezon City Hall Complex '\n",
      "                           \"located on the avenue's junction with Elliptical \"\n",
      "                           'Road.',\n",
      "                   'overlap_scores': '(0.1778, 0.3636, 0.1176, 0.2821, 0.0894, '\n",
      "                                     '0.3636)',\n",
      "                   'scores': '(0.16, 0.0, 0.16)',\n",
      "                   'title': 'East Avenue, Quezon City',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/East_Avenue,_Quezon_City'}],\n",
      " 'word_lists': {'answerwords': 'Ordinary || Military || ordinary || People || '\n",
      "                               'people || military',\n",
      "                'keywords': 'city || Quezon || City || Part || quezon || hall '\n",
      "                            '|| Building || Hall || complex || part || Complex '\n",
      "                            '|| building',\n",
      "                'noun_chunks': 'the Quezon City Hall complex'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(img_dataset['14044'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 769, 5: 538, 4: 486, 3: 288, 2: 186, 1: 178, 0: 40})\n",
      "Counter({6: 963, 5: 514, 4: 464, 3: 273, 2: 146, 1: 100, 0: 25})\n",
      "Counter({6: 1313, 5: 759, 4: 714, 3: 458, 1: 327, 2: 308, 0: 69})\n",
      "Counter({6: 1580, 5: 782, 4: 707, 3: 400, 2: 261, 1: 177, 0: 41})\n"
     ]
    }
   ],
   "source": [
    "print(Counter([np.sum(np.array(RE[k]) > 0.5) for k in RE if img_dataset[k]['split'] == 'ind_test']))\n",
    "print(Counter([np.sum(np.array(RE[k]) >= 0.3) for k in RE if img_dataset[k]['split'] == 'ind_test']))\n",
    "print(Counter([np.sum(np.array(RE[k]) > 0.5) for k in RE if img_dataset[k]['split'] == 'ood_test']))\n",
    "print(Counter([np.sum(np.array(RE[k]) >= 0.3) for k in RE if img_dataset[k]['split'] == 'ood_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "img_drop_k = [k for k in RE if np.sum(np.array(RE[k]) >= 0.3)<3]\n",
    "print(len(img_drop_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_clean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n",
      "3499\n",
      "3999\n",
      "4499\n",
      "4999\n",
      "5499\n",
      "5683\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "## 同一个sample单独，最后取avg/max, img_drop_k 里面自动忽略，RE<0.3的 full sentence 忽略\n",
    "eval_f = Evaluate()\n",
    "bleu4_img_clean = {}\n",
    "RE_img_clean = {}\n",
    "mul_img_clean = {}\n",
    "drop = 0\n",
    "for k in img_dataset:\n",
    "    if not 'test' in img_dataset[k]['split']: continue\n",
    "    #if k in img_drop_k:\n",
    "        #drop += 1\n",
    "        #continue\n",
    "    bleu4_img_clean[k] = []\n",
    "    RE_img_clean[k] = []\n",
    "    mul_img_clean[k] = []\n",
    "    datum = img_dataset[k]\n",
    "    Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    for i in range(len(all_A)):\n",
    "        Q = datum['Q'].replace('\"', \"\")\n",
    "        C = [all_A[i]]\n",
    "        A_list = all_A[:i] + all_A[i+1:]\n",
    "        scores = eval_f.evaluate(cand=[C], ref=[A_list], return_scores=True)\n",
    "        #print(scores)\n",
    "        \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A)\n",
    "        if RE_avg<0.3: continue\n",
    "        bleu4_img_clean[k].append(scores['Bleu_4'])\n",
    "        RE_img_clean[k].append(RE_avg)\n",
    "        mul_img_clean[k].append(RE_avg * scores['Bleu_4'])\n",
    "    if len(RE_img_clean[k]) < 3: \n",
    "        drop += 1\n",
    "        del RE_img_clean[k]\n",
    "        del mul_img_clean[k]\n",
    "        del bleu4_img_clean[k]\n",
    "        \n",
    "    if len(RE_img_clean) % 500 == 499: print(len(RE_img_clean))\n",
    "assert len(RE_img_clean) == len(mul_img_clean) == len(bleu4_img_clean)\n",
    "print(len(RE_img_clean))\n",
    "print(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 2543, 5: 1296, 4: 1171, 3: 673})\n",
      "max RE:  0.9940494712742778\n",
      "max mul:  0.9029879466059659\n",
      "max bleu4:  0.9231728016666029\n",
      "mean RE:  0.9533923859689158\n",
      "mean mul:  0.6240687179944258\n",
      "mean bleu4:  0.6504880563907047\n"
     ]
    }
   ],
   "source": [
    "# 0.3 threshold\n",
    "print(Counter([len(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "print(\"max RE: \", np.mean([max(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "print(\"max mul: \", np.mean([max(mul_img_clean[k]) for k in mul_img_clean]))\n",
    "print(\"max bleu4: \", np.mean([max(bleu4_img_clean[k]) for k in bleu4_img_clean]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_img_clean[k]) for k in mul_img_clean]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4_img_clean[k]) for k in bleu4_img_clean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n",
      "3499\n",
      "3999\n",
      "4499\n",
      "4499\n",
      "4999\n",
      "5499\n",
      "5567\n",
      "866\n"
     ]
    }
   ],
   "source": [
    "## 同一个sample单独，最后取avg/max, img_drop_k 里面自动忽略，RE<0.3的 full sentence 忽略\n",
    "eval_f = Evaluate()\n",
    "bleu4_img_clean = {}\n",
    "RE_img_clean = {}\n",
    "mul_img_clean = {}\n",
    "drop = 0\n",
    "for k in img_dataset:\n",
    "    if not 'test' in img_dataset[k]['split']: continue\n",
    "    #if k in img_drop_k:\n",
    "        #drop += 1\n",
    "        #continue\n",
    "    bleu4_img_clean[k] = []\n",
    "    RE_img_clean[k] = []\n",
    "    mul_img_clean[k] = []\n",
    "    datum = img_dataset[k]\n",
    "    Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    for i in range(len(all_A)):\n",
    "        Q = datum['Q'].replace('\"', \"\")\n",
    "        C = [all_A[i]]\n",
    "        A_list = all_A[:i] + all_A[i+1:]\n",
    "        scores = eval_f.evaluate(cand=[C], ref=[A_list], return_scores=True)\n",
    "        #print(scores)\n",
    "        \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A)\n",
    "        if RE_avg<0.5: continue\n",
    "        bleu4_img_clean[k].append(scores['Bleu_4'])\n",
    "        RE_img_clean[k].append(RE_avg)\n",
    "        mul_img_clean[k].append(RE_avg * scores['Bleu_4'])\n",
    "    if len(RE_img_clean[k]) < 3: \n",
    "        drop += 1\n",
    "        del RE_img_clean[k]\n",
    "        del mul_img_clean[k]\n",
    "        del bleu4_img_clean[k]\n",
    "        \n",
    "    if len(RE_img_clean) % 500 == 499: print(len(RE_img_clean))\n",
    "assert len(RE_img_clean) == len(mul_img_clean) == len(bleu4_img_clean)\n",
    "print(len(RE_img_clean))\n",
    "print(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 2377, 5: 1302, 4: 1184, 3: 704})\n",
      "max RE:  0.9955401807341809\n",
      "max mul:  0.9109116325133675\n",
      "max bleu4:  0.9252774135697667\n",
      "mean RE:  0.9667703676481375\n",
      "mean mul:  0.6345987014313298\n",
      "mean bleu4:  0.6541056214571918\n"
     ]
    }
   ],
   "source": [
    "# 0.5 threshold\n",
    "print(Counter([len(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "print(\"max RE: \", np.mean([max(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "print(\"max mul: \", np.mean([max(mul_img_clean[k]) for k in mul_img_clean]))\n",
    "print(\"max bleu4: \", np.mean([max(bleu4_img_clean[k]) for k in bleu4_img_clean]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_img_clean[k]) for k in RE_img_clean]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_img_clean[k]) for k in mul_img_clean]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4_img_clean[k]) for k in bleu4_img_clean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max RE:  0.9968079922027291\n",
      "max RE:  0.9947327477318657\n"
     ]
    }
   ],
   "source": [
    "print(\"max RE: \", np.mean([max(RE_img_clean[k]) for k in RE_img_clean if img_dataset[k]['split'] == 'ind_test']))\n",
    "print(\"max RE: \", np.mean([max(RE_img_clean[k]) for k in RE_img_clean if img_dataset[k]['split'] == 'ood_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n",
      "2999\n",
      "3499\n",
      "3499\n",
      "3999\n",
      "4076\n",
      "619\n"
     ]
    }
   ],
   "source": [
    "## 同一个sample单独，最后取avg/max, img_drop_k 里面自动忽略，RE<0.3的 full sentence 忽略\n",
    "eval_f = Evaluate()\n",
    "bleu4_txt_clean = {}\n",
    "RE_txt_clean = {}\n",
    "mul_txt_clean = {}\n",
    "drop = 0\n",
    "for k in txt_dataset:\n",
    "    if not 'test' in txt_dataset[k]['split']: continue\n",
    "    #if k in img_drop_k:\n",
    "        #drop += 1\n",
    "        #continue\n",
    "    bleu4_txt_clean[k] = []\n",
    "    RE_txt_clean[k] = []\n",
    "    mul_txt_clean[k] = []\n",
    "    datum = txt_dataset[k]\n",
    "    Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    for i in range(len(all_A)):\n",
    "        Q = datum['Q'].replace('\"', \"\")\n",
    "        C = [all_A[i]]\n",
    "        A_list = all_A[:i] + all_A[i+1:]\n",
    "        scores = eval_f.evaluate(cand=[C], ref=[A_list], return_scores=True)\n",
    "        #print(scores)\n",
    "        \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A)\n",
    "        if RE_avg<0.5: continue\n",
    "        bleu4_txt_clean[k].append(scores['Bleu_4'])\n",
    "        RE_txt_clean[k].append(RE_avg)\n",
    "        mul_txt_clean[k].append(RE_avg * scores['Bleu_4'])\n",
    "    if len(RE_txt_clean[k]) < 3: \n",
    "        drop += 1\n",
    "        del RE_txt_clean[k]\n",
    "        del mul_txt_clean[k]\n",
    "        del bleu4_txt_clean[k]\n",
    "    if len(RE_txt_clean) % 500 == 499: print(len(RE_txt_clean))\n",
    "assert len(RE_txt_clean) == len(mul_txt_clean) == len(bleu4_txt_clean)\n",
    "print(len(RE_txt_clean))\n",
    "print(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1366, 5: 1343, 4: 874, 3: 493})\n",
      "max RE:  0.9828222069856472\n",
      "max mul:  0.8212597143292607\n",
      "max bleu4:  0.8480550016967062\n",
      "mean RE:  0.9458907476823312\n",
      "mean mul:  0.5068453831403538\n",
      "mean bleu4:  0.5348509314403606\n"
     ]
    }
   ],
   "source": [
    "# 0.5 threshold\n",
    "print(Counter([len(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"max RE: \", np.mean([max(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"max mul: \", np.mean([max(mul_txt_clean[k]) for k in mul_txt_clean]))\n",
    "print(\"max bleu4: \", np.mean([max(bleu4_txt_clean[k]) for k in bleu4_txt_clean]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_txt_clean[k]) for k in mul_txt_clean]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4_txt_clean[k]) for k in bleu4_txt_clean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1483, 5: 1386, 4: 865, 3: 449})\n",
      "max RE:  0.9762826107052714\n",
      "max mul:  0.8114693233465995\n",
      "max bleu4:  0.8479138449716856\n",
      "mean RE:  0.92778444095049\n",
      "mean mul:  0.4953681230624369\n",
      "mean bleu4:  0.5316556978374075\n"
     ]
    }
   ],
   "source": [
    "# 0.3 threshold\n",
    "print(Counter([len(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"max RE: \", np.mean([max(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"max mul: \", np.mean([max(mul_txt_clean[k]) for k in mul_txt_clean]))\n",
    "print(\"max bleu4: \", np.mean([max(bleu4_txt_clean[k]) for k in bleu4_txt_clean]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_txt_clean[k]) for k in mul_txt_clean]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4_txt_clean[k]) for k in bleu4_txt_clean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_human_scores = {'RE': RE_txt, 'mul': mul_txt, 'bleu4': bleu4_txt}\n",
    "pickle.dump(txt_human_scores, open(\"./txt_human_scores.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max RE:  0.9419907587205651\n",
      "max mul:  0.7568339273336601\n",
      "max bleu4:  0.8467212422473528\n",
      "mean RE:  0.7943510636147798\n",
      "mean mul:  0.4195641452808087\n",
      "mean bleu4:  0.5223123738189273\n"
     ]
    }
   ],
   "source": [
    "print(\"max RE: \", np.mean([max(RE_txt[k]) for k in RE_txt]))\n",
    "print(\"max mul: \", np.mean([max(mul_txt[k]) for k in mul_txt]))\n",
    "print(\"max bleu4: \", np.mean([max(bleu4_txt[k]) for k in bleu4_txt]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_txt[k]) for k in RE_txt]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_txt[k]) for k in mul_txt]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4_txt[k]) for k in bleu4_txt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3888\n",
      "[0.36363636363636365, 0.2727272727272727, 0.5454545454545454, 0.36363636363636365, 0.45454545454545453, 0.36363636363636365]\n",
      "[2.1249835507091168e-05, 3.006454568869563e-05, 0.25802943580872184, 4.397856524709696e-09, 0.8282477530793885, 0.7577395671888485]\n",
      "\n",
      "1656\n",
      "[0.14285714285714285, 0.8571428571428571, 0.8571428571428571, 1.0, 0.8571428571428571, 0.8571428571428571]\n",
      "[1.3939047821026512e-11, 0.9999999998101192, 0.8515139815352575, 0.7307717332579815, 0.9255653651854665, 0.6774689750586048]\n",
      "\n",
      "4598\n",
      "[0, 0.09090909090909091, 1.0, 0, 0.18181818181818182]\n",
      "[3.455144512702623e-14, 0.8648454149611199, 0.3082627645885311, 0.4503303523424431, 0.4785543920365968]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in RE_txt:\n",
    "    for i in RE_txt[k]:\n",
    "        if i<0.3 and i>0.0:\n",
    "            if random.random()>0.02: break\n",
    "            print(k)\n",
    "            print(RE_txt[k])\n",
    "            #print(sorted(mul[k], reverse=True))\n",
    "            print(bleu4_txt[k])\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['Descents.',\n",
      "       '\"Anyone directly descended from original tribal enrollees can be part '\n",
      "       'of a lineage organization.\"',\n",
      "       '\"Descendants of a particular person or group of people of historical '\n",
      "       'importance can be part of a lineage organization.\"',\n",
      "       '\"Anyone directly descended from original tribal enrollees could be '\n",
      "       'eligible for tribal enrollment.\"',\n",
      "       '\"People descended from the organization\\'s past members can be part of '\n",
      "       'a lineage organization.\"'],\n",
      " 'BucketId': 1705667512,\n",
      " 'CuratedAnswer': [],\n",
      " 'DistractorImageIds': ['https://media.gettyimages.com/photos/american-flag-waving-with-the-capitol-hill-picture-id1154438278?b=1&k=6&m=1154438278&s=612x612&w=0&h=QG27Vppr-nBdx9F5YP_iPpElJqmd4quyHSMUeZ2CMVU=',\n",
      "                        'https://media.gettyimages.com/photos/aerial-view-of-beirut-lebanon-city-of-beirut-picture-id635844142?b=1&k=6&m=635844142&s=612x612&w=0&h=KGQjPtUk8TY3THB6VrNmjlNW8MFwWZOWyhCRjoRV_Ck=',\n",
      "                        'https://media.gettyimages.com/photos/in-session-picture-id137949252?b=1&k=6&m=137949252&s=612x612&w=0&h=g1GufnHQfoMzeEzHJMPaCXT637f9Ml6XPwQSavJE-6I=',\n",
      "                        'https://media.gettyimages.com/photos/manchester-united-football-stadium-manchester-england-picture-id543684722?b=1&k=6&m=543684722&s=612x612&w=0&h=55UPAtp6pGha17pbI_vCbkKvoNsgylDMrn85r3f7vOM=',\n",
      "                        'https://media.gettyimages.com/photos/east-facade-of-the-manchester-united-football-stadium-with-statue-of-picture-id148931604?b=1&k=6&m=148931604&s=612x612&w=0&h=IvsHwOARa5f4P-7Ca50MR8uctJVUvtPhpGyFfuQlXR0=',\n",
      "                        'https://media.gettyimages.com/photos/hands-of-poor-african-children-asking-for-drinking-water-picture-id600999260?b=1&k=6&m=600999260&s=612x612&w=0&h=_DZCw5PFoA00tBbZhLx4zivEjIeasjZaddPHUzlZC5c=',\n",
      "                        'https://media.gettyimages.com/photos/united-airlines-boeing-777200-n775ua-passenger-plane-departure-at-picture-id1166416423?b=1&k=6&m=1166416423&s=612x612&w=0&h=7053NL5hD9zseTf6q9Jo4EJAPutvD8OQ0jR4i9TsrPE=',\n",
      "                        'https://media.gettyimages.com/vectors/peace-dove-vector-id1090847744?b=1&k=6&m=1090847744&s=612x612&w=0&h=GkOAbS8S2jMhhvC28qcFNN7ys57Sn0r5B14lUXfsxQ0=',\n",
      "                        'https://media.gettyimages.com/photos/capitol-building-with-blue-sky-from-side-view-washington-dc-picture-id1079023448?b=1&k=6&m=1079023448&s=612x612&w=0&h=-8lPCjMZJ3XfypJK5wXaiRGgkfklyXwB5smg0gtisZI=',\n",
      "                        'https://media.gettyimages.com/vectors/peace-sign-vector-id909171424?b=1&k=6&m=909171424&s=612x612&w=0&h=kWfNiJ-5zA2hvtb5oFSyXqD6rL5_DFJ_wVc3sCz9Ihs='],\n",
      " 'GeneratedBy': 'A2MX5YW3DSQ61W',\n",
      " 'Guid': '38e5f45d-2eb0-4344-83ff-b834c3bcca52',\n",
      " 'Keywords_A': '\"descendants of a particular person or group of people of '\n",
      "               'historical importance\"',\n",
      " 'Major_Topic': 'international relations',\n",
      " 'Q': 'Who can be part of a lineage organization?',\n",
      " 'QType': 'text',\n",
      " 'SupportingImageIds': [],\n",
      " 'Topics': ['tribes', 'geneology', 'lineage'],\n",
      " 'TopicsCandidates': [],\n",
      " 'ValidatedBy': ['A3CD3C99T8S6ON', 'APRZ7BR8C0ZMQ', 'A2NMAOYBBJTO0R'],\n",
      " 'img_negFacts': [{'caption': ' Theodore Roosevelt, a member of the '\n",
      "                              'organization, signed its Congressional Charter '\n",
      "                              'in 1906',\n",
      "                   'image_id': 20094400,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/d/d2/Theodore_Roosevelt_%28Nobel_1906%29.png',\n",
      "                   'overlap_scores': '(0.1333, 0.2, 0.1, 0.3333, 0.1538, 0.2)',\n",
      "                   'scores': '(0.08, 0.0, 0.0, 0.08, 0.08)',\n",
      "                   'title': 'Sons of the American Revolution',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Sons_of_the_American_Revolution'},\n",
      "                  {'caption': '\"That Part\". ',\n",
      "                   'image_id': 20010784,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/en/thumb/d/de/ThatPartSchoolboyQ.jpg/220px-ThatPartSchoolboyQ.jpg',\n",
      "                   'overlap_scores': '(0.2, 0.2, 0.2, 0.0714, 0.2308, 0.2)',\n",
      "                   'scores': '(0.14, 0.0, 0.0, 0.14, 0.14)',\n",
      "                   'title': 'That Part',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/That_Part'},\n",
      "                  {'caption': ' Architecture of lineage systems',\n",
      "                   'image_id': 20094401,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Selection_065.png/799px-Selection_065.png',\n",
      "                   'overlap_scores': '(0.25, 0.2, 0.3333, 0.2143, 0.2812, 0.2)',\n",
      "                   'scores': '(0.12, 0.0, 0.0, 0.12, 0.12)',\n",
      "                   'title': 'Data lineage',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Data_lineage'},\n",
      "                  {'caption': ' Countries by World Health Organization '\n",
      "                              'membership status',\n",
      "                   'image_id': 20026019,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/World_Health_Organization_membership_status_map.png/800px-World_Health_Organization_membership_status_map.png',\n",
      "                   'overlap_scores': '(0.1818, 0.2, 0.1667, 0.2619, 0.193, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.09, 0.0, 0.01, 0.09, 0.08)',\n",
      "                   'title': 'World Health Organization',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/World_Health_Organization'},\n",
      "                  {'caption': 'A Quiet Place Part II. Theatrical release '\n",
      "                              'poster',\n",
      "                   'image_id': 20004592,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/en/thumb/0/02/A_Quiet_Place_Part_II.jpg/220px-A_Quiet_Place_Part_II.jpg',\n",
      "                   'overlap_scores': '(0.1429, 0.2, 0.1111, 0.0952, 0.0833, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.08, 0.0, 0.0, 0.08, 0.08)',\n",
      "                   'title': 'A Quiet Place Part II',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/A_Quiet_Place_Part_II'},\n",
      "                  {'caption': 'Part Lies, Part Heart, Part Truth, Part Garbage '\n",
      "                              '1982–2011. ',\n",
      "                   'image_id': 20094223,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/en/9/99/R.E.M._-_Part_Lies%2C_Part_Heart%2C_Part_Truth%2C_Part_Garbage_1982-2011.jpg',\n",
      "                   'overlap_scores': '(0.1, 0.2, 0.0667, 0.0952, 0.0678, '\n",
      "                                     '0.125)',\n",
      "                   'scores': '(0.09, 0.0, 0.0, 0.09, 0.09)',\n",
      "                   'title': 'Part Lies, Part Heart, Part Truth, Part Garbage '\n",
      "                            '1982–2011',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Part_Lies%2C_Part_Heart%2C_Part_Truth%2C_Part_Garbage_1982%E2%80%932011'},\n",
      "                  {'caption': '\\xa0 Timex Datalink Beepwear Pro: a wearable '\n",
      "                              'pager/watch featuring alphanumeric paging '\n",
      "                              'capability. Part of the Timex Datalink family '\n",
      "                              'of watches',\n",
      "                   'image_id': 20010787,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Timex_Datalink_Beepwear_Pro.jpg/532px-Timex_Datalink_Beepwear_Pro.jpg',\n",
      "                   'overlap_scores': '(0.0571, 0.2, 0.0333, 0.1667, 0.05, '\n",
      "                                     '-0.1)',\n",
      "                   'scores': '(0.06, 0.0, 0.0, 0.06, 0.06)',\n",
      "                   'title': 'pager',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/pager'},\n",
      "                  {'caption': ' Alexey Yablokov (left) and Vassili Nesterenko '\n",
      "                              '(farthest right) protesting in front of the '\n",
      "                              'World Health Organization headquarters in '\n",
      "                              'Geneva, Switzerland in 2008.',\n",
      "                   'image_id': 20026035,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Alexei_Yablokov%2C_Rosa_Goncharova%2C_Vassili_Nesterenko.jpg/800px-Alexei_Yablokov%2C_Rosa_Goncharova%2C_Vassili_Nesterenko.jpg',\n",
      "                   'overlap_scores': '(0.0556, 0.2, 0.0323, 0.2619, 0.0683, '\n",
      "                                     '-0.115)',\n",
      "                   'scores': '(0.06, 0.0, 0.0, 0.06, 0.06)',\n",
      "                   'title': 'World Health Organization',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/World_Health_Organization'},\n",
      "                  {'caption': ' A bar from J.S. Bach\\'s \"Fugue No.17 in A '\n",
      "                              'flat\", BWV 862, from Das Wohltemperierte '\n",
      "                              'Clavier (Part I), an example of contrapuntal '\n",
      "                              'polyphony.Play\\xa0(help·info) The two parts, or '\n",
      "                              'voices, on each staff may be distinguished by '\n",
      "                              'the direction of the stems. Play voice 4\\xa0'\n",
      "                              '(help·info), 3\\xa0(help·info), 2\\xa0'\n",
      "                              '(help·info), & 1\\xa0(help·info) separately.',\n",
      "                   'image_id': 20010778,\n",
      "                   'imgUrl': 'https://upload.wikimedia.org/wikipedia/commons/a/a6/BachFugueBar.png',\n",
      "                   'overlap_scores': '(0.02, 0.2, 0.0105, 0.119, 0.0152, '\n",
      "                                     '-1.075)',\n",
      "                   'scores': '(0.03, 0.0, 0.0, 0.03, 0.03)',\n",
      "                   'title': 'Part (music)',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Part_%28music%29'}],\n",
      " 'relevant_pages': 'List of hereditary and lineage organizations || List of '\n",
      "                   'ethnic organizations in the United States || Family '\n",
      "                   'association || Hereditary (film) || Family history society '\n",
      "                   '|| Hereditary Society Community of the United States of '\n",
      "                   'America || Sons of the American Revolution || List of '\n",
      "                   'Mormon family organizations || Association Royale des '\n",
      "                   'Descendants des Lignages de Bruxelles || Daughters of the '\n",
      "                   'Cincinnati || Lineal descendant || Issue (genealogy) || '\n",
      "                   'Legality of incest || Confucius || Priyadarshini Raje '\n",
      "                   'Scindia || Offspring || Aga Khan IV || Coppola family tree '\n",
      "                   '|| The Descendants (novel) || Descendant (2003 film) || '\n",
      "                   'Tokyo Anime Award || Descendents (2008 film) || The '\n",
      "                   'Descendants || The Descendants (2015 film) || Descendants '\n",
      "                   '(franchise) || Descendants (2015 film) || Descendants '\n",
      "                   '(soundtrack) || Descendents || tree structure || hierarchy '\n",
      "                   '|| Descendant (astrology) || Descendance || Ascendant || '\n",
      "                   'Ancestor || liberalism || Liberal parties by country || '\n",
      "                   'Liberal Party || Liberalism (international relations) || '\n",
      "                   'El Liberal || The Liberal || Liberalism (book) || '\n",
      "                   'Conqueror || Liberal, Indiana || Liberal, Kansas || '\n",
      "                   'Liberal, Missouri || Liberal, Oregon || Religious '\n",
      "                   'liberalism || Liberal Christianity || Liberalism and '\n",
      "                   'progressivism within Islam || Liberal Judaism || Liberal '\n",
      "                   'arts (disambiguation) || Neoliberalism || Liberal Wars || '\n",
      "                   'SARS-CoV-2 Gamma variant || Arvo Pärt || World Health '\n",
      "                   \"Organization || Who's Who || Who Killed Who? || Who Made \"\n",
      "                   'Who || Sons of the Revolution || SARS-CoV-2 Theta variant '\n",
      "                   '|| Part Lies, Part Heart, Part Truth, Part Garbage '\n",
      "                   '1982–2011 || Daughters of the American Revolution || Part '\n",
      "                   '(music) || Children of the American Revolution || That '\n",
      "                   'Part || The Who || Part of speech || Part number || '\n",
      "                   'SARS-CoV-2 Delta variant || WhoMadeWho || Doctor Who || '\n",
      "                   'Zayn Malik || Monticello Association || Fear Street Part '\n",
      "                   'Three: 1666 || Data lineage || Who? Who? ministry || A '\n",
      "                   'Quiet Place Part II || Variants of SARS-CoV-2 || Fear '\n",
      "                   'Street Part One: 1994 || Who (pronoun) || Five Ws || '\n",
      "                   'Horton Hears a Who! || How the Grinch Stole Christmas! || '\n",
      "                   \"Who's on First? || Who (film) || Who? (film) || Who \"\n",
      "                   '(album) || Who? (album) || Love This Giant || Silk '\n",
      "                   'Electric || Immortalized || Mouth Moods || Hole in the '\n",
      "                   'Head || Ghostbusters || Loveppears || WHO (AM) || WHO-DT '\n",
      "                   '|| Who (magazine) || Who? (novel) || White House Office || '\n",
      "                   'Washington Homeschool Organization || Jim Neidhart || who '\n",
      "                   '(Unix) || Doctor Who (disambiguation) || Guess Who || Hoo '\n",
      "                   '|| Hu || Woo || Page (paper) || Page (assistance '\n",
      "                   'occupation) || Page (servant) || Page boy (wedding '\n",
      "                   'attendant) || Page (given name) || Page (surname) || Page, '\n",
      "                   'Australian Capital Territory || Division of Page || Pages '\n",
      "                   'River || The Pages || The Pages Conservation Park || Page, '\n",
      "                   'Arizona || Page, Indiana || Page, Minneapolis || Page, '\n",
      "                   'Nebraska || Page, North Dakota || Page, Oklahoma || Page, '\n",
      "                   'Virginia || Page, Washington || Page, West Virginia || '\n",
      "                   'Page Airport (disambiguation) || Page City, Kansas || Page '\n",
      "                   'County || Page Township || Page Valley || Page (computer '\n",
      "                   'memory) || Memory paging || Bank switching || Pagination '\n",
      "                   '|| Multiple buffering || Ogg page || Pages (word '\n",
      "                   'processor) || Web page || pager || Polyacrylamide gel '\n",
      "                   'electrophoresis || SDS-PAGE || Skirt lifter || Page '\n",
      "                   '(novel) || Hannah Weiner || Pages (band) || Barenaked '\n",
      "                   'Ladies || Page (South Korean band) || Page (Swedish band) '\n",
      "                   '|| Pages (EP) || Pages (Bering Strait album) || Pages '\n",
      "                   '(Shane & Shane album) || Julie Feeney || Versailles '\n",
      "                   '(musician) || Pages (Sexy Zone album) || Tangerine || '\n",
      "                   'Public address system || Kimberly-Clark || Presidential '\n",
      "                   'Ambassadors for Global Entrepreneurship || Pakistan & Gulf '\n",
      "                   'Economist || Page Corps || Page Organ Company || Page '\n",
      "                   'playoff system || International Geosphere-Biosphere '\n",
      "                   'Programme || Justice Page || Page boy || Pager '\n",
      "                   '(disambiguation) || Paige',\n",
      " 'split': 'test',\n",
      " 'txt_negFacts': [{'fact': 'On March 17, 2021, Public Health England (PHE) '\n",
      "                           'named Lineage P.3 VUI-21MAR-02.On June 1, 2021, '\n",
      "                           'the World Health Organization (WHO) named lineage '\n",
      "                           'P.3 as Theta variant.',\n",
      "                   'overlap_scores': '(0.1176, 0.6, 0.0652, 0.2619, 0.0663, '\n",
      "                                     '0.61)',\n",
      "                   'scores': '(0.14, 0.0, 0.0, 0.14, 0.14)',\n",
      "                   'title': 'SARS-CoV-2 Theta variant',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/SARS-CoV-2_Theta_variant'},\n",
      "                  {'fact': 'The scope of the data lineage determines the '\n",
      "                           'volume of metadata required to represent its data '\n",
      "                           'lineage. Usually, data governance, and data '\n",
      "                           'management determines the scope of the data '\n",
      "                           'lineage based on their regulations, enterprise '\n",
      "                           'data management strategy, data impact, reporting '\n",
      "                           'attributes, and critical data elements of the '\n",
      "                           'organization.',\n",
      "                   'overlap_scores': '(0.0851, 0.4, 0.0476, 0.3333, 0.0414, '\n",
      "                                     '0.4)',\n",
      "                   'scores': '(0.11, 0.0, 0.0, 0.11, 0.11)',\n",
      "                   'title': 'Data lineage',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Data_lineage'},\n",
      "                  {'fact': 'These systems incur low capture overheads due to '\n",
      "                           'the small amount of lineage they capture. However, '\n",
      "                           'to answer fine-grain tracing queries, they must '\n",
      "                           'replay the data flow on all (or a large part) of '\n",
      "                           'its input and collect fine-grain lineage during '\n",
      "                           'the replay.',\n",
      "                   'overlap_scores': '(0.0889, 0.4, 0.05, 0.2143, 0.0352, 0.4)',\n",
      "                   'scores': '(0.1, 0.0, 0.0, 0.1, 0.1)',\n",
      "                   'title': 'Data lineage',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Data_lineage'},\n",
      "                  {'fact': 'However, to answer fine-grain tracing queries, '\n",
      "                           'they must replay the data flow on all (or a large '\n",
      "                           'part) of its input and collect fine-grain lineage '\n",
      "                           'during the replay. This approach is suitable for '\n",
      "                           'forensic systems, where a user wants to debug an '\n",
      "                           'observed bad output.',\n",
      "                   'overlap_scores': '(0.087, 0.4, 0.0488, 0.2143, 0.034, 0.4)',\n",
      "                   'scores': '(0.09, 0.0, 0.0, 0.09, 0.09)',\n",
      "                   'title': 'Data lineage',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Data_lineage'},\n",
      "                  {'fact': 'In part as a result of the successes of the '\n",
      "                           'Conferences, the Pan-American Sanitary Bureau '\n",
      "                           \"(1902), and the Office International d'Hygiène \"\n",
      "                           'Publique (1907) were soon founded. When the League '\n",
      "                           'of Nations was formed in 1920, they established '\n",
      "                           'the Health Organization of the League of Nations.',\n",
      "                   'overlap_scores': '(0.0889, 0.4, 0.05, 0.2619, 0.0386, 0.4)',\n",
      "                   'scores': '(0.08, 0.0, 0.0, 0.08, 0.08)',\n",
      "                   'title': 'World Health Organization',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/World_Health_Organization'},\n",
      "                  {'fact': 'However, B.1.1.248 later lost its status as a '\n",
      "                           'distinct lineage and was reclassified to B.1.1.28. '\n",
      "                           'P.1 has also been called B.1.1.28.1, while P.2 has '\n",
      "                           'been B.1.1.28.2 or VUI-202101/01.',\n",
      "                   'overlap_scores': '(0.0312, 0.2, 0.0169, 0.2143, 0.0497, '\n",
      "                                     '0.34)',\n",
      "                   'scores': '(0.07, 0.0, 0.0, 0.07, 0.07)',\n",
      "                   'title': 'SARS-CoV-2 Gamma variant',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/SARS-CoV-2_Gamma_variant'},\n",
      "                  {'fact': ' Lineage can be captured at the level of the job, '\n",
      "                           'using files and giving lineage tuples of form {IF '\n",
      "                           'i, M RJob, OF i }, lineage can also be captured at '\n",
      "                           'the level of each task, using records and giving, '\n",
      "                           'for example, lineage tuples of form {(k rr, v rr '\n",
      "                           '), map, (k m, v m )}.',\n",
      "                   'overlap_scores': '(0.0323, 0.2, 0.0175, 0.2143, 0.0332, '\n",
      "                                     '0.32)',\n",
      "                   'scores': '(0.1, 0.0, 0.0, 0.1, 0.1)',\n",
      "                   'title': 'Data lineage',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Data_lineage'},\n",
      "                  {'fact': 'Aries rising gives out a well-organized, slightly '\n",
      "                           'military bearing which makes them fit for any kind '\n",
      "                           'of military or civil service organization.',\n",
      "                   'overlap_scores': '(0.0833, 0.2, 0.0526, 0.3333, 0.0979, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.08, 0.0, 0.0, 0.08, 0.08)',\n",
      "                   'title': 'Ascendant',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Ascendant'},\n",
      "                  {'fact': 'The effect of the ascendant varies according to '\n",
      "                           'the modes of the sign in which it is placed. These '\n",
      "                           'relate to the quality of the part of the season '\n",
      "                           'they occur in.',\n",
      "                   'overlap_scores': '(0.087, 0.2, 0.0556, 0.2381, 0.0621, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.08, 0.0, 0.0, 0.08, 0.08)',\n",
      "                   'title': 'Ascendant',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Ascendant'},\n",
      "                  {'fact': 'The organization operates under the governance of '\n",
      "                           'a Council (Board of Directors) and a Cabinet '\n",
      "                           '(Advisory Board). The Council consists of fifteen '\n",
      "                           'individuals.',\n",
      "                   'overlap_scores': '(0.0769, 0.2, 0.0476, 0.3333, 0.0892, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.07, 0.0, 0.0, 0.07, 0.07)',\n",
      "                   'title': 'Hereditary Society Community of the United States '\n",
      "                            'of America',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Hereditary_Society_Community_of_the_United_States_of_America'},\n",
      "                  {'fact': 'According to Kulish (2002), almost every system of '\n",
      "                           'organization applied to the world is arranged '\n",
      "                           'hierarchically. By their common definitions, every '\n",
      "                           'nation has a government and every government is '\n",
      "                           'hierarchical.',\n",
      "                   'overlap_scores': '(0.0606, 0.2, 0.0357, 0.3095, 0.0622, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.07, 0.0, 0.0, 0.07, 0.07)',\n",
      "                   'title': 'hierarchy',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/hierarchy'},\n",
      "                  {'fact': 'For these reasons, according to many commentators, '\n",
      "                           \"Confucius's teachings may be considered a Chinese \"\n",
      "                           'example of humanism. Notice the implication that '\n",
      "                           'animal lives are part of property.',\n",
      "                   'overlap_scores': '(0.0645, 0.2, 0.0385, 0.2381, 0.0543, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.06, 0.0, 0.0, 0.06, 0.06)',\n",
      "                   'title': 'Confucius',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Confucius'},\n",
      "                  {'fact': 'We spent a good part of my adult life being '\n",
      "                           'somewhat estranged from each other. He became ill '\n",
      "                           'and I took care of him for a little while.',\n",
      "                   'overlap_scores': '(0.0909, 0.2, 0.0588, 0.2143, 0.0662, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.06, 0.0, 0.0, 0.06, 0.06)',\n",
      "                   'title': 'Descendents',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Descendents'},\n",
      "                  {'fact': 'A flat hierarchy (also known for companies as flat '\n",
      "                           'organization) is a branching hierarchy in which '\n",
      "                           'the maximum degree approaches infinity, i.e., that '\n",
      "                           'has a wide span. Most often, systems intuitively '\n",
      "                           'regarded as hierarchical have at most a moderate '\n",
      "                           'span.',\n",
      "                   'overlap_scores': '(0.05, 0.2, 0.0286, 0.3095, 0.0514, 0.2)',\n",
      "                   'scores': '(0.06, 0.0, 0.0, 0.06, 0.06)',\n",
      "                   'title': 'hierarchy',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/hierarchy'},\n",
      "                  {'fact': 'Lineage capture systems must also be fault '\n",
      "                           'tolerant to avoid rerunning data flows to capture '\n",
      "                           'lineage. At the same time, they must also '\n",
      "                           'accommodate failures in the DISC system.',\n",
      "                   'overlap_scores': '(0.0645, 0.2, 0.0385, 0.1905, 0.0457, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.12, 0.0, 0.0, 0.12, 0.12)',\n",
      "                   'title': 'Data lineage',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Data_lineage'},\n",
      "                  {'fact': 'Part-writing (or voice leading) is the composition '\n",
      "                           'of parts in consideration of harmony and '\n",
      "                           'counterpoint. In the context of polyphonic '\n",
      "                           'composition the term voice may be used instead of '\n",
      "                           'part to denote a single melodic line or textural '\n",
      "                           'layer.',\n",
      "                   'overlap_scores': '(0.0526, 0.2, 0.0303, 0.1429, 0.025, '\n",
      "                                     '0.2)',\n",
      "                   'scores': '(0.1, 0.0, 0.0, 0.1, 0.1)',\n",
      "                   'title': 'Part (music)',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Part_%28music%29'}],\n",
      " 'txt_posFacts': [{'fact': 'This is a list of notable hereditary and lineage '\n",
      "                           'organizations. It includes societies that limit '\n",
      "                           'their membership to those who meet group inclusion '\n",
      "                           'criteria, such as descendants of a particular '\n",
      "                           'person or group of people of historical '\n",
      "                           'importance. It does not include general ethnic '\n",
      "                           'heritage societies.',\n",
      "                   'title': 'List of hereditary and lineage organizations - '\n",
      "                            'Wikipedia',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/List_of_hereditary_and_lineage_organizations'},\n",
      "                  {'fact': 'Lineal descent means that anyone directly '\n",
      "                           'descended from original tribal enrollees could be '\n",
      "                           'eligible for tribal enrollment, regardless of how '\n",
      "                           'much native blood they have. The antonym of '\n",
      "                           'descendant is antecedent .',\n",
      "                   'title': 'Lineal descendant - Wikipedia',\n",
      "                   'url': 'https://en.wikipedia.org/wiki/Lineal_descendant'}],\n",
      " 'type': 61,\n",
      " 'word_lists': {'answerwords': 'descents || Descents',\n",
      "                'goldfactwords': 'Descendants || particular || antonym || '\n",
      "                                 'importance || ethnic || descent || Notable '\n",
      "                                 '|| lineal || person || Ethnic || Descendant '\n",
      "                                 '|| Importance || Much || Historical || '\n",
      "                                 'Organizations || group || societies || '\n",
      "                                 'descendants || antecedent || blood || '\n",
      "                                 'Membership || notable || people || Enrollees '\n",
      "                                 '|| general || Enrollment || Lineal || '\n",
      "                                 'Descent || Heritage || Particular || much || '\n",
      "                                 'criteria || hereditary || descendant || '\n",
      "                                 'historical || Societies || Group || Antonym '\n",
      "                                 '|| Person || original || Blood || Tribal || '\n",
      "                                 'Such || Antecedent || Native || Inclusion || '\n",
      "                                 'organizations || membership || enrollees || '\n",
      "                                 'People || heritage || inclusion || tribal || '\n",
      "                                 'native || List || enrollment || General || '\n",
      "                                 'Original || Eligible || eligible || list || '\n",
      "                                 'Criteria || such || Hereditary',\n",
      "                'keywords': 'lineage || part || Lineage || Part || '\n",
      "                            'Organization || organization',\n",
      "                'titlewords': 'lineage || descendant || Lineal || List || '\n",
      "                              'organizations || hereditary'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(txt_dataset['4598'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({5: 1167, 6: 962, 4: 937, 3: 582, 0: 408, 2: 362, 1: 277})\n",
      "Counter({5: 1229, 6: 1099, 4: 941, 3: 575, 2: 330, 0: 285, 1: 236})\n"
     ]
    }
   ],
   "source": [
    "print(Counter([np.sum(np.array(RE_txt[k]) > 0.75) for k in RE_txt if txt_dataset[k]['split'] == 'test']))\n",
    "print(Counter([np.sum(np.array(RE_txt[k]) > 0.5) for k in RE_txt if txt_dataset[k]['split'] == 'test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 548703, 'reflen': 553081, 'guess': [548703, 510365, 472027, 433689], 'correct': [482229, 397449, 332196, 279545]}\n",
      "ratio: 0.9920843420764751\n",
      "Bleu_1\n",
      "Bleu_2\n",
      "Bleu_3\n",
      "Bleu_4\n",
      "ROUGE_L\n",
      "Bleu_1:\t 0.8718681923526563\n",
      "Bleu_2:\t 0.8207163809672817\n",
      "Bleu_3:\t 0.7776476582958962\n",
      "Bleu_4:\t 0.7405240931886949\n",
      "ROUGE_L: 0.775898078614296\n"
     ]
    }
   ],
   "source": [
    "eval_f = Evaluate()\n",
    "scores = eval_f.evaluate(cand=C, ref=A_list, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_avg = 0.18892881294338235\n",
      "F1_max = 0.18892881294338235\n",
      "EM = 0.00013041890552454484\n",
      "RE_avg = 0.7355976452156661\n",
      "PR_avg = 0.11738792355336282\n",
      "RE * BLEU4 = 0.5447277791750704\n"
     ]
    }
   ],
   "source": [
    "# SQuAD style vqa eval: EM, F1\n",
    "F1_avg_scores = []\n",
    "F1_max_scores = []\n",
    "EM_scores = []\n",
    "RE_scores = []\n",
    "PR_scores = []\n",
    "#F1_avg_bertscores = []\n",
    "#F1_max_bertscores = []\n",
    "for cands, a in zip(C, Keywords_A):\n",
    "    assert len(cands)==1\n",
    "    F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([cands[0]], a)\n",
    "    F1_avg_scores.append(F1_avg)\n",
    "    F1_max_scores.append(F1_max)\n",
    "    EM_scores.append(EM)\n",
    "    RE_scores.append(RE_avg)\n",
    "    PR_scores.append(PR_avg)\n",
    "\n",
    "    #F1_avg_bertscore, F1_max_bertscore = compute_bertscore([cands[0]], a)\n",
    "    #F1_avg_bertscores.append(F1_avg_bertscore)\n",
    "    #F1_max_bertscores.append(F1_max_bertscore)\n",
    "\n",
    "F1_avg = np.mean(F1_avg_scores)\n",
    "F1_max = np.mean(F1_max_scores)\n",
    "EM = np.mean(EM_scores)\n",
    "RE_avg = np.mean(RE_scores)\n",
    "PR_avg = np.mean(PR_scores)\n",
    "\n",
    "#F1_avg_bertscore = np.mean(F1_avg_bertscores)\n",
    "#F1_max_bertscore = np.mean(F1_max_bertscores)\n",
    "print(\"F1_avg = {}\".format(F1_avg))\n",
    "print(\"F1_max = {}\".format(F1_max))\n",
    "print(\"EM = {}\".format(EM))\n",
    "print(\"RE_avg = {}\".format(RE_avg))\n",
    "print(\"PR_avg = {}\".format(PR_avg))\n",
    "\n",
    "#print(\"F1_avg_bertscore = {}\".format(F1_avg_bertscore))\n",
    "#print(\"F1_max_bertscore = {}\".format(F1_max_bertscore))\n",
    "\n",
    "print(\"RE * BLEU4 = {}\".format(RE_avg * scores['Bleu_4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25015\n"
     ]
    }
   ],
   "source": [
    "Q = []\n",
    "A_list = []\n",
    "C = []\n",
    "Keywords_A = []\n",
    "for k in txt_dataset:\n",
    "    if not 'test' in txt_dataset[k]['split']: continue\n",
    "    datum = txt_dataset[k]\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    for i in range(len(all_A)):\n",
    "        Q.append(datum['Q'].replace('\"', \"\"))\n",
    "        C.append([all_A[i]])\n",
    "        A_list.append(all_A[:i] + all_A[i+1:])\n",
    "        Keywords_A.append(datum['Keywords_A'].replace('\"', \"\"))\n",
    "assert len(C) == len(Q) == len(A_list) == len(Keywords_A)\n",
    "print(len(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 365753, 'reflen': 368388, 'guess': [365753, 340738, 317201, 294375], 'correct': [302169, 244036, 202037, 168574]}\n",
      "ratio: 0.9928472154358965\n",
      "Bleu_1\n",
      "Bleu_2\n",
      "Bleu_3\n",
      "Bleu_4\n",
      "ROUGE_L\n",
      "Bleu_1:\t 0.820225403511018\n",
      "Bleu_2:\t 0.7636931139008422\n",
      "Bleu_3:\t 0.7171364099147381\n",
      "Bleu_4:\t 0.6766927878708912\n",
      "ROUGE_L: 0.6734202548341871\n"
     ]
    }
   ],
   "source": [
    "eval_f = Evaluate()\n",
    "scores = eval_f.evaluate(cand=C, ref=A_list, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_avg = 0.2933705406243417\n",
      "F1_max = 0.2933705406243417\n",
      "EM = 0.07567459524285429\n",
      "RE_avg = 0.7901031055052431\n",
      "PR_avg = 0.22620708177082618\n",
      "RE * BLEU4 = 0.5346570731697918\n"
     ]
    }
   ],
   "source": [
    "# SQuAD style vqa eval: EM, F1\n",
    "F1_avg_scores = []\n",
    "F1_max_scores = []\n",
    "EM_scores = []\n",
    "RE_scores = []\n",
    "PR_scores = []\n",
    "#F1_avg_bertscores = []\n",
    "#F1_max_bertscores = []\n",
    "for cands, a in zip(C, Keywords_A):\n",
    "    assert len(cands)==1\n",
    "    F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics([cands[0]], a)\n",
    "    F1_avg_scores.append(F1_avg)\n",
    "    F1_max_scores.append(F1_max)\n",
    "    EM_scores.append(EM)\n",
    "    RE_scores.append(RE_avg)\n",
    "    PR_scores.append(PR_avg)\n",
    "\n",
    "    #F1_avg_bertscore, F1_max_bertscore = compute_bertscore([cands[0]], a)\n",
    "    #F1_avg_bertscores.append(F1_avg_bertscore)\n",
    "    #F1_max_bertscores.append(F1_max_bertscore)\n",
    "\n",
    "F1_avg = np.mean(F1_avg_scores)\n",
    "F1_max = np.mean(F1_max_scores)\n",
    "EM = np.mean(EM_scores)\n",
    "RE_avg = np.mean(RE_scores)\n",
    "PR_avg = np.mean(PR_scores)\n",
    "\n",
    "#F1_avg_bertscore = np.mean(F1_avg_bertscores)\n",
    "#F1_max_bertscore = np.mean(F1_max_bertscores)\n",
    "print(\"F1_avg = {}\".format(F1_avg))\n",
    "print(\"F1_max = {}\".format(F1_max))\n",
    "print(\"EM = {}\".format(EM))\n",
    "print(\"RE_avg = {}\".format(RE_avg))\n",
    "print(\"PR_avg = {}\".format(PR_avg))\n",
    "\n",
    "#print(\"F1_avg_bertscore = {}\".format(F1_avg_bertscore))\n",
    "#print(\"F1_max_bertscore = {}\".format(F1_max_bertscore))\n",
    "\n",
    "print(\"RE * BLEU4 = {}\".format(RE_avg * scores['Bleu_4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
