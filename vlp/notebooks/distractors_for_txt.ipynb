{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, time, os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import copy\n",
    "import string\n",
    "import wikipedia\n",
    "import spacy\n",
    "from itertools import tee\n",
    "import pylcs\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'http://en.wikipedia.org/w/api.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT_LIST = [\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "            \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n",
    "            \"Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_blocklist = ['seal', 'sign ', 'pdf', 'gif', 'icon', 'notice', 'cartoon', 'publish', 'menu', 'logo', 'svg', 'webm', 'page', \\\n",
    "                     'ogg', 'flickr', 'poster', 'ogv', 'banner', 'tif', 'montage', 'centralautologin', 'footer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['NUM', 'NOUN', 'ADJ', 'PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*|\\(|\\)|-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATIONS = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7921\n"
     ]
    }
   ],
   "source": [
    "new_txt_data = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/output_mine_all_schema.json\", \"r\"))\n",
    "print(len(new_txt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(A, B):\n",
    "    intersection = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    return round(intersection / (union+1e-7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wiki_request(params):\n",
    "  \n",
    "    global USER_AGENT_LIST\n",
    "\n",
    "    params['format'] = 'json'\n",
    "    if not 'action' in params:\n",
    "        params['action'] = 'query'\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(USER_AGENT_LIST)#'wikipedia (https://github.com/goldsmith/Wikipedia/)'\n",
    "    }\n",
    "\n",
    "    r = requests.get(API_URL, params=params, headers=headers)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content(title):\n",
    "\n",
    "    query_params = {\n",
    "        'prop': 'extracts|revisions',\n",
    "        'explaintext': '',\n",
    "        'rvprop': 'ids',\n",
    "        'titles': title\n",
    "    }\n",
    "    request = _wiki_request(query_params)\n",
    "    result = request['query']['pages']\n",
    "    content = result[list(result.keys())[0]]['extract']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "19\n",
      "29\n",
      "39\n",
      "49\n",
      "59\n",
      "69\n",
      "79\n",
      "89\n",
      "99\n",
      "40.43351101875305\n"
     ]
    }
   ],
   "source": [
    "### Takeaway: get html from wikipedia pypi is somehow pretty slow. But the library gives cleaner content\n",
    "### Decision: html is to be obtained from url request; \n",
    "###           content is to be obtained from the (stolen) library function, with USER_AGENT_LIST (using the wiki github user agent doesn't make much difference)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    if i%10 == 9: print(i)\n",
    "    url = \"https://en.wikipedia.org/wiki/Egyptian_tombs\"\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        html = f.read().decode('utf-8')\n",
    "    end_indx = html.find('<h2><span class=\"mw-headline\" id=\"References\">References</span>')\n",
    "    html = html[:end_indx]\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    cont = content(\"Egyptian tombs\") # The stolen function works faster than wikipedia.page().content\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        html = f.read().decode('utf-8')\n",
    "    end_indx = html.find('<h2><span class=\"mw-headline\" id=\"References\">References</span>')\n",
    "    html = html[:end_indx]\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrap imgs and their captions. \n",
    "def get_imgs_and_captions(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('img')\n",
    "    imgUrls = []\n",
    "    captions = []\n",
    "    for l in links:\n",
    "        imgUrl = l.get('src')\n",
    "        try: \n",
    "            width = int(l['width'])\n",
    "            height = int(l['height'])\n",
    "        except:\n",
    "            continue\n",
    "        if width<100 or height<100:\n",
    "            continue\n",
    "        if any(b in imgUrl.lower() for b in url_blocklist): \n",
    "            continue\n",
    "        imgUrls.append(imgUrl)\n",
    "\n",
    "        # Special case for thumb images, which are located inside a table\n",
    "        thumbinner_div = l.find_parent(\"div\", class_='thumbinner')\n",
    "        if thumbinner_div: \n",
    "            captions.append(thumbinner_div.text)\n",
    "            continue\n",
    "        \n",
    "        segments = []\n",
    "        prev_th = l.find_previous('th')\n",
    "        if prev_th: segments.append(prev_th.get_text(strip=True))\n",
    "            \n",
    "        tr_parent = l.find_parent('tr')\n",
    "        if tr_parent: segments.append(tr_parent.get_text(strip=True))\n",
    "\n",
    "        captions.append(\". \".join(segments))\n",
    "    return imgUrls, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pages_by_hyperlink(keywords, url):\n",
    "    if 'en.wikipedia.org' not in url: return {}\n",
    "    anchor2page = {}\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    try: \n",
    "        with urllib.request.urlopen(req) as f:\n",
    "            html = f.read().decode('utf-8')\n",
    "    except:\n",
    "        return anchor2page\n",
    "    end_indx = html.find('<h2><span class=\"mw-headline\" id=\"References\">References</span>')\n",
    "    html = html[:end_indx]\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('a', attrs={'href': re.compile(\"^/wiki/(?!.*(:|\\(identifier\\))).*\")})\n",
    "    for link in links:\n",
    "        title = link.get('title')\n",
    "        text = link.text\n",
    "        if title is None or not text: continue\n",
    "        #print(link)\n",
    "        if any(b in keywords for b in title.split()):\n",
    "            pagelink = 'https://en.wikipedia.org' + link.get('href')\n",
    "            if pagelink.find(\"#\") > -1:\n",
    "                pagelink = pagelink[:pagelink.find(\"#\")]\n",
    "            anchor2page[__load(title)] = pagelink\n",
    "        if len(text) == 0: print(link)\n",
    "        elif pylcs.lcs(title.lower(), text.lower())/len(text) < 0.85:\n",
    "            if any(b in keywords for b in text.split()):\n",
    "                pagelink = 'https://en.wikipedia.org' + link.get('href')\n",
    "                if pagelink.find(\"#\") > -1:\n",
    "                    pagelink = pagelink[:pagelink.find(\"#\")]\n",
    "                anchor2page[__load(title)] = pagelink\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    return anchor2page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_and_relevant_pages_from_txt_sample(k):\n",
    "    Q = new_txt_data[str(k)]['Q']\n",
    "    doc = nlp(Q)\n",
    "    keywords = set([t.text for s in doc.sents for t in s if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    titlewords = set()\n",
    "    anchor2page = {}\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        if not 'en.wikipedia.org/wiki/' in f['url']: continue\n",
    "        fact_title_raw = ' '.join(urllib.parse.unquote(f['url']).split('/')[-1].split('_'))\n",
    "        fact_title = pattern.sub('', fact_title_raw)\n",
    "        titlewords = titlewords.union(fact_title.split())\n",
    "        for title in wikipedia.search(fact_title_raw):\n",
    "            anchor2page[__load(title)] = \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(\"_\".join(title.split()))\n",
    "    keywords = keywords - PUNCTUATIONS\n",
    "    keywords = set(sum([[w.capitalize(), w.lower()] for w in keywords], []))\n",
    "    titlewords = titlewords - PUNCTUATIONS\n",
    "    \n",
    "    print(\"#pages before extension by hyperlink: \", len(anchor2page))\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        d = find_pages_by_hyperlink(keywords.union(titlewords), urllib.parse.quote(f['url'], safe='://', encoding=None, errors=None))\n",
    "        anchor2page.update(d)\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    print(\"#pages after extension by hyperlink: \", len(anchor2page))\n",
    "    \n",
    "    A = new_txt_data[str(k)]['A']\n",
    "    doc = nlp(A)\n",
    "    answerwords = set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())]) - keywords\n",
    "    answerwords = answerwords - PUNCTUATIONS\n",
    "    answerwords = set(sum([[w.capitalize(), w.lower()] for w in answerwords], []))\n",
    "    goldfactwords = set()\n",
    "    Q_A_words = keywords.union(answerwords)\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        doc = nlp(f['fact'])\n",
    "        goldfactwords = goldfactwords.union(set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())]) - Q_A_words)\n",
    "    goldfactwords = goldfactwords - PUNCTUATIONS\n",
    "    goldfactwords = set(sum([[w.capitalize(), w.lower()] for w in goldfactwords], []))\n",
    "    \n",
    "    for a in list(anchor2page.keys()):\n",
    "        if is_disambiguation_page(anchor2page[a]):\n",
    "            print(a, \" is an disambiguation page\")\n",
    "            for t in recover_disambiguation_page(a):\n",
    "                try: anchor2page[__load(t)] = \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(\"_\".join(t.split())) \n",
    "                ### Rarely get some keywordError cuz the returned value from _wiki_request is missing a certain field\n",
    "                except: pass\n",
    "            del anchor2page[a]\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    return titlewords, keywords, goldfactwords, answerwords, Q, A, anchor2page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_categories(title):\n",
    "    url = 'https://en.wikipedia.org/w/api.php?format=xml&action=query&prop=categories&titles='+urllib.parse.quote(title)\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        xml = f.read().decode('utf-8')\n",
    "    soup= BeautifulSoup(xml,\"lxml-xml\")\n",
    "    tags = soup.find('categories')\n",
    "    if tags is None: return []\n",
    "    categories = [c.get('title').replace(\"Category:\", \"\") for c in tags]\n",
    "    return categories\n",
    "def is_disambiguation_page(url):\n",
    "    title = url.split('/')[-1]\n",
    "    return 'disambiguation' in \" \".join(get_page_categories(title))\n",
    "def recover_disambiguation_page(title):\n",
    "    try: \n",
    "        wikipedia.page(title)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        return err[err.find(\"may refer to:\")+13:].strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load(title):\n",
    "    '''\n",
    "    Load basic information from Wikipedia.\n",
    "    Confirm that page exists and is not a disambiguation/redirect.\n",
    "    Does not need to be called manually, should be called automatically during __init__.\n",
    "    '''\n",
    "    query_params = {\n",
    "        'prop': 'info|pageprops',\n",
    "        'inprop': 'url',\n",
    "        'ppprop': 'disambiguation',\n",
    "        'redirects': '',\n",
    "        'titles': title\n",
    "    }\n",
    "\n",
    "    request = _wiki_request(query_params)\n",
    "\n",
    "    query = request['query']\n",
    "    pageid = list(query['pages'].keys())[0]\n",
    "    page = query['pages'][pageid]\n",
    "\n",
    "    # missing is present if the page is missing\n",
    "    if 'missing' in page:\n",
    "        print(\"Page is missing: \", title)\n",
    "        return \"\"\n",
    "\n",
    "    # same thing for redirect, except it shows up in query instead of page for whatever silly reason\n",
    "    elif 'redirects' in query:\n",
    "        redirects = query['redirects'][0]\n",
    "        return redirects['to']\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_from_page(title, page, keywords, answerwords, goldfactwords):\n",
    "    try: \n",
    "        cont = content(title)\n",
    "        paragraphs = cont[:cont.find('== References ==')].split('\\n')\n",
    "        \n",
    "    except: \n",
    "        print(\"Exception from find_sentences_from_page, title = \", title)\n",
    "        return {}\n",
    "    sen2score = {}\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            doc = nlp(p)\n",
    "            for s in doc.sents:\n",
    "                if len(s) < 10: \n",
    "                    continue\n",
    "                nouns_in_s = [t.text for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A > 0.06:\n",
    "                    #print(IoU_Q, IoU_A, s.text)\n",
    "                    IoU_G = IoU(set(nouns_in_s), goldfactwords)\n",
    "                    sen2score[s.text] = {'scores': (IoU_Q, IoU_A, IoU_G, IoU_Q - IoU_A, IoU_Q - IoU_A - IoU_G), 'link': page, 'title': title}\n",
    "\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            doc = nlp(p)\n",
    "            it1, it2 = tee(doc.sents)\n",
    "            next(it2, None)\n",
    "            for s1, s2 in zip(it1, it2):\n",
    "                if len(s1) < 5 or len(s2) < 5 or len(s1)+len(s2) > 70 or len(s1)+len(s2) < 10: \n",
    "                    continue \n",
    "                nouns_in_s = [t.text for s in [s1, s2] for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A >= 0.06:\n",
    "                    #print(IoU_Q, IoU_A, \" \".join([s1.text, s2.text]))\n",
    "                    IoU_G = IoU(set(nouns_in_s), goldfactwords)\n",
    "                    sen2score[\" \".join([s1.text, s2.text])] = {'scores': (IoU_Q, IoU_A, IoU_G, IoU_Q - IoU_A, IoU_Q - IoU_A - IoU_G), 'link': page, 'title': title}\n",
    "                    #print(s)\n",
    "    #print(len(sen2score))\n",
    "    return sen2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_imgs_from_page(title, page, keywords, answerwords, goldfactwords):\n",
    "    try: \n",
    "        html = get_html(page)\n",
    "        imgs, caps = get_imgs_and_captions(html)\n",
    "        \n",
    "    except: \n",
    "        print(\"Exception from find_imgs_from_page, page = \", page)\n",
    "        return {}\n",
    "    \n",
    "    cap2score = {}\n",
    "    for im, cap in zip(imgs, caps):\n",
    "        doc = nlp(cap)\n",
    "        nouns_in_s = [t.text for t in doc if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "        IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "        IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "        if IoU_Q -  IoU_A > 0.0: # 0.06\n",
    "            IoU_G = IoU(set(nouns_in_s), goldfactwords)\n",
    "            cap2score[doc.text] = {'scores': (IoU_Q, IoU_A, IoU_G, IoU_Q - IoU_A, IoU_Q - IoU_A - IoU_G), 'img':im, 'link': page, 'title': title}    \n",
    "    #print(len(cap2score))\n",
    "    return cap2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_from_Q_via_noun_chunks(Q):\n",
    "    doc = nlp(Q)\n",
    "    \n",
    "    ### Extract noun chunks\n",
    "    proper_words = [t.text for s in doc.sents for t in s if t.pos_ in ['NUM', 'PROPN', 'ADJ'] or ((not t.is_sent_start) and t.text[0].isupper())]\n",
    "    chunks = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([n in proper_words for n in chunk.text.split()]):\n",
    "            chunks.add(chunk.text)\n",
    "    if not chunks: \n",
    "        chunks = chunks.union([c.text for c in doc.noun_chunks])\n",
    "        chunks = chunks.union([t.text for s in doc.sents for t in s if t.pos_ == 'PROPN' or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    \n",
    "    pages = set()\n",
    "    for chunk in chunks:\n",
    "        pages = pages.union(set(wikipedia.search(chunk)))\n",
    "    if len(pages) < 5:\n",
    "        more_chunks = set()\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'amod' or token.dep_ == 'compound':\n",
    "                more_chunks.add(doc[token.i: token.head.i+1].text if token.head.i > token.i else doc[token.head.i:token.i+1].text)\n",
    "        more_chunks = more_chunks - chunks\n",
    "        print(Q)\n",
    "        print(\"More chunks: \", more_chunks)\n",
    "        for chunk in more_chunks:\n",
    "            pages = pages.union(wikipedia.search(chunk))\n",
    "        chunks = chunks.union(more_chunks)\n",
    "    \n",
    "    print(\"num of pages: \", len(pages))\n",
    "    anchor2page = {}\n",
    "    for title in pages:\n",
    "        anchor2page[__load(title)] = \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(\"_\".join(title.split()))\n",
    "    for a in list(anchor2page.keys()):\n",
    "        if is_disambiguation_page(anchor2page[a]):\n",
    "            print(a, \" is an disambiguation page\")\n",
    "            for t in recover_disambiguation_page(a):\n",
    "                anchor2page[__load(t)] = \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(\"_\".join(t.split()))\n",
    "            del anchor2page[a]\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    return anchor2page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_get_sen2score_from_indx(k):\n",
    "    print('k = ', k)\n",
    "    titlewords, keywords, goldfactwords, answerwords, Q, A, anchor2page = get_keywords_and_relevant_pages_from_txt_sample(k)\n",
    "    print(\"Q = \", Q)\n",
    "    print(\"A = \", A)\n",
    "    print(\"keywords = \", keywords)\n",
    "    print(\"titlewords = \", titlewords)\n",
    "    print(\"answerwords = \", answerwords)\n",
    "    print(\"goldfactwords = \", goldfactwords)\n",
    "    \n",
    "    new_anchor2page = get_pages_from_Q_via_noun_chunks(Q)\n",
    "    sen2score = {}\n",
    "    cap2score = {}\n",
    "    for title in new_anchor2page:\n",
    "        sen2score.update(find_sentences_from_page(title, new_anchor2page[title], keywords, answerwords, goldfactwords))\n",
    "        cap2score.update(find_imgs_from_page(title, new_anchor2page[title], keywords, answerwords, goldfactwords))\n",
    "    sen2score = dict(sorted(sen2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    cap2score = dict(sorted(cap2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    print(\"total num of sentences found = \", len(sen2score))\n",
    "    print(\"total num of imgs found = \", len(cap2score))\n",
    "    if len(sen2score) == 0:\n",
    "        for title in anchor2page: sen2score.update(find_sentences_from_page(title, anchor2page[title], keywords.union(titlewords), answerwords, goldfactwords))\n",
    "        for title in new_anchor2page: sen2score.update(find_sentences_from_page(title, new_anchor2page[title], keywords.union(titlewords), answerwords, goldfactwords))\n",
    "        print(\"After accepting overlap with titlewords, total num of sentences found = \", len(sen2score))\n",
    "    if len(cap2score) == 0:\n",
    "        for title in anchor2page: cap2score.update(find_imgs_from_page(title, anchor2page[title], keywords.union(titlewords), answerwords, goldfactwords))\n",
    "        for title in new_anchor2page: cap2score.update(find_imgs_from_page(title, new_anchor2page[title], keywords.union(titlewords), answerwords, goldfactwords))\n",
    "        print(\"After accepting overlap with titlewords, total num of imgs found = \", len(cap2score))\n",
    "    \n",
    "    word_lists = (titlewords, keywords, goldfactwords, answerwords)\n",
    "    return sen2score, cap2score, word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sen2score_from_indx(k):\n",
    "    print('k = ', k)\n",
    "    titlewords, keywords, goldfactwords, answerwords, Q, A, anchor2page = get_keywords_and_relevant_pages_from_txt_sample(k)\n",
    "    print(\"Q = \", Q)\n",
    "    print(\"A = \", A)\n",
    "    print(\"keywords = \", keywords)\n",
    "    print(\"titlewords = \", titlewords)\n",
    "    print(\"answerwords = \", answerwords)\n",
    "    print(\"goldfactwords = \", goldfactwords)\n",
    "    \n",
    "    print(\" -- find sentences from each page --\")\n",
    "    sen2score = {}\n",
    "    cap2score = {}\n",
    "    for title in anchor2page:\n",
    "        sen2score.update(find_sentences_from_page(title, anchor2page[title], keywords, answerwords, goldfactwords))\n",
    "        cap2score.update(find_imgs_from_page(title, anchor2page[title], keywords, answerwords, goldfactwords))\n",
    "    sen2score = dict(sorted(sen2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    cap2score = dict(sorted(cap2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    print(\"total num of sentences found = \", len(sen2score))\n",
    "    print(\"total num of imgs found = \", len(cap2score))\n",
    "    \n",
    "    word_lists = (titlewords, keywords, goldfactwords, answerwords)\n",
    "    return sen2score, cap2score, word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words(word_lists, colors, sentence):\n",
    "    s = copy.deepcopy(sentence)\n",
    "    for word_list, color in zip(word_lists, colors):\n",
    "        try: \n",
    "            if \"\".join(word_list): \n",
    "                s = re.sub(r'\\b(' + r'|'.join([re.escape(c) for c in word_list]) + r')\\b\\s*', lambda m: '<span style=\"background-color:rgb{}\">{}</span>'.format(color, m.group()), s)\n",
    "        except: \n",
    "            print(s)\n",
    "            print(word_list)\n",
    "            raise\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_html_row(k, sen2score, cap2score, word_lists, colors = [\"(223, 255, 238)\", \"(193, 239, 253)\", \"(253, 252, 152)\", \"(255, 214, 222)\"]):\n",
    "    html = \"\"\n",
    "    html += '<tr><td>{}.</td><td>Q: {}<br>'.format(k, highlight_words(word_lists, colors, new_txt_data[str(k)]['Q']))\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        html += '<br><br>&nbsp;&nbsp;{}'.format(highlight_words(word_lists, colors, f['fact']))\n",
    "        html += '<a href=\"{}\"> link</a>'.format(f['url'])\n",
    "    html += '<br><br>A: {}<br>'.format(highlight_words(word_lists, colors, new_txt_data[str(k)]['A']))\n",
    "    html += '</td><td>'\n",
    "    s_buckets = defaultdict(lambda: [])\n",
    "    for s in sen2score:\n",
    "        if sen2score[s]['scores'][2] == 0.0 and sen2score[s]['scores'][1] == 0.0 and len(s.split()) in range(22, 60):\n",
    "            s_buckets['good'].append(s)\n",
    "        elif sen2score[s]['scores'][1] > 0.0 or sen2score[s]['scores'][2] > 0.0:\n",
    "            s_buckets['(maybe)falseneg'].append(s)\n",
    "    \n",
    "    for s in s_buckets['good'][:10]:\n",
    "        html += '{} --- {} '.format(highlight_words(word_lists, colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"> {}</a><br><br>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "    html += '<strong> ----------------- (maybe)falseneg ---------------- </strong><br>'\n",
    "    for s in random.sample(s_buckets['(maybe)falseneg'], min(len(s_buckets['(maybe)falseneg']), 5)):\n",
    "        html += '{} --- {} '.format(highlight_words(word_lists, colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"> {}</a><br><br>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "        \n",
    "    html += '</td><td>'\n",
    "    for k in list(cap2score.keys())[:10]:\n",
    "        html += '<a href=\"{}\" class=\"tool-tip\" target=\"_blank\"><img style=\"display:block; max-height:300px; max-width:100%;\" src = \"{}\"></a>'.format(cap2score[k]['img'], cap2score[k]['img'])\n",
    "        html += '<br>Caption: {}<br>{}<br>'.format(highlight_words(word_lists, colors, k), str(cap2score[k]['scores']))\n",
    "        html += '<a href=\"{}\"> {}</a><br><br>'.format(cap2score[k]['link'], cap2score[k]['title'])\n",
    "    html += '</td></tr>'\n",
    "    html += '<tr><td colspan=4><hr></td></tr>'\n",
    "    return html.encode('ascii', 'xmlcharrefreplace').decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  5409\n",
      "#pages before extension by hyperlink:  20\n",
      "#pages after extension by hyperlink:  28\n",
      "Zuni  is an disambiguation page\n",
      "Q =  Can the Zuni and Mitsubishi AAM-4 be used for the same type targets?\n",
      "A =  The Zuni and Mitsubishi AAM-4 can both be used for air-to-air targets, but only the Zuni can be used for air-to-ground targets.\n",
      "keywords =  {'same', 'Targets', 'Aam-4', 'Same', 'Type', 'Zuni', 'targets', 'zuni', 'mitsubishi', 'Mitsubishi', 'type', 'aam-4'}\n",
      "titlewords =  {'Zuni', 'rocket', 'AAM4'}\n",
      "answerwords =  {'Ground', 'Aam-4', 'Air', 'air', 'ground', 'aam-4'}\n",
      "goldfactwords =  {'Armed', 'ffar', 'unguided', 'Radar', 'Ffar', 'radar', 'N-4', 'Douglas', 'inch', 'hunter', 'Folding', 'semi', 'division', 'aircraft', 'United', '5', 'operations', 'shiki', 'Division', 'Service', 'Operations', 'Semi', 'Japan', 'Fin', 'Company', 'modern', 'Shiki', 'united', 'rocket', 'states', 'aam', 'Unguided', 'folding', 'missile', 'Sparrow', 'Inch', 'n-4', 'Modern', 'aim-7', 'service', 'yūdōdan', '99式空対空誘導弾', 'Oriole', 'company', 'Brass', 'Rocket', 'brass', 'Aam', 'Bridgeport', 'visual', 'Visual', 'kūtaikū', 'oriole', 'Kūtaikū', 'range', 'Active', 'douglas', '127', 'active', 'aam-4', 'japan', 'fin', 'mm', 'Aam-4', 'armed', 'forces', 'medium', 'Forces', 'Medium', 'Missile', 'States', 'Hunter', '5.0', '99', 'sparrow', 'Aim-7', 'Aircraft', 'Mm', 'Range', 'Yūdōdan', 'bridgeport'}\n",
      "num of pages:  30\n",
      "total num of sentences found =  59\n",
      "total num of imgs found =  25\n"
     ]
    }
   ],
   "source": [
    "### Mining + Save as json\n",
    "upd_txt_data = {} #json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data.json\", \"r\"))\n",
    "for k in [5409]:\n",
    "    if str(k) in upd_txt_data: continue\n",
    "    upd_txt_data[str(k)] = copy.deepcopy(new_txt_data[str(k)])\n",
    "    upd_txt_data[str(k)]['new_negFacts'] = []\n",
    "    upd_txt_data[str(k)]['img_negFacts'] = []\n",
    "    sen2score, cap2score, word_lists = dummy_get_sen2score_from_indx(k)\n",
    "    upd_txt_data[str(k)]['word_lists'] = {\n",
    "        'titlewords': \" || \".join(word_lists[0]), \n",
    "        'keywords': \" || \".join(word_lists[1]), \n",
    "        'goldfactwords': \" || \".join(word_lists[2]), \n",
    "        'answerwords': \" || \".join(word_lists[3])\n",
    "    }\n",
    "    new_negFacts_count = 0\n",
    "    for s in sen2score:\n",
    "        if new_negFacts_count >= 40: break\n",
    "        if sen2score[s]['scores'][2] == 0.0 and sen2score[s]['scores'][1] == 0.0 and len(s.split()) in range(22, 60):\n",
    "            upd_txt_data[str(k)]['new_negFacts'].append({\n",
    "                'title': sen2score[s]['title'],\n",
    "                'scores': str(sen2score[s]['scores']),\n",
    "                'fact': s,\n",
    "                'url': sen2score[s]['link']\n",
    "            })\n",
    "            new_negFacts_count += 1\n",
    "    \n",
    "    img_negFacts_count = 0\n",
    "    for c in cap2score:\n",
    "        if img_negFacts_count >= 40: break\n",
    "        upd_txt_data[str(k)]['img_negFacts'].append({\n",
    "            'title': cap2score[c]['title'],\n",
    "            'scores': str(cap2score[c]['scores']),\n",
    "            'caption': c,\n",
    "            'url': cap2score[c]['link'],\n",
    "            'imgUrl': cap2score[c]['img']\n",
    "        })\n",
    "        img_negFacts_count += 1\n",
    "json.dump(upd_txt_data, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17430\n"
     ]
    }
   ],
   "source": [
    "ids_16k = list(json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/txt_dataset_0728_16k_new.json\", \"r\")).keys())\n",
    "print(len(ids_16k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['upd_txt_data_16k_20000.json', 'upd_txt_data_16k_26000.json', 'upd_txt_data_16k_22000.json', 'upd_txt_data_16k_11600.json', 'upd_txt_data_16k_11800.json', 'upd_txt_data_16k_2000.json', '.ipynb_checkpoints', 'upd_txt_data_16k_11400.json', 'upd_txt_data_16k_6000.json', 'upd_txt_data_16k_24000.json', 'upd_txt_data_16k_16000.json', 'upd_txt_data_16k_10000.json', 'upd_txt_data_16k_18000.json', 'upd_txt_data_16k_14000.json', 'upd_txt_data_16k_4000.json', 'upd_txt_data_16k_12000.json', 'upd_txt_data_16k_8000.json']\n",
      "17 files found\n",
      "upd_txt_data_16k_10000.json\n"
     ]
    }
   ],
   "source": [
    "### check progress\n",
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data_16k/\"\n",
    "print(os.listdir(path))\n",
    "if os.path.isdir(path):\n",
    "    finished_samples = []\n",
    "    files = os.listdir(path)\n",
    "    print(\"{} files found\".format(len(files)))\n",
    "    for f in files:\n",
    "        if not '.json' in f: continue\n",
    "        try: finished_samples.extend(list(json.load(open(os.path.join(path, f), \"r\")).keys()))\n",
    "        except: print(f)\n",
    "else:\n",
    "    finished_samples.extend(list(json.load(open(path, \"r\")).keys()))\n",
    "print(\"{} samples found\".format(len(finished_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3756', '9949', '9950', '9951', '9954', '9955', '9956', '9957', '9958', '9959', '9960', '9962', '9964', '9965', '9968', '9970', '9971', '9974', '9977', '9978', '9980', '9981', '9983', '9985', '9987', '9992', '9993', '9994', '9995', '9997', '9998', '10830', '11245', '11246', '11247', '11249', '11250', '11251', '11252', '11253', '11254', '11255', '11257', '11258', '11260', '11261', '11262', '11263', '11264', '11266', '11267', '11268', '11271', '11272', '11274', '11275', '11276', '11277', '11278', '11279', '11281', '11282', '11283', '11284', '11286', '11289', '11290', '11292', '11293', '11294', '11296', '11297', '11299', '11300', '11301', '11302', '11303', '11304', '11306', '11309', '11310', '11311', '11312', '11314', '11316', '11317', '11318', '11319', '11321', '11322', '11323', '11324', '11325', '11326', '11327', '11328', '11329', '11330', '11332', '11334', '11340', '11341', '11343', '11344', '11345', '11346', '11348', '11349', '11350', '11351', '11352', '11353', '11354', '11356', '11360', '11362', '11363', '11365', '11366', '11369', '11371', '11372', '11374', '11375', '11376', '11378', '11379', '11380', '11381', '11382', '11383', '11384', '11386', '11387', '11388', '11392', '11394', '11396', '11397', '11403', '11404', '11406', '11407', '11408', '11410', '11411', '11412', '11414', '11415', '11417', '11422', '11423', '11424', '11428', '11429', '11430', '11432', '11433', '11436', '11438', '11439', '11441', '11446', '11447', '11448', '11452', '11453', '11454', '11456', '11458', '11459', '11460', '11461', '11462', '11463', '11464', '11465', '11467', '11468', '11469', '11470', '11473', '11474', '11475', '11476', '11480', '11481', '11482', '11483', '11484', '11485', '11486', '11487', '11491', '11493', '11494', '11495', '11496', '11498', '11499', '11500', '11502', '11504', '11505', '11506', '11508', '11509', '11510', '11511', '11512', '11513', '11516', '11517', '11518', '11521', '11523', '11524', '11525', '11526', '11527', '11528', '11531', '11532', '11533', '11534', '11535', '11536', '11537', '11541', '11542', '11543', '11546', '11548', '11549', '11550', '11551', '11552', '11555', '11556', '11558', '11560', '11564', '11565', '11569', '11570', '11571', '11572', '11573', '11574', '11575', '11577', '11578', '11579', '11580', '11583', '11584', '11585', '11586', '11587', '11589', '11590', '11592', '11593', '11594', '11596', '11598', '11599', '11602', '11603', '11604', '11605', '11606', '11607', '11609', '11611', '11613', '11614', '11616', '11617', '11618', '11619', '11621', '11622', '11624', '11625', '11626', '11633', '11634', '11635', '11636', '11637', '11638', '11639', '11640', '11644', '11645', '11647', '11648', '11649', '11650', '11651', '11652', '11653', '11655', '11657', '11659', '11660', '11661', '11662', '11664', '11665', '11666', '11668', '11670', '11674', '11675', '11676', '11677', '11678', '11679', '11680', '11681', '11683', '11684', '11685', '11686', '11687', '11688', '11691', '11692', '11694', '11695', '11696', '11697', '11698', '11700', '11703', '11704', '11705', '11706', '11708', '11712', '11713', '11716', '11717', '11718', '11719', '11720', '11721', '11722', '11723', '11724', '11726', '11728', '11730', '11731', '11734', '11736', '11737', '11739', '11741', '11742', '11743', '11745', '11746', '11747', '11751', '11752', '11753', '11754', '11755', '11756', '11758', '11759', '11760', '11761', '11762', '11763', '11765', '11767', '11768', '11769', '11770', '11771', '11772', '11774', '11775', '11776', '11777', '11780', '11781', '11784', '11785', '11786', '11787', '11789', '11791', '11792', '11793', '11794', '11796', '11798', '11799', '11802', '11803', '11804', '11805', '11806', '11807', '11808', '11809', '11810', '11811', '11812', '11813', '11814', '11815', '11817', '11818', '11819', '11821', '11823', '11824', '11825', '11826', '11827', '11828', '11829', '11830', '11831', '11832', '11833', '11836', '11837', '11838', '11840', '11842', '11843', '11845', '11848', '11849', '11850', '11851', '11852', '11855', '11860', '11861', '11863', '11865', '11866', '11867', '11868', '11870', '11871', '11872', '11873', '11875', '11876', '11877', '11878', '11880', '11882', '11884', '11885', '11887', '11888', '11891', '11893', '11898', '11899', '11901', '11904', '11905', '11906', '11907', '11908', '11909', '11911', '11912', '11913', '11915', '11918', '11920', '11921', '11922', '11923', '11924', '11925', '11926', '11928', '11929', '11931', '11934', '11935', '11936', '11941', '11944', '11947', '11948', '11949', '11950', '11951', '11952', '11953', '11954', '11955', '11957', '11958', '11960', '11961', '11962', '11963', '11964', '11966', '11968', '11970', '11972', '11974', '11976', '11977', '11978', '11979', '11980', '11983', '11984', '11985', '11987', '11988', '11990', '11993', '11994', '11995', '11996', '11997', '11998', '15748', '17713']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in ids_16k if k not in finished_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 686), (1, 479), (2, 429), (3, 386), (4, 344), (5, 320), (6, 313), (7, 314), (8, 274), (9, 316), (10, 261), (11, 234), (12, 244), (13, 221), (14, 214), (15, 196), (16, 190), (17, 192), (18, 211), (19, 192), (20, 181), (21, 180), (22, 148), (23, 169), (24, 165), (25, 151), (26, 133), (27, 134), (28, 116), (29, 115), (30, 134), (31, 129), (32, 125), (33, 134), (34, 127), (35, 108), (36, 123), (37, 102), (38, 102), (39, 84), (40, 6033)]\n",
      "[(0, 283), (1, 595), (2, 538), (3, 521), (4, 449), (5, 441), (6, 391), (7, 372), (8, 353), (9, 381), (10, 330), (11, 302), (12, 280), (13, 278), (14, 289), (15, 255), (16, 243), (17, 233), (18, 250), (19, 227), (20, 220), (21, 188), (22, 185), (23, 177), (24, 158), (25, 196), (26, 167), (27, 190), (28, 120), (29, 172), (30, 159), (31, 131), (32, 134), (33, 136), (34, 129), (35, 127), (36, 124), (37, 111), (38, 122), (39, 120), (40, 4632)]\n",
      "3568\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data_16k/\"\n",
    "\n",
    "snippets_count = []\n",
    "img_count = []\n",
    "\n",
    "no_snippet_k = []\n",
    "no_img_k = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "data = {}\n",
    "for f in files:\n",
    "    if not '.json' in f: continue\n",
    "    try: x = json.load(open(os.path.join(path, f), \"r\"))\n",
    "    except:\n",
    "        print(f)\n",
    "        continue\n",
    "    for k in x:\n",
    "        num_snippets = len(x[k]['new_negFacts'])\n",
    "        num_imgs = len(x[k]['img_negFacts'])\n",
    "        if num_snippets == 0:\n",
    "            no_snippet_k.append(k)\n",
    "        if num_imgs == 0:\n",
    "            no_img_k.append(k)\n",
    "        snippets_count.append(num_snippets)\n",
    "        img_count.append(num_imgs)\n",
    "    data.update(x)\n",
    "#assert len(snippets_count) == len(img_count) == 7921\n",
    "print(sorted(Counter(snippets_count).items()))\n",
    "print(sorted(Counter(img_count).items()))\n",
    "print(len([k for k in data if len(data[k]['new_negFacts'])< 5 or len(data[k]['img_negFacts']) < 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 files found\n",
      "upd_txt_data_8000.json []\n",
      "upd_txt_data_1000.json []\n",
      "upd_txt_data_3000.json []\n",
      "upd_txt_data_4000.json []\n",
      "upd_txt_data_6000.json []\n",
      "upd_txt_data_2000.json []\n",
      "upd_txt_data_7000.json []\n",
      "upd_txt_data_5000.json []\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data/\"\n",
    "files = os.listdir(path)\n",
    "print(\"{} files found\".format(len(files)))\n",
    "for f in files:\n",
    "    if not '.json' in f: continue\n",
    "    try: \n",
    "        d = json.load(open(os.path.join(path, f), \"r\"))\n",
    "        print(f, [k for k in d if 'word_lists' not in d[k]])\n",
    "    except: print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 files found\n",
      "7921 samples found\n"
     ]
    }
   ],
   "source": [
    "### Create demo from json (dataset is divided into 8 chunks of size 1000 for parallel distractor mining)\n",
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data/\"\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    data = {}\n",
    "    files = os.listdir(path)\n",
    "    print(\"{} files found\".format(len(files)))\n",
    "    for f in files:\n",
    "        if not '.json' in f: continue\n",
    "        data.update(json.load(open(os.path.join(path, f), \"r\")))\n",
    "else:\n",
    "    data = json.load(open(path, \"r\"))\n",
    "print(\"{} samples found\".format(len(data)))\n",
    "\n",
    "html = \"<html><body>\"\n",
    "html += \"<style>th {position: sticky; top: 0;background: FloralWhite;}</style>\"\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=gray><th width=5%>Index</th><th width=25%>Q & Pos Snippets</th><th width=40%>Neg Snippets</th><th width=30%>X_modal Facts</th></tr>'\n",
    "count = 0\n",
    "for k in ['3035', '7895', '5563', '4698', '2836', '7798', '4960']: #random.sample(list(data.keys()), 20): \n",
    "    word_lists = [l.split(\" || \") for l in list(data[k]['word_lists'].values())]\n",
    "    sen2score = {}\n",
    "    for f in data[k]['new_negFacts']:\n",
    "        sen2score[f['fact']] = {\n",
    "            'title': f['title'],\n",
    "            'scores': tuple(float(x) for x in f['scores'][1:-1].split(\",\")),\n",
    "            'link': f['url']\n",
    "        }\n",
    "    cap2score = {}\n",
    "    for f in data[k]['img_negFacts']:\n",
    "        cap2score[f['caption']] = {\n",
    "            'title': f['title'],\n",
    "            'scores': tuple(float(x) for x in f['scores'][1:-1].split(\",\")),\n",
    "            'link': f['url'],\n",
    "            'img': f['imgUrl']\n",
    "        }\n",
    "    html += add_html_row(k, sen2score, cap2score, word_lists)\n",
    "    o = open('x_distractor_for_txt_demo2.html', 'wt')\n",
    "\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_txt_demo2.html', 'wt')\n",
    "\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  3787\n",
      "#pages before extension by hyperlink:  18\n",
      "#pages after extension by hyperlink:  42\n",
      "Q =  Which rival groups did the people who are currently the dominant ethnic group in the Xinjiang region compete with in the past?\n",
      "A =  They competed with other Altaic tribes, Indo-European empires from the south and west and Sino-Tibetan empires to the east.\n",
      "keywords =  {'dominant', 'ethnic', 'rival', 'Region', 'region', 'xinjiang', 'Past', 'Rival', 'Xinjiang', 'groups', 'group', 'Group', 'People', 'people', 'Dominant', 'Groups', 'Ethnic', 'past'}\n",
      "titlewords =  {'History', 'Xinjiang', 'Christianity', 'people', 'Uyghur'}\n",
      "answerwords =  {'tribes', 'indo', 'south', 'West', 'Sino', 'Tibetan', 'european', 'Other', 'Empires', 'Altaic', 'other', 'Indo', 'South', 'tibetan', 'east', 'European', 'East', 'empires', 'sino', 'Tribes', 'west', 'altaic'}\n",
      "goldfactwords =  {'Brief', 'Republic', 'powers', 'story', 'Christianity', 'muslim', 'Tribe', 'few', 'Mountains', 'Nomadic', 'nomadic', 'small', 'Story', 'republic', 'Religion', 'Small', 'Muslim', 'Central', 'Uyghur', 'asia', 'Christian', 'China', 'altai', 'Powers', 'mountains', 'minority', 'Few', 'Asia', 'religion', 'christianity', 'china', 'History', 'history', 'christian', 'brief', 'central', 'uyghur', 'tribe', 'Minority', 'Altai'}\n",
      " -- find sentences from each page --\n",
      "total num of sentences found =  385\n",
      "total num of imgs found =  6\n"
     ]
    }
   ],
   "source": [
    "### Mining + Create demo\n",
    "\n",
    "html = \"<html><body>\"\n",
    "html += \"<style>th {position: sticky; top: 0;background: FloralWhite;}</style>\"\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=gray><th width=5%>Index</th><th width=25%>Q & Pos Snippets</th><th width=40%>Neg Snippets</th><th width=30%>X_modal Facts</th></tr>'\n",
    "count = 0\n",
    "for k in [3787]: #[34, 279, 450, 474, 563, 613, 712, 842, 1311, 1793, 1936, 2340, 4266, 5279, 5620, 5800, 6845, 7018, 7529]:\n",
    "    count += 1\n",
    "    sen2score, cap2score, word_lists = get_sen2score_from_indx(k)\n",
    "    html += add_html_row(k, sen2score, cap2score, word_lists)\n",
    "    o = open('x_distractor_for_txt_demo3.html', 'wt')\n",
    "\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_txt_demo3.html', 'wt')\n",
    "\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "### Post-editing compensation for no 'https:' before '//upload' in demo\n",
    "with open('0719demo.html','r') as f:\n",
    "    file = f.read()\n",
    "f.close()\n",
    "print(type(file))\n",
    "print(len(file))\n",
    "indices = [i for i in range(len(file)) if file.startswith('//upload.wikimedia.org/wikipedia/commons/', i)]\n",
    "count = 0\n",
    "for i in indices:\n",
    "    file = file[:i+6*count] + 'https:' + file[6*count+i:]\n",
    "    count += 1\n",
    "print(len(file))\n",
    "o = open('0719_demo.html', 'wt')\n",
    "\n",
    "o.write(file)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "no_word_list_k = []\n",
    "for k in data:\n",
    "    if not 'word_lists' in data[k]:\n",
    "        no_word_list_k.append(k)\n",
    "print(len(no_word_list_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Patrick_Dunne_(priest)', 'https://en.wikipedia.org/wiki/Patrick_Dunne_(priest)', 'https://en.wikipedia.org/wiki/Patrick_Dunne_(priest)', 'https://en.wikipedia.org/wiki/Patrick_Dunne_(priest)', 'https://en.wikipedia.org/wiki/Patrick_Dunne_(priest)', 'https://sbcl.wa.edu.au/about/history/', 'https://www.broadwayworld.com/people/Patrick-Dunn/', 'https://stbrigidsms.wa.edu.au/our-school/history-of-our-school']\n",
      "Principals || More || scoil || Many || bhride || principals || 1960s || more || many || Scoil || Bhride || 1970s\n",
      "['https://en.wikipedia.org/wiki/Social_media_use_in_politics', 'https://en.wikipedia.org/wiki/List_of_social_networking_websites', 'https://en.wikipedia.org/wiki/Social_media_use_in_politics', 'https://en.wikipedia.org/wiki/Social_media_marketing', 'https://en.wikipedia.org/wiki/Social_media_use_in_politics']\n",
      "politicians || Social || younger || social || media || Platforms || Younger || Politicians || Voters || Media || voters || platforms || energized || Energized\n",
      "['http://www.understandconstruction.com/waterproofing-membranes.html', 'https://www.cliffsnotes.com/study-guides/biology/plant-biology/roots/primary-root-tissues-and-structure', 'https://quizlet.com/240240930/water-and-major-minerals-nutrition-flash-cards/', 'https://www.biologydiscussion.com/plant-tissues/tissue-system/top-4-types-of-ground-tissues-in-plants-with-diagram-botany/20340', 'https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/endoderm', 'https://www.sciencetopia.net/biology/botany/dicot-stem', 'https://www.hunker.com/12002059/how-to-keep-water-from-leaking-outside-a-shower-curtain', 'https://link.springer.com/article/10.1007/s00344-003-0002-2', 'https://classnotes.org.in/class-10/life-processes/transportation-in-plants/', 'https://onlinelibrary.wiley.com/doi/10.1111/tpj.14459', 'https://qsstudy.com/biology/difference-endodermis-exodermis']\n",
      "endodermis || permeable || membrane || Permeable || Membrane || Cell || cell || Endodermis\n",
      "['https://en.wikipedia.org/wiki/List_of_lakes_in_Missouri', 'https://www.worldatlas.com/articles/the-largest-lakes-in-the-us.html', 'https://www.worldatlas.com/articles/the-largest-lakes-in-the-us.html', 'https://en.m.wikipedia.org/wiki/List_of_lakes_by_area', 'https://en.wikipedia.org/wiki/Talk:List_of_largest_lakes_of_the_United_States_by_area', 'https://en.wikipedia.org/wiki/List_of_lakes_in_Delaware']\n",
      "deeper || lake || Deeper || Okeechobee || tahoe || Tahoe || Lake || okeechobee\n",
      "['https://en.wikipedia.org/wiki/CPU_socket', 'https://en.wikipedia.org/wiki/Slot_2', 'https://en.wikipedia.org/wiki/CPU_socket', 'https://en.wikipedia.org/wiki/CPU_socket', 'https://en.wikipedia.org/wiki/Slot_1', 'https://www.intel.co.uk/content/www/uk/en/products/processors/xeon.html', 'https://www.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-processor-7500-series-vol-1-datasheet.pdf', 'https://en.wikipedia.org/wiki/Slot_1']\n",
      "processor || right || slot || intel || Slot || True || Way || true || Only || way || Intel || Right || Processor || only\n",
      "['http://www.understandconstruction.com/waterproofing-membranes.html', 'https://www.cliffsnotes.com/study-guides/biology/plant-biology/roots/primary-root-tissues-and-structure', 'https://quizlet.com/240240930/water-and-major-minerals-nutrition-flash-cards/', 'https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/endoderm', 'https://www.sciencetopia.net/biology/botany/dicot-stem', 'https://www.hunker.com/12002059/how-to-keep-water-from-leaking-outside-a-shower-curtain', 'https://link.springer.com/article/10.1007/s00344-003-0002-2', 'https://classnotes.org.in/class-10/life-processes/transportation-in-plants/', 'https://onlinelibrary.wiley.com/doi/10.1111/tpj.14459', 'https://www.bbc.co.uk/bitesize/guides/zqdhjty/revision/1', 'https://qsstudy.com/biology/difference-endodermis-exodermis']\n",
      "part || Tissues || vascular || Vascular || cylinder || epidermis || Cell || water || flow || Part || Epidermis || Outer || Water || tissues || Cylinder || Flow || cell || outer\n",
      "['https://en.wikipedia.org/wiki/Reggie_Watts', 'https://en.m.wikipedia.org/wiki/Late_Show_(CBS_TV_series)', 'https://en.wikipedia.org/wiki/Steve_Higgins', 'https://en.wikipedia.org/wiki/The_Late_Late_Show_(U.S._TV_series)', 'https://en.wikipedia.org/wiki/Late_Show_with_David_Letterman', 'https://en.wikipedia.org/wiki/Late_Show_with_David_Letterman', 'https://en.wikipedia.org/wiki/Reggie_Watts', 'https://en.wikipedia.org/wiki/The_Late_Late_Show_(CBS_TV_series)', 'https://en.wikipedia.org/wiki/Reggie_Watts']\n",
      "oliver || letterman || person || entertainment || David || Chairman || Cbs || Oliver || john || John || Letterman || cbs || david || Entertainment || chairman || Job || Person || job\n",
      "['https://en.wikipedia.org/wiki/Social_media_use_in_politics', 'https://en.wikipedia.org/wiki/List_of_social_networking_websites', 'https://en.wikipedia.org/wiki/Social_media_use_in_politics', 'https://en.wikipedia.org/wiki/Social_media_marketing', 'https://en.wikipedia.org/wiki/Social_media_use_in_politics']\n",
      "Media || Ways || Two || ways || social || two || Social || Politics || politics || media\n",
      "['https://www.loc.gov/item/tx0741/', 'https://www.cnic.navy.mil/regions/cnrse/installations/nas_jrb_fort_worth.html', 'https://fas.org/man/dod-101/fac/port/ingleside.htm', 'https://www.globalsecurity.org/wmd/facility/medina.htm', 'http://www.militarybases.us/navy/dallas-nas/', 'https://en.wikipedia.org/wiki/Naval_Air_Station_Corpus_Christi', 'https://inglesidetx.gov/finance/property-taxes/']\n",
      "Air || bodies || Christi || air || Station || ingleside || christi || Water || naval || station || Same || same || Ingleside || Naval || corpus || Corpus || Bodies || water\n",
      "['https://en.wikipedia.org/wiki/Branson_micropolitan_area', 'https://www.mapquest.com/us/missouri/branson-mo-282039766', 'https://www.mapquest.com/us/missouri/branson-mo-282039766', 'https://en.wikipedia.org/wiki/Columbia_Bottom_Conservation_Area', 'https://en.wikipedia.org/wiki/Young_conservation_area']\n",
      "Population || Mo || larger || mo || population || Springfield || springfield || branson || much || Branson || Much || Larger\n",
      "['http://self.gutenberg.org/articles/eng/North_Russian_Expeditionary_Force', 'https://quod.lib.umich.edu/b/bhlead/umich-bhl-89348?view=text', 'https://www.britannica.com/place/Arctic/Early-Russian-exploration', 'http://ethelbertsdiary.co.uk/?page=about', 'https://nationalinterest.org/blog/buzz/yes-it-true-1918-america-invaded-russia-77646', 'https://www.americanrifleman.org/articles/2018/9/12/frozen-indecision-american-intervention-in-siberian-russia-1918/', 'https://www.smithsonianmag.com/history/forgotten-doughboys-who-died-fighting-russian-civil-war-180971470/', 'https://en.wikipedia.org/wiki/List_of_Russian_explorers', 'https://military.wikia.org/wiki/American_Expeditionary_Force_Siberia', 'https://en.m.wikipedia.org/wiki/Russian_Legion', 'https://hyperleap.com/topic/American_Expeditionary_Force%2C_North_Russia', 'https://theconversation.com/lessons-from-white-house-disinformation-a-century-ago-its-dangerous-to-believe-your-own-propaganda-102155']\n",
      "initial || war || General || doughboys || Pershing || 1 || general || Corps || world || Exploration || pershing || corps || signal || Signal || John || Initial || exploration || World || Artic || Doughboys || john || artic || War\n",
      "['http://www.ga.gov.au/mapspecs/250k100k/appendix_m.html', 'https://www.geodatos.net/en/coordinates/australia', 'https://www.icsm.gov.au/datum/gda-transformation-products-and-tools/transformation-grids', 'https://www2.landgate.wa.gov.au/gola/FAQGola.html', 'http://dmp.wa.gov.au/Minerals/Geocentric-Datum-of-Australia-24738.aspx', 'http://www.geoproject.com.au/gda.faq.html', 'https://www.spatial.nsw.gov.au/surveying/geodesy/projections']\n",
      "Mga || Crs || mga || Australia || revision || Geoscience || recent || crs || zone || Gda94/ || australia || Gda || Lambert || Zone || Recent || lambert || Revision || gda || 94/ || 54 || geoscience || gda94/\n",
      "['https://en.wikipedia.org/wiki/Portal:Current_events/April_2010', 'https://www.infoplease.com/current-events/2010/april-2010-current-events-sciencedisasters-news', 'https://en.wikipedia.org/wiki/2010_in_the_United_States', 'https://en.wikipedia.org/wiki/2010_in_the_United_States', 'https://en.wikipedia.org/wiki/2010_in_the_United_States']\n",
      "guidelines || 7.2 || protection || Formal || Greenhouse || month || cars || Year || Cars || Amount || Earthquake || Protection || greenhouse || agency || california || Agency || able || gas || amount || Month || earthquake || Gas || year || Guidelines || emissions || Able || California || Environmental || environmental || Emissions || formal\n",
      "['https://www.vivomed.com/en/help/groin-strain-adductor-strain-symptoms-and-treatment', 'https://www.nychernia.com/adductor-longus-sprains-tears-athletic-pubalgia/', 'https://www.sensational-yoga-poses.com/adductor-magnus.html', 'https://www.dummies.com/education/science/anatomy/nerves-of-the-hip-and-thigh/', 'https://www.kenhub.com/en/library/anatomy/adductor-magnus', 'https://anatomyinfo.com/thigh-muscles/']\n",
      "longus || Biceps || higher || Lower || Femoris || Body || adductor || biceps || body || Muscles || muscles || femoris || Longus || Higher || Adductor || lower\n",
      "['https://balkhandshambhala.blogspot.com/2016/01/polaris-ursa-major-saptarishi-mandala.html', 'https://www.drikpanchang.com/astrology/prediction/info/find-rashi-with-name.html', 'https://indianastrology.co.in/6430-saptarishi-7-great-sages-of-india/', 'https://www.momjunction.com/articles/marathi-surnames-last-names-caste-meanings_00497007/', 'http://www.freebsd.nfo.sk/hinduism/rishi.htm', 'https://balkhandshambhala.blogspot.com/2016/01/polaris-ursa-major-saptarishi-mandala.html', 'https://rocketleague.fandom.com/wiki/Saptarishi', 'https://knowtifyindia.com/saptrishi-ursa-major-and-pleiades/']\n",
      "rashis || taramandal || saptarshi || Rashis || Taramandal || Saptarshi || Names || names || rishis || Rishis\n",
      "['https://archnet.org/sites/1713/publications/1415', 'https://en.wikipedia.org/wiki/Category:Mosques_in_Libya', 'https://en.wikipedia.org/wiki/Gurgi_Mosque', 'https://en.wikipedia.org/wiki/Gurgi_Mosque', 'https://www.libyaobserver.ly/culture/gurgi-mosque']\n",
      "darghut || sultan || same || Same || Darghut || Century || Mosque || Sultan || mosque || century\n",
      "['http://www.ga.gov.au/mapspecs/250k100k/appendix_m.html', 'https://www.geodatos.net/en/coordinates/australia', 'https://www.icsm.gov.au/datum/gda-transformation-products-and-tools/transformation-grids', 'https://www2.landgate.wa.gov.au/gola/FAQGola.html', 'http://www.geoproject.com.au/gda.faq.html', 'https://www.spatial.nsw.gov.au/surveying/geodesy/projections']\n",
      "Mga || Crs || Last || mga || Australian || Australia || revision || last || crs || Gda94 || zone || Geodetic || geodetic || Datum || datum || australia || Zone || australian || 1984 || gda94 || Revision || 54\n",
      "['https://en.wikipedia.org/wiki/Miss_Universe_Philippines_2020', 'https://en.wikipedia.org/wiki/Miss_Universe_Philippines_2020', 'https://www.thesummitexpress.com/2020/10/list-of-winners-miss-universe-philippines-2020.html', 'https://www.preview.ph/culture/miss-universe-filipino-winners-a00038-a00191-20181217', 'https://en.wikipedia.org/wiki/Philippines_at_major_beauty_pageants']\n",
      "wide || Wide || contests || Contests || Universe || More || catriona || Mateo || Miss || rabiya || contestant || contestants || gray || universe || miss || title || Rabiya || mateo || Title || Catriona || Contestant || Gray || more || Contestants\n",
      "['https://www.cricbuzz.com/profiles/1600/laxmi-shukla', 'https://timesofindia.indiatimes.com/india/west-bengal-minister-laxmi-ratan-shukla-resigns-from-mamata-banerjees-cabinet/articleshow/80113928.cms', 'http://www.espncricinfo.com/india/content/player/34019.html', 'https://www.ndtv.com/india-news/laxmi-ratan-shukla-trinamool-congress-mla-quits-as-minister-in-mamata-banerjee-government-weeks-after-party-rebels-joined-bjp-2347964']\n",
      "Shukla || shukla || 1990 || Cricket || 2000 || laxmi || Laxmi || cricket\n",
      "['https://www.tecmint.com/learn-to-code-2018-bundle/', 'https://en.wikipedia.org/wiki/PWYW', 'https://www.moneyland.ch/en/pay-what-you-want', 'https://xendpay.com/pay-what-you-want/', 'https://en.m.wikipedia.org/wiki/Pay_what_you_want']\n",
      "useful || Pricing || Useful || pay || Pay || Want || you || want || risky || model || pricing || You || Model || Risky\n",
      "['http://www.jasinski.co.uk/wojna/conflicts/conf05.htm', 'https://www.onthisday.com/events/date/1671', 'https://www.telegraph.co.uk/books/what-to-read/plague-fire-and-war-for-london-1666-was-truly-an-annus-horribili/', 'https://www.historyguy.com/wars_of_poland.htm', 'https://www.globalsecurity.org/military/world/europe/pl-army-history-02.htm', 'http://www.zum.de/whkmla/military/17cen/polott16721676.html']\n",
      "Dutch || Fire || The || great || fire || England || War || war || Year || the || peril || england || Peril || dutch || year || Great || Plague || plague\n",
      "['https://en.m.wikipedia.org/wiki/Buchan_Bakers', 'https://en.wikipedia.org/wiki/Buchan_Bakers', 'https://soldiersangels.org/programs/deployed-support/angel-bakers/', 'https://en.wikipedia.org/wiki/Buchan_Bakers', 'https://soldiersangels.org/programs/deployed-support/angel-bakers/']\n",
      "Top || Trautman || Rambo || rambo || knife || colonel || sam || Grooves || Tribute || top || tribute || Sam || trautman || team || Colonel || grooves || Knife || Team\n",
      "['https://orlandostylemagazine.com/rob-mandell/', 'https://www.americanambassadors.org/members/robert-a-mandell', 'https://orlandostylemagazine.com/rob-mandell/', 'https://orlandostylemagazine.com/rob-mandell/', 'https://en.wikipedia.org/wiki/Robert_A._Mandell']\n",
      "robert || two || Two || Career || career || governmental || Mandell || mandell || Positions || Governmental || Robert || positions\n",
      "['https://www.allisontibbs.com/', 'https://www.telegraph.co.uk/obituaries/2017/09/13/david-tibbs-ramc-officer-won-mc-d-day-obituary/', 'https://www.baseball-almanac.com/players/player.php?p=tibbsja01', 'https://www.allisontibbs.com/', 'https://www.nps.gov/waba/learn/historyculture/walter-m-kennedy.htm', 'https://en.wikipedia.org/wiki/Jay_Tibbs', 'https://www.telegraph.co.uk/obituaries/2017/09/13/david-tibbs-ramc-officer-won-mc-d-day-obituary/', 'https://coachad.com/news/lost-state-title-ring-from-1976-is-returned-to-coach/']\n",
      "Magic || magic || tibbs || Jay || why || Tibbs || moments || jay || Old || old || Why || not || Moments || Not\n",
      "['https://www.iwm.org.uk/collections/item/object/30004047', 'https://en.wikipedia.org/wiki/Cockle_Mark_II_canoe', 'https://en.wikipedia.org/wiki/Cockle_Mark_II_canoe', 'https://www.warhistoryonline.com/history/cockleshell-heroes-the-story-of-extreme-bravery-sacrifice-and-endurance-but-was-it-all-worth-it.html', 'https://sites.google.com/site/historicalmaritimesociety/about-us/world-war-two-reenactment/the-mark-8-motorised-canoe-project']\n",
      "ii || mk7 || Cockle || Mk7 || British || military || british || cockle || Military || Mk || Ii || canoes || mk || Canoes\n",
      "['https://balkhandshambhala.blogspot.com/2016/01/polaris-ursa-major-saptarishi-mandala.html', 'https://www.drikpanchang.com/astrology/prediction/info/find-rashi-with-name.html', 'https://indianastrology.co.in/6430-saptarishi-7-great-sages-of-india/', 'https://www.momjunction.com/articles/marathi-surnames-last-names-caste-meanings_00497007/', 'http://www.freebsd.nfo.sk/hinduism/rishi.htm', 'https://balkhandshambhala.blogspot.com/2016/01/polaris-ursa-major-saptarishi-mandala.html', 'https://knowtifyindia.com/saptrishi-ursa-major-and-pleiades/', 'https://www.urbandictionary.com/define.php?term=Saptarshi']\n",
      "taramandal || Shape || shape || saptarishi || Taramandal || Saptarishi\n",
      "['https://en.wikipedia.org/wiki/Miss_Universe_Ireland', 'https://www.thewhiskyexchange.com/p/5239/irish-mist-liqueur', 'https://www.racenet.com.au/horse/irish-mist', 'http://miss-ireland.ie/', 'http://www.completecocktails.com/Ingredients/IrishMist.aspx', 'https://en.wikipedia.org/wiki/Michelle_Rocca', 'https://en.wikipedia.org/wiki/Miss_Ireland']\n",
      "Miss || Competition || ireland || miss || Ireland || 72nd || competition\n",
      "['https://www.ons.gov.uk/methodology/geography/ukgeographies/administrativegeography/ourchanginggeography/localgovernmentrestructuring', 'https://www.gov.uk/guidance/abandoned-vehicles-council-responsibilities', 'https://en.wikipedia.org/wiki/Local_government_areas_of_Scotland_1973_to_1996', 'https://lgiu.org/local-government-facts-and-figures-england/', 'https://www.dataunitwales.gov.uk/local-government-benchmarking-framework-scotland', 'https://en.wikipedia.org/wiki/Scottish_local_elections,_1970', 'https://www.politics.co.uk/reference/local-government-structure/', 'https://en.wikipedia.org/wiki/Category:Regions_of_Scotland', 'https://www.gov.scot/publications/local-government-borrowing-factsheet/', 'https://www.heraldscotland.com/news/18915417.scottish-councils-get-european-style-powers-protections/', 'https://en.wikipedia.org/wiki/History_of_the_local_government_of_Scotland', 'https://www.dailyrecord.co.uk/news/local-news/snp-relinquish-control-north-ayrshire-8652308', 'https://www.gov.scot/publications/town-centre-action-plan-scottish-government-response/']\n",
      "Scottish || Next || Month || election || Parliament || next || 2016 || month || Election || parliament || scottish\n",
      "['https://www.vivomed.com/en/help/groin-strain-adductor-strain-symptoms-and-treatment', 'https://www.nychernia.com/adductor-longus-sprains-tears-athletic-pubalgia/', 'https://www.sensational-yoga-poses.com/adductor-magnus.html', 'https://www.dummies.com/education/science/anatomy/nerves-of-the-hip-and-thigh/', 'https://www.kenhub.com/en/library/anatomy/adductor-magnus', 'https://anatomyinfo.com/thigh-muscles/']\n",
      "groin || Muscles || Hip || Total || muscles || Hamstring || Muscle || more || Many || Groin || total || many || muscle || More || hamstring || hip\n",
      "['https://www.learncbse.in/important-questions-for-class-12-biology-cbse-biodiversity/', 'https://www.oldest.org/animals/species/', 'https://www.thoughtco.com/how-many-animal-species-on-planet-130923', 'https://oxfordre.com/business/view/10.1093/acrefore/9780190224851.001.0001/acrefore-9780190224851-e-62', 'https://www.nationalgeographic.org/encyclopedia/rain-forest/', 'https://en.wikipedia.org/wiki/Timeline_of_the_evolutionary_history_of_life', 'https://quizlet.com/61831662/evidence-chart-flash-cards/']\n",
      "Biodiversity || species || richness || Species || biodiversity || Richness\n",
      "['https://www.ons.gov.uk/methodology/geography/ukgeographies/administrativegeography/ourchanginggeography/localgovernmentrestructuring', 'https://www.gov.uk/guidance/abandoned-vehicles-council-responsibilities', 'https://en.wikipedia.org/wiki/Local_government_areas_of_Scotland_1973_to_1996', 'https://lgiu.org/local-government-facts-and-figures-england/', 'https://www.dataunitwales.gov.uk/local-government-benchmarking-framework-scotland', 'https://en.wikipedia.org/wiki/Scottish_local_elections,_1970', 'https://www.politics.co.uk/reference/local-government-structure/', 'https://en.wikipedia.org/wiki/Category:Regions_of_Scotland', 'https://www.gov.scot/publications/local-government-borrowing-factsheet/', 'https://www.heraldscotland.com/news/18915417.scottish-councils-get-european-style-powers-protections/', 'https://en.wikipedia.org/wiki/History_of_the_local_government_of_Scotland', 'https://www.dailyrecord.co.uk/news/local-news/snp-relinquish-control-north-ayrshire-8652308', 'https://www.gov.scot/publications/town-centre-action-plan-scottish-government-response/']\n",
      "members || same || Same || 2021 || Members || scotland || Parliament || 2016 || number || Number || Scotland || parliament\n",
      "['http://maddyromanportfolio6.weebly.com/middle-ages-europe-and-japan.html', 'https://www.dumblittleman.com/finding-our-warrior-nature/', 'https://www.nba.com/warriors/team_history_index.html', 'http://www.legendsandchronicles.com/ancient-warriors/', 'https://en.wikipedia.org/wiki/Horses_in_East_Asian_warfare', 'http://www.legendsandchronicles.com/ancient-warriors/', 'https://www.teachjapan.org/category/warrior-government/']\n",
      "Warriors || championship || 1947 || philadelphia || player || Philadelphia || Championship || 1946 || warriors || Player\n",
      "['https://en.wikipedia.org/wiki/2015_Middle_East_respiratory_syndrome_outbreak_in_South_Korea', 'https://www.buzzfeed.com/carolinekee/scary-diseases-of-2015', 'https://en.wikipedia.org/wiki/Tornadoes_of_2015', 'https://en.wikipedia.org/wiki/Tornado_outbreak_of_April_8%E2%80%939,_2015', 'https://en.wikipedia.org/wiki/October_29%E2%80%9331,_2015_tornado_outbreak']\n",
      "Rate || Measles || Us || measles || Many || cases || 2015 || Florida || rate || us || florida || higher || leprosy || Times || Leprosy || Higher || many || times || Cases\n",
      "['https://en.wikipedia.org/wiki/Okanagan_Indian_Band', 'https://en.wikipedia.org/wiki/Okanagan_Indian_Band', 'https://theturtleislandnews.com/index.php/2020/12/24/they-are-jeopardizing-our-lives-okanagan-chief-calls-for-change-after-clashes-with-b-c-conservation-officers/', 'http://www.bigorrin.org/okanagan_kids.htm', 'http://sssicamous.ca/earlyokanagan/']\n",
      "arbor || nation || Chief || Death || Nation || Arbor || cultural || death || house || Okib || Cultural || House || chief || Traditional || pit || okib || traditional || Pit || nkwala || Nkwala || okanagan || Okanagan\n",
      "['https://www.crosswordsclue.com/asian-city-on-the-yamuna-river/', 'https://en.wikipedia.org/wiki/List_of_cities_in_Uttar_Pradesh_by_population', 'https://en.wikipedia.org/wiki/Jharkhand', 'https://www.veenaworld.com/blog/12-tourist-places-to-visit-in-uttar-pradesh', 'https://www.postalpincodefor.com/201307', 'https://www.ancestry.com/name-origin?surname=utter', 'https://en.m.wikipedia.org/wiki/List_of_cities_in_maharashtra', 'https://crosswordtracker.com/clue/uttar-pradesh-city/', 'https://www.britannica.com/place/Andhra-Pradesh']\n",
      "Geographic || india || Uttar || India || gujarat || uttar || directional || area || Pradesh || pradesh || Gujarat || geographic || Area || Directional\n",
      "['https://www.bandainamco.co.jp/en/about/history.html', 'https://en.wikipedia.org/wiki/Bandai', 'https://pokeboon.com/first-printed-pokemon-card/', 'https://en.wikipedia.org/wiki/BANDAI_Co.,Ltd']\n",
      "March || uk || Ltd. || ltd. || term || 2006 || Uk || teps || company || name || Name || one || Namco || Teps || Company || Term || 31 || One || march || namco\n",
      "['https://www.stack.com/a/was-aaron-rodgers-61-yard-hail-mary-the-longest-pass-through-the-air-in-football-history', 'https://en.wikipedia.org/wiki/Long-running_musical_theatre_productions', 'https://sports.yahoo.com/nfl-history-derrick-henry-becomes-second-nfl-player-99-yard-td-run-022657021.html', 'https://en.wikipedia.org/wiki/Long-running_musical_theatre_productions', 'https://www.nfl.com/videos/siciliano-reveals-nfl-s-longest-air-distance-pass-since-2017', 'https://bleacherreport.com/articles/2844083-the-nfls-longest-touchdowns-of-all-time', 'https://www.foxnews.com/sports/browns-baker-mayfield-hail-mary-attempt-longest-pass-ever-nfl-game', 'https://www.guinnessworldrecords.com/world-records/longest-play-/', 'https://larrybrownsports.com/football/this-baker-mayfield-pass-longest-in-nfl-history/569938', 'https://blogs.bl.uk/english-and-drama/2013/07/whats-the-longest-play-in-the-world-anyway-anyone.html']\n",
      "Jim || positions || jim || O'kelley || o'kelley || plunkett || Same || Positions || same || Different || different || Plunkett || John || john\n",
      "['http://www.the-crossword-solver.com/word/half+man%2C+half+horse', 'https://narnia.fandom.com/wiki/Centaur', 'https://greekgodsandgoddesses.net/myths/centaurs/', 'https://mythology.net/roman/roman-creatures/faun/', 'https://en.wikipedia.org/wiki/Hippocentaur', 'https://thehorse.com/17000/horses-humans-and-trust/', 'https://ceh.vetmed.ucdavis.edu/diseases-horse-human-transmission', 'https://www.spartacusbrasil.com/l/centaur-thessaly-centauromachy/', 'https://en.wikipedia.org/wiki/Hayagriva', 'https://www.britannica.com/topic/Centaur-Greek-mythology', 'https://www.answers.com/Q/What_is_the_name_of_a_half_human_half_dog_called', 'https://answers.yahoo.com/question/index?qid=20060610135352AAl0UzM', 'https://www.proprofsdiscuss.com/q/106069/person-horse-racing-instead-counting-number-humans-horses-co', 'https://www.tor.com/2020/05/25/how-does-a-centaur-eat-anyway/']\n",
      "mr. || sex || Mr. || Tumnus || tumnus || Sex || Rugnor || rugnor\n",
      "['http://www.the-crossword-solver.com/word/half+man%2C+half+horse', 'https://greekgodsandgoddesses.net/myths/centaurs/', 'https://mythology.net/roman/roman-creatures/faun/', 'https://en.wikipedia.org/wiki/Hippocentaur', 'https://princeofpersia.fandom.com/wiki/Rugnor', 'https://thehorse.com/17000/horses-humans-and-trust/', 'https://ceh.vetmed.ucdavis.edu/diseases-horse-human-transmission', 'https://www.spartacusbrasil.com/l/centaur-thessaly-centauromachy/', 'https://en.wikipedia.org/wiki/Hayagriva', 'https://www.britannica.com/topic/Centaur-Greek-mythology', 'https://www.answers.com/Q/What_is_the_name_of_a_half_human_half_dog_called', 'https://answers.yahoo.com/question/index?qid=20060610135352AAl0UzM', 'https://www.proprofsdiscuss.com/q/106069/person-horse-racing-instead-counting-number-humans-horses-co', 'https://www.tor.com/2020/05/25/how-does-a-centaur-eat-anyway/']\n",
      "type || centaurs || Centaurs || Type || Common || common || Hybrid || hybrid || Tumnus || tumnus\n",
      "['https://interestingliterature.com/2015/10/10-classic-victorian-ghost-stories-everyone-should-read/', 'https://the-line-up.com/gothic-horror-books', 'https://www.invaluable.com/blog/elements-of-gothic-literature/', 'https://www.gothicandamazing.com/top-10-gothic-novels-of-all-time/', 'https://madamewriterblog.com/2017/05/13/top-9-gothic-romance-novelists/']\n",
      "bowen || Serpent || essex || The || genre || Genre || professor || Key || motifs || Essex || John || Professor || Bowen || Motifs || serpent || john || key || the\n",
      "['https://www.stltoday.com/news/archives/photos-from-the-archive-the-1917-east-st-louis-race-riots/collection_ddbb6cfb-8942-551e-9ee4-0be478808ab2.html', 'http://www.blackwallstreet.freeservers.com/red%20summer%20riots.htm', 'https://www.pri.org/stories/2018-02-01/remembering-black-soldiers-executed-after-houstons-1917-race-riot', 'https://en.wikipedia.org/wiki/East_St._Louis_riots']\n",
      "V. || plessy || Plessy || v. || louis || Louis || east || Saint || ferguson || Race || Ferguson || saint || East || race\n",
      "['https://www.stltoday.com/news/archives/photos-from-the-archive-the-1917-east-st-louis-race-riots/collection_ddbb6cfb-8942-551e-9ee4-0be478808ab2.html', 'http://www.blackwallstreet.freeservers.com/red%20summer%20riots.htm', 'https://www.pri.org/stories/2018-02-01/remembering-black-soldiers-executed-after-houstons-1917-race-riot', 'https://en.wikipedia.org/wiki/East_St._Louis_riots']\n",
      "Other || Plessy || black || Incidents || white || ferguson || attacks || Attacks || persons || Race || other || Ferguson || more || Support || plessy || White || Black || More || incidents || Persons || support || race\n",
      "['http://www.guitarsjapan.com/burnyfernandesinfopage.html', 'http://www.guitarsjapan.com/burnyfernandesinfopage.html', 'https://www.thegearpage.net/board/index.php?threads/fernandes-sustainer.821043/', 'http://www.wiredguitarist.com/2016/05/03/guitar-sustainer-guide/', 'https://shop.fender.com/en-US/electric-guitars/stratocaster/fender-eob-sustainer-stratocaster/0140192305.html', 'http://www.wiredguitarist.com/2016/05/03/guitar-sustainer-guide/', 'https://en.wikipedia.org/wiki/Fernandez_Stratocaster_(blue)']\n",
      "Recent || Predecessor || Fernandes || model || Sustainer || recent || battery || fernandes || 9v || Battery || predecessor || Model || sustainer\n",
      "['http://www.guitarsjapan.com/burnyfernandesinfopage.html', 'https://www.thegearpage.net/board/index.php?threads/fernandes-sustainer.821043/', 'http://www.wiredguitarist.com/2016/05/03/guitar-sustainer-guide/', 'https://shop.fender.com/en-US/electric-guitars/stratocaster/fender-eob-sustainer-stratocaster/0140192305.html', 'http://www.wiredguitarist.com/2016/05/03/guitar-sustainer-guide/', 'https://en.wikipedia.org/wiki/Fernandez_Stratocaster_(blue)', 'https://guitarfretbuzz.com/fernandes-sustainer']\n",
      "replica || company || Sustainer || fender || Fender || Guitars || 401 || guitars || Replica || Company || sustainer\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "### At least this many samples will not get new_negFacts from my alg\n",
    "no_wiki_k = []\n",
    "for k in new_txt_data:\n",
    "    if k in no_word_list_k: continue\n",
    "    if all(['wikipedia.org/' not in f['url'] for f in new_txt_data[k]['SupportingFacts']]):\n",
    "        no_wiki_k.append(k)\n",
    "        print([f['url'] for f in new_txt_data[k]['DistractorFacts']])\n",
    "        print(data[k]['word_lists']['keywords'])\n",
    "print(len(no_wiki_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
