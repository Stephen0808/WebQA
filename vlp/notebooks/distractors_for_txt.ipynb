{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, time, os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import copy\n",
    "import string\n",
    "import wikipedia\n",
    "import spacy\n",
    "from itertools import tee\n",
    "import pylcs\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = 'http://en.wikipedia.org/w/api.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT_LIST = [\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "            \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)\",\n",
    "            \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1\",\n",
    "            \"Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_blocklist = ['seal', 'sign ', 'pdf', 'gif', 'icon', 'notice', 'cartoon', 'publish', 'menu', 'logo', 'svg', 'webm', 'page', \\\n",
    "                     'ogg', 'flickr', 'poster', 'ogv', 'banner', 'tif', 'montage', 'centralautologin', 'footer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['NUM', 'NOUN', 'ADJ', 'PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*|\\(|\\)|-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATIONS = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7921\n"
     ]
    }
   ],
   "source": [
    "new_txt_data = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/output_mine_all_schema.json\", \"r\"))\n",
    "print(len(new_txt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(A, B):\n",
    "    intersection = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    return round(intersection / (union+1e-7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wiki_request(params):\n",
    "  \n",
    "    global USER_AGENT_LIST\n",
    "\n",
    "    params['format'] = 'json'\n",
    "    if not 'action' in params:\n",
    "        params['action'] = 'query'\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': random.choice(USER_AGENT_LIST)#'wikipedia (https://github.com/goldsmith/Wikipedia/)'\n",
    "    }\n",
    "\n",
    "    r = requests.get(API_URL, params=params, headers=headers)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content(title):\n",
    "\n",
    "    query_params = {\n",
    "        'prop': 'extracts|revisions',\n",
    "        'explaintext': '',\n",
    "        'rvprop': 'ids',\n",
    "        'titles': title\n",
    "    }\n",
    "    request = _wiki_request(query_params)\n",
    "    result = request['query']['pages']\n",
    "    content = result[list(result.keys())[0]]['extract']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "19\n",
      "29\n",
      "39\n",
      "49\n",
      "59\n",
      "69\n",
      "79\n",
      "89\n",
      "99\n",
      "40.43351101875305\n"
     ]
    }
   ],
   "source": [
    "### Takeaway: get html from wikipedia pypi is somehow pretty slow. But the library gives cleaner content\n",
    "### Decision: html is to be obtained from url request; \n",
    "###           content is to be obtained from the (stolen) library function, with USER_AGENT_LIST (using the wiki github user agent doesn't make much difference)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    if i%10 == 9: print(i)\n",
    "    url = \"https://en.wikipedia.org/wiki/Egyptian_tombs\"\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        html = f.read().decode('utf-8')\n",
    "    end_indx = html.find('<h2><span class=\"mw-headline\" id=\"References\">References</span>')\n",
    "    html = html[:end_indx]\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    cont = content(\"Egyptian tombs\") # The stolen function works faster than wikipedia.page().content\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Elfed is a Welsh language place name and personal name. It commonly refers to one of two geographic areas:\\n\\nThe Kingdom of Elfed (English: Elmet), an Old Welsh kingdom in what is now northern England during the Early Middle Ages\\nElfed, a cymwd (commote) of Cantref Gwarthaf (in modern Carmarthenshire, Wales), later anglicised to 'Elvet Hundred'Elfed may also refers to:\\n\\nCynwyl Elfed, a community in the cymwd of Elfed, Wales\\nElfed High School, in Buckley, Flintshire, Wales\\nElfed Davies, Baron Davies of Penrhys (1913–1992), Welsh politician\\nElfed Evans (1926–1988), Welsh professional footballer\\nHowell Elvet Lewis hymn-writer, poet and Archdruid whose bardic name was Elfed\\nElfed Morris (born 1942), Welsh professional footballer\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content('Elfed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        html = f.read().decode('utf-8')\n",
    "    end_indx = html.find('<h2><span class=\"mw-headline\" id=\"References\">References</span>')\n",
    "    html = html[:end_indx]\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scrap imgs and their captions. \n",
    "def get_imgs_and_captions(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('img')\n",
    "    imgUrls = []\n",
    "    captions = []\n",
    "    for l in links:\n",
    "        imgUrl = l.get('src')\n",
    "        try: \n",
    "            width = int(l['width'])\n",
    "            height = int(l['height'])\n",
    "        except:\n",
    "            continue\n",
    "        if width<100 or height<100:\n",
    "            continue\n",
    "        if any(b in imgUrl.lower() for b in url_blocklist): \n",
    "            continue\n",
    "        imgUrls.append(imgUrl)\n",
    "\n",
    "        # Special case for thumb images, which are located inside a table\n",
    "        thumbinner_div = l.find_parent(\"div\", class_='thumbinner')\n",
    "        if thumbinner_div: \n",
    "            captions.append(thumbinner_div.text)\n",
    "            continue\n",
    "        \n",
    "        segments = []\n",
    "        prev_th = l.find_previous('th')\n",
    "        if prev_th: segments.append(prev_th.get_text(strip=True))\n",
    "            \n",
    "        tr_parent = l.find_parent('tr')\n",
    "        if tr_parent: segments.append(tr_parent.get_text(strip=True))\n",
    "\n",
    "        captions.append(\". \".join(segments))\n",
    "    return imgUrls, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don't have width or height:  //upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Beirut_1965.webm/120px--Beirut_1965.webm.jpg\n",
      "don't have width or height:  //upload.wikimedia.org/wikipedia/commons/thumb/2/29/Beirut-in-1919.webm/120px--Beirut-in-1919.webm.jpg\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/ChurchMosque.jpg/124px-ChurchMosque.jpg\n",
      "BeirutبيروتBeyrouth. Top to bottom, and left to right:Maronite Cathedral of Saint George(left) andMohammad Al-Amin Mosque(right), Clock Tower atNejmeh Square,Sahat al Shouhada,Sursock Museum, Pigeon Rocks ofRaouché\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/a/a5/1934_Clock_Tower.PNG/124px-1934_Clock_Tower.PNG\n",
      "BeirutبيروتBeyrouth. Top to bottom, and left to right:Maronite Cathedral of Saint George(left) andMohammad Al-Amin Mosque(right), Clock Tower atNejmeh Square,Sahat al Shouhada,Sursock Museum, Pigeon Rocks ofRaouché\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/47/Lebanon_location_map_Topographic.png/250px-Lebanon_location_map_Topographic.png\n",
      "BeirutبيروتBeyrouth. BeirutLocation of Beirut within LebanonShow map of LebanonBeirutBeirut (Eastern Mediterranean)Show map of Eastern MediterraneanBeirutBeirut (Arab world)Show map of Arab world\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Canaanean_Blade.jpg/220px-Canaanean_Blade.jpg\n",
      " Canaanean Blade. Suggested to be part of a javelin. Fresh grey flint, both sides showing pressure flaking. Somewhat narrower at the base, suggesting a haft. Polished at the extreme point. Found on land of the Lebanese Evangelical School for Girls in the Patriarchate area of Beirut.\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Beirut_-_Lebanon_-_Paris_of_the_East%21_-_November_2008_-_Downtown_Beirut_is_re-constructed_mostly_thanks_to_Rafik_Hariri_-_The_Paris_of_the_East_is_back%21.jpg/200px-Beirut_-_Lebanon_-_Paris_of_the_East%21_-_November_2008_-_Downtown_Beirut_is_re-constructed_mostly_thanks_to_Rafik_Hariri_-_The_Paris_of_the_East_is_back%21.jpg\n",
      " Roman Columns of Basilica near the Forum of Berytus\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Beyrouth-histoire1.jpg/220px-Beyrouth-histoire1.jpg\n",
      " View of Beirut with snow-capped Mount Sannine in the background – 19th century\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Beirut_Panorama_%28cropped%29.jpg/220px-Beirut_Panorama_%28cropped%29.jpg\n",
      " Beirut Castle and waterfront, 1868\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/8/89/SL_1914_D052_among_the_pine_groves_of_the_cape_of_beirut.jpg/220px-SL_1914_D052_among_the_pine_groves_of_the_cape_of_beirut.jpg\n",
      " Pine Forest of Beirut, 1914\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/2/21/Grand_serail_solidere_6.jpg/220px-Grand_serail_solidere_6.jpg\n",
      " View of Beirut's Grand Serail- circa 1930\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/%D8%A8%D9%8A%D8%B1%D9%88%D8%AA_%D8%A7%D9%84%D9%82%D8%B1%D9%86_19_%D8%A3%D8%B3%D9%88%D8%AF_%D8%A3%D8%A8%D9%8A%D8%B6.jpg/800px-%D8%A8%D9%8A%D8%B1%D9%88%D8%AA_%D8%A7%D9%84%D9%82%D8%B1%D9%86_19_%D8%A3%D8%B3%D9%88%D8%AF_%D8%A3%D8%A8%D9%8A%D8%B6.jpg\n",
      " An aerial panoramic view of Beirut in the last third of the 19th century\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Escalier_saint-nicolas_beyrouth.jpg/220px-Escalier_saint-nicolas_beyrouth.jpg\n",
      " Saint Nicholas staircase in Ashrafieh\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Ferris_wheel_and_the_corniche.jpg/220px-Ferris_wheel_and_the_corniche.jpg\n",
      " Ras Beirut and the Mediterranean Sea\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Badaro_nightlife.jpeg/220px-Badaro_nightlife.jpeg\n",
      " Nightlife scene in Badaro\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Pigeon%27s_Rock_Beirut_Lebanon.jpg/220px-Pigeon%27s_Rock_Beirut_Lebanon.jpg\n",
      " Pigeon Rock (Raouché)\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/9/90/Beirut_SPOT_1113.jpg/220px-Beirut_SPOT_1113.jpg\n",
      " Beirut seen from SPOT satellite\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Beirut_Districts.png/220px-Beirut_Districts.png\n",
      " Map of the 12 quarters of Beirut\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/ChurchMosque.jpg/120px-ChurchMosque.jpg\n",
      "Dec\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/7/76/Centre-ville_de_Beyrouth.JPG/220px-Centre-ville_de_Beyrouth.JPG\n",
      " Roman baths park in Downtown Beirut.\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/10/Beirut_Downtown.jpg/220px-Beirut_Downtown.jpg\n",
      " Cafés in downtown Beirut\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/1/16/Zaitunay_Bay%2C_Downtown_Beirut%2C_Lebanon.jpg/220px-Zaitunay_Bay%2C_Downtown_Beirut%2C_Lebanon.jpg\n",
      " Zaitunay Bay\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/bb/West-Beirut1983.jpg/220px-West-Beirut1983.jpg\n",
      " Ras Beirut 1983\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/d/d1/RockyRaouch%C3%A9.jpg/220px-RockyRaouch%C3%A9.jpg\n",
      " Raouché\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/6/67/Pigeon_Rocks_Sunset_%2848707394%29.jpg/220px-Pigeon_Rocks_Sunset_%2848707394%29.jpg\n",
      " Pigeon Rocks Sunset\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Beyrouth_%289861430944%29.jpg/220px-Beyrouth_%289861430944%29.jpg\n",
      " Downtown Beirut Mosque\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/5/53/Beirut_Airport_DSC_0439.JPG/220px-Beirut_Airport_DSC_0439.JPG\n",
      " Beirut–Rafic Hariri International Airport\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/9/9d/The_Garden_Show_%26_Spring_Festival.jpg/220px-The_Garden_Show_%26_Spring_Festival.jpg\n",
      " The Garden Show & Spring Festival at the Beirut Hippodrome\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Beirut_Museum.jpg/220px-Beirut_Museum.jpg\n",
      " The National Museum of Beirut\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Sursock_house.jpg/220px-Sursock_house.jpg\n",
      " Sursock Museum\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Beirut_Souks.jpg/220px-Beirut_Souks.jpg\n",
      " Beirut Souks shopping mall\n",
      " ----------------------- \n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Beirut_1913.jpg/119px-Beirut_1913.jpg\n",
      "Left office\n",
      " ----------------------- \n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Beirut'\n",
    "html = get_html(url)\n",
    "imgs, caps = get_imgs_and_captions(html)\n",
    "for im, cap in zip(imgs, caps):\n",
    "    print(im)\n",
    "    print(cap)\n",
    "    print(\" ----------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pages_by_hyperlink(keywords, url):\n",
    "    if 'en.wikipedia.org' not in url: return {}\n",
    "    anchor2page = {}\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        html = f.read().decode('utf-8')\n",
    "    end_indx = html.find('<h2><span class=\"mw-headline\" id=\"References\">References</span>')\n",
    "    html = html[:end_indx]\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('a', attrs={'href': re.compile(\"^/wiki/(?!.*(:|\\(identifier\\))).*\")})\n",
    "    for link in links:\n",
    "        title = link.get('title')\n",
    "        text = link.text\n",
    "        if title is None or not text: continue\n",
    "        #print(link)\n",
    "        if any(b in keywords for b in title.split()):\n",
    "            pagelink = 'https://en.wikipedia.org' + link.get('href')\n",
    "            if pagelink.find(\"#\") > -1:\n",
    "                pagelink = pagelink[:pagelink.find(\"#\")]\n",
    "            anchor2page[__load(title)] = pagelink\n",
    "        if len(text) == 0: print(link)\n",
    "        elif pylcs.lcs(title.lower(), text.lower())/len(text) < 0.85:\n",
    "            if any(b in keywords for b in text.split()):\n",
    "                pagelink = 'https://en.wikipedia.org' + link.get('href')\n",
    "                if pagelink.find(\"#\") > -1:\n",
    "                    pagelink = pagelink[:pagelink.find(\"#\")]\n",
    "                anchor2page[__load(title)] = pagelink\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    return anchor2page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_and_relevant_pages_from_txt_sample(k):\n",
    "    Q = new_txt_data[str(k)]['Q']\n",
    "    doc = nlp(Q)\n",
    "    keywords = set([t.text for s in doc.sents for t in s if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    titlewords = set()\n",
    "    anchor2page = {}\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        if not 'en.wikipedia.org/wiki/' in f['url']: continue\n",
    "        fact_title_raw = ' '.join(urllib.parse.unquote(f['url']).split('/')[-1].split('_'))\n",
    "        fact_title = pattern.sub('', fact_title_raw)\n",
    "        titlewords = titlewords.union(fact_title.split())\n",
    "        for title in wikipedia.search(fact_title_raw):\n",
    "            anchor2page[__load(title)] = \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(\"_\".join(title.split()))\n",
    "    keywords = keywords - PUNCTUATIONS\n",
    "    keywords = set(sum([[w.capitalize(), w.lower()] for w in keywords], []))\n",
    "    titlewords = titlewords - PUNCTUATIONS\n",
    "    \n",
    "    print(\"#pages before extension by hyperlink: \", len(anchor2page))\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        d = find_pages_by_hyperlink(keywords.union(titlewords), f['url'])\n",
    "        anchor2page.update(d)\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    print(\"#pages after extension by hyperlink: \", len(anchor2page))\n",
    "    \n",
    "    A = new_txt_data[str(k)]['A']\n",
    "    doc = nlp(A)\n",
    "    answerwords = set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())]) - keywords\n",
    "    answerwords = answerwords - PUNCTUATIONS\n",
    "    answerwords = set(sum([[w.capitalize(), w.lower()] for w in answerwords], []))\n",
    "    goldfactwords = set()\n",
    "    Q_A_words = keywords.union(answerwords)\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        doc = nlp(f['fact'])\n",
    "        goldfactwords = goldfactwords.union(set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())]) - Q_A_words)\n",
    "    goldfactwords = goldfactwords - PUNCTUATIONS\n",
    "    goldfactwords = set(sum([[w.capitalize(), w.lower()] for w in goldfactwords], []))\n",
    "    \n",
    "    for a in list(anchor2page.keys()):\n",
    "        if is_disambiguation_page(anchor2page[a]):\n",
    "            print(a, \" is an disambiguation page\")\n",
    "            for t in recover_disambiguation_page(a):\n",
    "                anchor2page[__load(t)] = \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(\"_\".join(t.split()))\n",
    "            del anchor2page[a]\n",
    "    if '' in anchor2page: del anchor2page['']\n",
    "    return titlewords, keywords, goldfactwords, answerwords, Q, A, anchor2page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_categories(title):\n",
    "    url = 'https://en.wikipedia.org/w/api.php?format=xml&action=query&prop=categories&titles='+urllib.parse.quote(title)\n",
    "    req = urllib.request.Request(url, headers = {'User-Agent': random.choice(USER_AGENT_LIST)})\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        xml = f.read().decode('utf-8')\n",
    "    soup= BeautifulSoup(xml,\"lxml-xml\")\n",
    "    tags = soup.find('categories')\n",
    "    if tags is None: return []\n",
    "    categories = [c.get('title').replace(\"Category:\", \"\") for c in tags]\n",
    "    return categories\n",
    "def is_disambiguation_page(url):\n",
    "    title = url.split('/')[-1]\n",
    "    return 'disambiguation' in \" \".join(get_page_categories(title))\n",
    "def recover_disambiguation_page(title):\n",
    "    try: \n",
    "        wikipedia.page(title)\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        return err[err.find(\"may refer to:\")+13:].strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __load(title):\n",
    "    '''\n",
    "    Load basic information from Wikipedia.\n",
    "    Confirm that page exists and is not a disambiguation/redirect.\n",
    "    Does not need to be called manually, should be called automatically during __init__.\n",
    "    '''\n",
    "    query_params = {\n",
    "        'prop': 'info|pageprops',\n",
    "        'inprop': 'url',\n",
    "        'ppprop': 'disambiguation',\n",
    "        'redirects': '',\n",
    "        'titles': title\n",
    "    }\n",
    "\n",
    "    request = _wiki_request(query_params)\n",
    "\n",
    "    query = request['query']\n",
    "    pageid = list(query['pages'].keys())[0]\n",
    "    page = query['pages'][pageid]\n",
    "\n",
    "    # missing is present if the page is missing\n",
    "    if 'missing' in page:\n",
    "        print(\"Page is missing: \", title)\n",
    "        return \"\"\n",
    "\n",
    "    # same thing for redirect, except it shows up in query instead of page for whatever silly reason\n",
    "    elif 'redirects' in query:\n",
    "        redirects = query['redirects'][0]\n",
    "        return redirects['to']\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_from_page(title, page, keywords, answerwords, goldfactwords):\n",
    "    try: \n",
    "        cont = content(title)\n",
    "        paragraphs = cont[:cont.find('== References ==')].split('\\n')\n",
    "        \n",
    "    except: \n",
    "        print(\"Exception from find_sentences_from_page, title = \", title)\n",
    "        return {}\n",
    "    sen2score = {}\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            doc = nlp(p)\n",
    "            for s in doc.sents:\n",
    "                if len(s) < 10: \n",
    "                    continue\n",
    "                nouns_in_s = [t.text for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A > 0.06:\n",
    "                    #print(IoU_Q, IoU_A, s.text)\n",
    "                    IoU_G = IoU(set(nouns_in_s), goldfactwords)\n",
    "                    sen2score[s.text] = {'scores': (IoU_Q, IoU_A, IoU_G, IoU_Q - IoU_A, IoU_Q - IoU_A - IoU_G), 'link': page, 'title': title}\n",
    "\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            doc = nlp(p)\n",
    "            it1, it2 = tee(doc.sents)\n",
    "            next(it2, None)\n",
    "            for s1, s2 in zip(it1, it2):\n",
    "                if len(s1) < 5 or len(s2) < 5 or len(s1)+len(s2) > 70 or len(s1)+len(s2) < 10: \n",
    "                    continue \n",
    "                nouns_in_s = [t.text for s in [s1, s2] for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A >= 0.06:\n",
    "                    #print(IoU_Q, IoU_A, \" \".join([s1.text, s2.text]))\n",
    "                    IoU_G = IoU(set(nouns_in_s), goldfactwords)\n",
    "                    sen2score[\" \".join([s1.text, s2.text])] = {'scores': (IoU_Q, IoU_A, IoU_G, IoU_Q - IoU_A, IoU_Q - IoU_A - IoU_G), 'link': page, 'title': title}\n",
    "                    #print(s)\n",
    "    #print(len(sen2score))\n",
    "    return sen2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_imgs_from_page(title, page, keywords, answerwords, goldfactwords):\n",
    "    try: \n",
    "        html = get_html(page)\n",
    "        imgs, caps = get_imgs_and_captions(html)\n",
    "        \n",
    "    except: \n",
    "        print(\"Exception from find_imgs_from_page, page = \", page)\n",
    "        return {}\n",
    "    \n",
    "    cap2score = {}\n",
    "    for im, cap in zip(imgs, caps):\n",
    "        doc = nlp(cap)\n",
    "        nouns_in_s = [t.text for t in doc if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "        IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "        IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "        if IoU_Q -  IoU_A > 0.06:\n",
    "            IoU_G = IoU(set(nouns_in_s), goldfactwords)\n",
    "            cap2score[doc.text] = {'scores': (IoU_Q, IoU_A, IoU_G, IoU_Q - IoU_A, IoU_Q - IoU_A - IoU_G), 'img':im, 'link': page, 'title': title}    \n",
    "    #print(len(cap2score))\n",
    "    return cap2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sen2score_from_indx(k):\n",
    "    print('k = ', k)\n",
    "    titlewords, keywords, goldfactwords, answerwords, Q, A, anchor2page = get_keywords_and_relevant_pages_from_txt_sample(k)\n",
    "    print(\"Q = \", Q)\n",
    "    print(\"A = \", A)\n",
    "    print(\"keywords = \", keywords)\n",
    "    print(\"titlewords = \", titlewords)\n",
    "    print(\"answerwords = \", answerwords)\n",
    "    print(\"goldfactwords = \", goldfactwords)\n",
    "    \n",
    "    print(\" -- find sentences from each page --\")\n",
    "    sen2score = {}\n",
    "    cap2score = {}\n",
    "    for title in anchor2page:\n",
    "        sen2score.update(find_sentences_from_page(title, anchor2page[title], keywords, answerwords, goldfactwords))\n",
    "        cap2score.update(find_imgs_from_page(title, anchor2page[title], keywords, answerwords, goldfactwords))\n",
    "    sen2score = dict(sorted(sen2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    cap2score = dict(sorted(cap2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    print(\"total num of sentences found = \", len(sen2score))\n",
    "    print(\"total num of imgs found = \", len(cap2score))\n",
    "    \n",
    "    word_lists = (titlewords, keywords, goldfactwords, answerwords)\n",
    "    return sen2score, cap2score, word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words(word_lists, colors, sentence):\n",
    "    s = copy.deepcopy(sentence)\n",
    "    for word_list, color in zip(word_lists, colors):\n",
    "        try: \n",
    "            if \"\".join(word_list): \n",
    "                s = re.sub(r'\\b(' + r'|'.join([re.escape(c) for c in word_list]) + r')\\b\\s*', lambda m: '<span style=\"background-color:rgb{}\">{}</span>'.format(color, m.group()), s)\n",
    "        except: \n",
    "            print(s)\n",
    "            print(word_list)\n",
    "            raise\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_html_row(k, sen2score, cap2score, word_lists, colors = [\"(223, 255, 238)\", \"(193, 239, 253)\", \"(253, 252, 152)\", \"(255, 214, 222)\"]):\n",
    "    html = \"\"\n",
    "    html += '<tr><td>{}.</td><td>Q: {}<br>'.format(k, highlight_words(word_lists, colors, new_txt_data[str(k)]['Q']))\n",
    "    for f in new_txt_data[str(k)]['SupportingFacts']:\n",
    "        html += '<br><br>&nbsp;&nbsp;{}'.format(highlight_words(word_lists, colors, f['fact']))\n",
    "        html += '<a href=\"{}\"> link</a>'.format(f['url'])\n",
    "    html += '<br><br>A: {}<br>'.format(highlight_words(word_lists, colors, new_txt_data[str(k)]['A']))\n",
    "    html += '</td><td>'\n",
    "    s_buckets = defaultdict(lambda: [])\n",
    "    for s in sen2score:\n",
    "        if sen2score[s]['scores'][2] == 0.0 and sen2score[s]['scores'][1] == 0.0 and len(s.split()) in range(22, 60):\n",
    "            s_buckets['good'].append(s)\n",
    "        elif sen2score[s]['scores'][1] > 0.0 or sen2score[s]['scores'][2] > 0.0:\n",
    "            s_buckets['(maybe)falseneg'].append(s)\n",
    "    \n",
    "    for s in s_buckets['good'][:10]:\n",
    "        html += '{} --- {} '.format(highlight_words(word_lists, colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"> {}</a><br><br>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "    html += '<strong> ----------------- (maybe)falseneg ---------------- </strong><br>'\n",
    "    for s in random.sample(s_buckets['(maybe)falseneg'], min(len(s_buckets['(maybe)falseneg']), 5)):\n",
    "        html += '{} --- {} '.format(highlight_words(word_lists, colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"> {}</a><br><br>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "        \n",
    "    html += '</td><td>'\n",
    "    for k in list(cap2score.keys())[:10]:\n",
    "        html += '<a href=\"{}\" class=\"tool-tip\" target=\"_blank\"><img style=\"display:block; max-height:300px; max-width:100%;\" src = \"{}\"></a>'.format(cap2score[k]['img'], cap2score[k]['img'])\n",
    "        html += '<br>Caption: {}<br>{}<br>'.format(highlight_words(word_lists, colors, k), str(cap2score[k]['scores']))\n",
    "        html += '<a href=\"{}\"> {}</a><br><br>'.format(cap2score[k]['link'], cap2score[k]['title'])\n",
    "    html += '</td></tr>'\n",
    "    html += '<tr><td colspan=4><hr></td></tr>'\n",
    "    return html.encode('ascii', 'xmlcharrefreplace').decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  3787\n",
      "#pages before extension by hyperlink:  18\n",
      "#pages after extension by hyperlink:  42\n",
      "Q =  Which rival groups did the people who are currently the dominant ethnic group in the Xinjiang region compete with in the past?\n",
      "A =  They competed with other Altaic tribes, Indo-European empires from the south and west and Sino-Tibetan empires to the east.\n",
      "keywords =  {'dominant', 'ethnic', 'rival', 'Region', 'region', 'xinjiang', 'Past', 'Rival', 'Xinjiang', 'groups', 'group', 'Group', 'People', 'people', 'Dominant', 'Groups', 'Ethnic', 'past'}\n",
      "titlewords =  {'History', 'Xinjiang', 'Christianity', 'people', 'Uyghur'}\n",
      "answerwords =  {'tribes', 'indo', 'south', 'West', 'Sino', 'Tibetan', 'european', 'Other', 'Empires', 'Altaic', 'other', 'Indo', 'South', 'tibetan', 'east', 'European', 'East', 'empires', 'sino', 'Tribes', 'west', 'altaic'}\n",
      "goldfactwords =  {'Brief', 'Republic', 'powers', 'story', 'Christianity', 'muslim', 'Tribe', 'few', 'Mountains', 'Nomadic', 'nomadic', 'small', 'Story', 'republic', 'Religion', 'Small', 'Muslim', 'Central', 'Uyghur', 'asia', 'Christian', 'China', 'altai', 'Powers', 'mountains', 'minority', 'Few', 'Asia', 'religion', 'christianity', 'china', 'History', 'history', 'christian', 'brief', 'central', 'uyghur', 'tribe', 'Minority', 'Altai'}\n",
      " -- find sentences from each page --\n",
      "total num of sentences found =  385\n",
      "total num of imgs found =  6\n"
     ]
    }
   ],
   "source": [
    "### Mining + Save as json\n",
    "upd_txt_data = {} #json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data.json\", \"r\"))\n",
    "for k in [3787]:\n",
    "    if str(k) in upd_txt_data: continue\n",
    "    upd_txt_data[str(k)] = copy.deepcopy(new_txt_data[str(k)])\n",
    "    upd_txt_data[str(k)]['new_negFacts'] = []\n",
    "    upd_txt_data[str(k)]['img_negFacts'] = []\n",
    "    sen2score, cap2score, word_lists = get_sen2score_from_indx(k)\n",
    "    upd_txt_data[str(k)]['word_lists'] = {\n",
    "        'titlewords': \" || \".join(word_lists[0]), \n",
    "        'keywords': \" || \".join(word_lists[1]), \n",
    "        'goldfactwords': \" || \".join(word_lists[2]), \n",
    "        'answerwords': \" || \".join(word_lists[3])\n",
    "    }\n",
    "    new_negFacts_count = 0\n",
    "    for s in sen2score:\n",
    "        if new_negFacts_count >= 40: break\n",
    "        if sen2score[s]['scores'][2] == 0.0 and sen2score[s]['scores'][1] == 0.0 and len(s.split()) in range(22, 60):\n",
    "            upd_txt_data[str(k)]['new_negFacts'].append({\n",
    "                'title': sen2score[s]['title'],\n",
    "                'scores': str(sen2score[s]['scores']),\n",
    "                'fact': s,\n",
    "                'url': sen2score[s]['link']\n",
    "            })\n",
    "            new_negFacts_count += 1\n",
    "    \n",
    "    img_negFacts_count = 0\n",
    "    for c in cap2score:\n",
    "        if img_negFacts_count >= 40: break\n",
    "        upd_txt_data[str(k)]['img_negFacts'].append({\n",
    "            'title': cap2score[c]['title'],\n",
    "            'scores': str(cap2score[c]['scores']),\n",
    "            'caption': c,\n",
    "            'url': cap2score[c]['link'],\n",
    "            'imgUrl': cap2score[c]['img']\n",
    "        })\n",
    "        img_negFacts_count += 1\n",
    "json.dump(upd_txt_data, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 files found\n",
      "657 samples found\n"
     ]
    }
   ],
   "source": [
    "### check progress\n",
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data/\"\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    data = {}\n",
    "    files = os.listdir(path)\n",
    "    print(\"{} files found\".format(len(files)))\n",
    "    for f in files:\n",
    "        if not '.json' in f: continue\n",
    "        data.update(json.load(open(os.path.join(path, f), \"r\")))\n",
    "else:\n",
    "    data = json.load(open(path, \"r\"))\n",
    "print(\"{} samples found\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3053\n",
      "4041\n",
      "6051\n"
     ]
    }
   ],
   "source": [
    "print(max([k for k in data if int(k)>3000 and int(k)<4000]))\n",
    "print(max([k for k in data if int(k)>4000 and int(k)<5000]))\n",
    "print(max([k for k in data if int(k)>6000 and int(k)<7000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 files found\n",
      "60 samples found\n"
     ]
    }
   ],
   "source": [
    "### Create demo from json (dataset is divided into 8 chunks of size 1000 for parallel distractor mining)\n",
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_txt_data/\"\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    data = {}\n",
    "    files = os.listdir(path)\n",
    "    print(\"{} files found\".format(len(files)))\n",
    "    for f in files:\n",
    "        if not '.json' in f: continue\n",
    "        data.update(json.load(open(os.path.join(path, f), \"r\")))\n",
    "else:\n",
    "    data = json.load(open(path, \"r\"))\n",
    "print(\"{} samples found\".format(len(data)))\n",
    "\n",
    "html = \"<html><body>\"\n",
    "html += \"<style>th {position: sticky; top: 0;background: FloralWhite;}</style>\"\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=gray><th width=5%>Index</th><th width=25%>Q & Pos Snippets</th><th width=40%>Neg Snippets</th><th width=30%>X_modal Facts</th></tr>'\n",
    "count = 0\n",
    "for k in data: #[34, 279, 450, 474, 563, 613, 712, 842, 1311, 1793, 1936, 2340, 4266, 5279, 5620, 5800, 6845, 7018, 7529]:\n",
    "    word_lists = [l.split(\" || \") for l in list(data[k]['word_lists'].values())]\n",
    "    sen2score = {}\n",
    "    for f in data[k]['new_negFacts']:\n",
    "        sen2score[f['fact']] = {\n",
    "            'title': f['title'],\n",
    "            'scores': tuple(float(x) for x in f['scores'][1:-1].split(\",\")),\n",
    "            'link': f['url']\n",
    "        }\n",
    "    cap2score = {}\n",
    "    for f in data[k]['img_negFacts']:\n",
    "        cap2score[f['caption']] = {\n",
    "            'title': f['title'],\n",
    "            'scores': tuple(float(x) for x in f['scores'][1:-1].split(\",\")),\n",
    "            'link': f['url'],\n",
    "            'img': f['imgUrl']\n",
    "        }\n",
    "    html += add_html_row(k, sen2score, cap2score, word_lists)\n",
    "    o = open('x_distractor_for_txt_demo2.html', 'wt')\n",
    "\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_txt_demo2.html', 'wt')\n",
    "\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  3787\n",
      "#pages before extension by hyperlink:  18\n",
      "#pages after extension by hyperlink:  42\n",
      "Q =  Which rival groups did the people who are currently the dominant ethnic group in the Xinjiang region compete with in the past?\n",
      "A =  They competed with other Altaic tribes, Indo-European empires from the south and west and Sino-Tibetan empires to the east.\n",
      "keywords =  {'dominant', 'ethnic', 'rival', 'Region', 'region', 'xinjiang', 'Past', 'Rival', 'Xinjiang', 'groups', 'group', 'Group', 'People', 'people', 'Dominant', 'Groups', 'Ethnic', 'past'}\n",
      "titlewords =  {'History', 'Xinjiang', 'Christianity', 'people', 'Uyghur'}\n",
      "answerwords =  {'tribes', 'indo', 'south', 'West', 'Sino', 'Tibetan', 'european', 'Other', 'Empires', 'Altaic', 'other', 'Indo', 'South', 'tibetan', 'east', 'European', 'East', 'empires', 'sino', 'Tribes', 'west', 'altaic'}\n",
      "goldfactwords =  {'Brief', 'Republic', 'powers', 'story', 'Christianity', 'muslim', 'Tribe', 'few', 'Mountains', 'Nomadic', 'nomadic', 'small', 'Story', 'republic', 'Religion', 'Small', 'Muslim', 'Central', 'Uyghur', 'asia', 'Christian', 'China', 'altai', 'Powers', 'mountains', 'minority', 'Few', 'Asia', 'religion', 'christianity', 'china', 'History', 'history', 'christian', 'brief', 'central', 'uyghur', 'tribe', 'Minority', 'Altai'}\n",
      " -- find sentences from each page --\n",
      "total num of sentences found =  385\n",
      "total num of imgs found =  6\n"
     ]
    }
   ],
   "source": [
    "### Mining + Create demo\n",
    "\n",
    "html = \"<html><body>\"\n",
    "html += \"<style>th {position: sticky; top: 0;background: FloralWhite;}</style>\"\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=gray><th width=5%>Index</th><th width=25%>Q & Pos Snippets</th><th width=40%>Neg Snippets</th><th width=30%>X_modal Facts</th></tr>'\n",
    "count = 0\n",
    "for k in [3787]: #[34, 279, 450, 474, 563, 613, 712, 842, 1311, 1793, 1936, 2340, 4266, 5279, 5620, 5800, 6845, 7018, 7529]:\n",
    "    count += 1\n",
    "    sen2score, cap2score, word_lists = get_sen2score_from_indx(k)\n",
    "    html += add_html_row(k, sen2score, cap2score, word_lists)\n",
    "    o = open('x_distractor_for_txt_demo3.html', 'wt')\n",
    "\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_txt_demo3.html', 'wt')\n",
    "\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "### no https: before '//upload'\n",
    "with open('0719demo.html','r') as f:\n",
    "    file = f.read()\n",
    "f.close()\n",
    "print(type(file))\n",
    "print(len(file))\n",
    "indices = [i for i in range(len(file)) if file.startswith('//upload.wikimedia.org/wikipedia/commons/', i)]\n",
    "count = 0\n",
    "for i in indices:\n",
    "    file = file[:i+6*count] + 'https:' + file[6*count+i:]\n",
    "    count += 1\n",
    "print(len(file))\n",
    "o = open('0719_demo.html', 'wt')\n",
    "\n",
    "o.write(file)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k in new_txt_data:\n",
    "    if all(['en.wikipedia.org/wiki/' not in f['url'] for f in new_txt_data[k]['SupportingFacts']]):\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pages before extension by hyperlink:  20\n",
      "#pages after extension by hyperlink:  40\n"
     ]
    }
   ],
   "source": [
    "titlewords, keywords, goldfactwords, answerwords, Q, A, anchor2page = get_keywords_and_relevant_pages_from_txt_sample(\"3435\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Who lived a longer life: Henry I, Count of Anhalt or Louis IV, Landgrave of Thuringia?\n",
      "279\n",
      "Were the attackers during the Battle of Malaga and the Catalonia Offensive of the Spanish Civil War Nationalist or Republican?\n",
      "349\n",
      "Did the Russian Revolution conclude before or after the Russian Famine began?\n",
      "361\n",
      "Did United Airlines Flight 811 and United Airlines Flight 553 crash in the same year or different years?\n",
      "448\n",
      "Was the album Goodnight Moon released before or after the book of the same name was published?\n",
      "450\n",
      "Which organization had more countries as members: the Arab League or the Entente Powers?\n",
      "474\n",
      "Has Yeshimon and the Judaean desert been urbanized or unaltered by human activity?\n",
      "514\n",
      "Is the abdominal area or the lower torso closer to the medial compartment that contains the gracilis muscle?\n",
      "563\n",
      "Who has been more prolific and varied in their career: Joel Coen or Anthony Cotton?\n",
      "613\n",
      "Which took place later in the 2017 calendar year: The attack on Saint Menas church or the Palm Sunday church bombings?\n",
      "712\n",
      "Which is older, the Burwood Villa house, or the Lynton heritage-listed residence in Burwood, Australia?\n",
      "777\n",
      "Which disorder might cause more fear in the person suffering from it, actually feeling paranoid because of it more; Capgras delusion or Amnesia?\n",
      "842\n",
      "Which state elected more representatives to the US house in 2016; Georgia or Hawaii?\n",
      "1006\n",
      "Do the words Pelota and Shinty have the same country origin or different origins?\n",
      "1089\n",
      "Which UN resolution is more pointed, directed more exactly at a specific location; 1663 or 1664?\n",
      "1118\n",
      "Which describes something more normal on the outside of a body; Carcinoma or surface ectoderm?\n",
      "1157\n",
      "Where is regional French more prevalent; France or Canada?\n",
      "1311\n",
      "Which botanist was older when they died; George Samuel Perrottet or Charles Louis L'Heritier de Brutelle?\n",
      "1342\n",
      "Did Kenton Keith sign with the Saskatchewan Roughriders before or after William Morgan made his first-team debut against Sheffield?\n",
      "1414\n",
      "Was The Socialist Propaganda League of America ( SPLA) established before or after the Brotherhood of the Cooperative Commonwealth?\n",
      "1519\n",
      "Which tribe depended more on heavy furs; Alaska Native tribes or Awaswas?\n",
      "1530\n",
      "Was the team formerly known as Chengdu Wuniu founded before or after the Chinese Volleyball Super League went fully professional?\n",
      "1681\n",
      "Who was born earlier, cardinal Juan Sandoval Iniguez of Jalisco, Mexico or Jesus Ocampo Geronimo of Lexington, Kentucky?\n",
      "1773\n",
      "Did the government buy land to the west of Slough and adjacent to the Great Western Railway main line to form a motor repair depot for army transport  earlier or later than the first section of the Great Western Railway between the original station at Paddington and the original station at Maidenhead opened?\n",
      "1793\n",
      "What is another name for polar light, northern light or southern light and are strong enough to alter trajectories of charged particles in solar wind and magnetospheric plasma?\n",
      "1825\n",
      "Which gained higher relative chart ranking; What a Beautiful Day (Film) or What a Beautiful Day (single song)?\n",
      "1882\n",
      "Was SUSE Linux or Lubuntu launched first?\n",
      "1901\n",
      "Did Georgia Lee of Australia and Georgie Dann of France start their musical careers in the same decade or different decades?\n",
      "1923\n",
      "In which event were fewer killed; Melbourne Storm season or Typhoon Kong-rey?\n",
      "1936\n",
      "Who was born earlier in the calendar year: Anni-Frid Lyngstad or Frida Amundsen?\n",
      "2340\n",
      "Which contained more features that shares a common name with insanity; Beach Boulevard (Hamilton Ontario) or Confederation Beach Park?\n",
      "2405\n",
      "Which refers more to a way of organizing or compiling things more; Compendium or Catechism?\n",
      "2824\n",
      "Which time period was longer, Duke Xiao's rule over the State of Qi or the Qing dynasty's rule over China?\n",
      "2825\n",
      "If you wanted to meet a person, would you be able to  meet Alexey Voyevoda or Polkovnik?\n",
      "3003\n",
      "If you were getting divorced, which would need to be more involved, directly; Registration District or District?\n",
      "3303\n",
      "Which event occurred first, the creation of the Royal Afghan Air Force or the end of foreign tourists visiting Afghanistan through the Kabul International Airport?\n",
      "3337\n",
      "Which has a vantage point that allows someone to look further; North Point Lighthouse Museum or North Point in St. Louis Missouri?\n",
      "3373\n",
      "Which musical group is older: Black Cherry or The Browns?\n",
      "3475\n",
      "Which is likely to be more interesting to someone who doesn't like cooking; Grantchester (TV series) or The Experienced English Housekeeper (book)?\n",
      "3719\n",
      "Which is a type of name that might be a handle for a user; name or nickname?\n",
      "3733\n",
      "Which was over first; Indian Civil Service or Battle of Palmito?\n",
      "3847\n",
      "Which church is older, the Concordia Lutheran Church of Frohna, Missouri or The First Baptist Church of Charleston, South Carolina?\n",
      "3878\n",
      "Are the Prime Ministers of the United Kingdom and Norway the same gender or different genders?\n",
      "3947\n",
      "Which came out first; Mary, Mary or Greatest hits (Mary Wells Album)?\n",
      "3975\n",
      "Did the Humboldt Broncos bus crash and Carrollton bus collision take place in the same country or different countries?\n",
      "4103\n",
      "Does the current Goalkeeper for Bath City have the same or different last name than the Tennessee Titans running back?\n",
      "4266\n",
      "Was the hammer throw event of the Summer Olympics held earlier in the year during the 1948 or 1952 events?\n",
      "4491\n",
      "Did Swindon resign from the Western league before or after the club turned professional?\n",
      "4522\n",
      "Was the Duchy of Parma created before the Treaty of Tastatt was signed or after it was signed?\n",
      "4725\n",
      "Does taxation in the United Kingdom occur at the more, less, or the same amount of government levels as it occurs in the United States?\n",
      "4803\n",
      "Which bird's feet enable it to swim better; Pigeons and doves or Heliornithidae?\n",
      "4864\n",
      "Who announced a provisional list of players for the UEFA Euro 2012 squads first, Bert van Marwijk or Blokhin?\n",
      "5035\n",
      "Was the additional dock siding of the main station at London Waterloo station opened before or after the LCDR connected Blackfriars to Farringdon via the Snow Hill Tunnel?\n",
      "5050\n",
      "Did Jon Stewart replace Craig Kilborn before or after Ron Bennington began his radio career on WYNF?\n",
      "5118\n",
      "Which is composed of more buildings, Nina Tower or The Tregunter Towers?\n",
      "5149\n",
      "Did the English colonists settle at Jamestown, Williamsburg, and the Northern Neck and along the James River before or after West Virginia formed by seceding from a Confederate state?\n",
      "5179\n",
      "What anti androgen for transexual M to Fs is deemed safe and accounts for about 80% or the potassium-sparing effect of it?\n",
      "5279\n",
      "Are Catholic University of America and Washington University in St. Louis both private, or is only one private?\n",
      "5346\n",
      "Which anemone is more voracious, eating more; Anthopleura xanthogrammica or Dahlia Anemones?\n",
      "5393\n",
      "Who was known by a shorter name: Antonio Alberto Bastos Pimparel or Ricardo dos Santos Nascimento?\n",
      "5620\n",
      "What is does a subfield of calculus have in common with a gear train with three shafts that has the property that the rotational speed of one shaft is the average of the speeds of the others, or a fixed multiple of that average?\n",
      "5695\n",
      "In massacres in 1984 where were more killed; Tamil or Putis?\n",
      "5705\n",
      "In the narrative intended to explain the origin of life or the universe, what is the highest deity credited with doing that is related to this, in the society that has the solar deity Ra?\n",
      "5745\n",
      "Which movie aired first, Austin Powers: The Spy Who Shagged Me or Austin Powers: International Man of Mystery?\n",
      "5787\n",
      "What song was released later: You're the One by Dwight Yoakam or Bing Monsters by Tragedy Khadafi?\n",
      "5800\n",
      "What do Mycoviruses and Fungivores both affect or eat?\n",
      "5951\n",
      "Which is more of a cure to something; Clonogenic assay or Adalimumab?\n",
      "6010\n",
      "Did Paramore's first album, All We Know Is Falling, or their self-titled album, Paramore, have more hit singles?\n",
      "6096\n",
      "Is locality a principal shared by physics and computer systems or by environmental pipelines and core branches?\n",
      "6220\n",
      "Which religious person got into more ethical trouble; George Bell or Peter Ball?\n",
      "6312\n",
      "Which was produced earlier; Northwest passage, Film, or Northwest Passage TV series?\n",
      "6313\n",
      "Did Bela IV rule before or after the birth of Margaret of Hungary?\n",
      "6399\n",
      "Are there more novels in the Twilight Saga or are there more current locations of the Twilight Zone Tower of Terror theme park ride?\n",
      "6449\n",
      "Did the song 'If I Ain't Got You' by Alicia Keys win the Best R&B Video accolade at the MTV Video Music Awards in the same or a different year as when her song 'My Boo' was released to US contemporary hit radio?\n",
      "6507\n",
      "What is the name of the region that has a range of mountains running from immediately south of Aberdeen westward, and is contained within all or part of the electoral districts of Buninyong?\n",
      "6748\n",
      "Which came first; Music of Friends or formation of Arrival the band?\n",
      "6767\n",
      "If you needed someone to help you get out of jail whom would you choose; Bobby knutt or Bobby diamond?\n",
      "6845\n",
      "What happens if the radiant point of the Orionids is at or below the horizon?\n",
      "6860\n",
      "Which band has more albums: U2 or The Steel Drivers?\n",
      "6939\n",
      "Which is more of a political leader, setting an example for others to follow; Kanye West or Martin of Tours?\n",
      "6956\n",
      "Which area includes more postcode districts, the UB postcode area or the BA postcode area?\n",
      "7018\n",
      "When Senator Robert Corker Jr was elected to begin his Senate participation in  2007,  did he fill a majority or minority seat?\n",
      "7037\n",
      "Which could you have possibly watched in your home with subtitles more effectively; Absolutely (TV series) or Abbasalutely?\n",
      "7172\n",
      "Which was something you could experience if you bought a ticket at a box office; Escape from New York or Metal Gear Solid V: Ground Zeroes?\n",
      "7191\n",
      "Which spirit is more likely to be seen as a sort of shade or dark entity, unfocused; Morgen or Ankou?\n",
      "7221\n",
      "Which has more variety in the creatures it is made up of: Nue or Shojo?\n",
      "7529\n",
      "Did the removal of Pope Elect Stephen from the list of legitimate popes occur during or after the papacy of Pope Pius XII?\n",
      "7548\n",
      "Which agreement consisted of the most countries: the Four Power Agreement or the Potsdam Agreement?\n",
      "7639\n",
      "The Exarchate of Ravenna or of Italy was held by a member of what empire with a capital named \"City of Constantine\"?\n",
      "7698\n",
      "Which happened first, the Theban War or the Battle of Plataea?\n"
     ]
    }
   ],
   "source": [
    "for k in new_txt_data:\n",
    "    Q = new_txt_data[k]['Q']\n",
    "    if ' or ' in Q:\n",
    "        if random.random() < 0.05:\n",
    "            print(k)\n",
    "            print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([b in \"in the same or different years\" for b in [' same ', ' or ', ' different ']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This feature is standardized on CD-R, CD-RW, DVD-R and DVD-RW, but not on DVD+R and DVD+RW, on which only Plextor optical drives support simulated writing so far.\n",
      "\n",
      "{'scores': (0.15, 0.0, 0.0, 0.15, 0.15), 'link': 'https://en.wikipedia.org/wiki/Optical_disc_recording_technologies', 'title': 'Optical disc recording technologies'}\n"
     ]
    }
   ],
   "source": [
    "for s in sen2score:\n",
    "    if 'This feature is standardized on CD-R' in s:\n",
    "        print(s)\n",
    "        print('')\n",
    "        print(sen2score[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DET', 'This'),\n",
       " ('NOUN', 'feature'),\n",
       " ('AUX', 'is'),\n",
       " ('VERB', 'standardized'),\n",
       " ('ADP', 'on'),\n",
       " ('PROPN', 'CD'),\n",
       " ('PUNCT', '-'),\n",
       " ('NOUN', 'R'),\n",
       " ('PUNCT', ','),\n",
       " ('PROPN', 'CD'),\n",
       " ('PUNCT', '-'),\n",
       " ('NOUN', 'RW'),\n",
       " ('PUNCT', ','),\n",
       " ('NOUN', 'DVD'),\n",
       " ('PUNCT', '-'),\n",
       " ('NOUN', 'R'),\n",
       " ('CCONJ', 'and'),\n",
       " ('NOUN', 'DVD'),\n",
       " ('PUNCT', '-'),\n",
       " ('PROPN', 'RW'),\n",
       " ('PUNCT', ','),\n",
       " ('CCONJ', 'but'),\n",
       " ('PART', 'not'),\n",
       " ('ADP', 'on'),\n",
       " ('PROPN', 'DVD+R'),\n",
       " ('CCONJ', 'and'),\n",
       " ('PROPN', 'DVD+RW'),\n",
       " ('PUNCT', ','),\n",
       " ('ADP', 'on'),\n",
       " ('DET', 'which'),\n",
       " ('ADV', 'only'),\n",
       " ('PROPN', 'Plextor'),\n",
       " ('ADJ', 'optical'),\n",
       " ('NOUN', 'drives'),\n",
       " ('VERB', 'support'),\n",
       " ('VERB', 'simulated'),\n",
       " ('NOUN', 'writing'),\n",
       " ('ADV', 'so'),\n",
       " ('ADV', 'far'),\n",
       " ('PUNCT', '.')]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nlp('This feature is standardized on CD-R, CD-RW, DVD-R and DVD-RW, but not on DVD+R and DVD+RW, on which only Plextor optical drives support simulated writing so far.')\n",
    "[(t.pos_, t.text) for t in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------ snippets -------------\n",
      "Kramer's physical format is used in all optical discs. {'scores': (0.29, 0.0, 0.0, 0.29, 0.29), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "The discs are then silkscreened or a label is otherwise applied. {'scores': (0.2, 0.0, 0.0, 0.2, 0.2), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "Most optical discs exhibit a characteristic iridescence as a result of the diffraction grating formed by its grooves. {'scores': (0.18, 0.0, 0.0, 0.18, 0.18), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "For computer data backup and physical data transfer, optical discs such as CDs and DVDs are gradually being replaced with faster, smaller solid-state devices, especially the USB flash drive. This trend is expected to continue as USB flash drives continue to increase in capacity and drop in price. {'scores': (0.18, 0.0, 0.01, 0.18, 0.16999999999999998), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "As a result, it was found that by using optical means large data storing devices can be made that in turn gave rise to the optical discs. {'scores': (0.18, 0.0, 0.02, 0.18, 0.16), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "Initially, optical discs were used to store broadcast-quality analog video, and later digital media such as music or computer software. {'scores': (0.15, 0.0, 0.0, 0.15, 0.15), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "Kramer's physical format is used in all optical discs. In 1975, Philips and MCA began to work together, and in 1978, commercially much too late, they presented their long-awaited Laserdisc in Atlanta. {'scores': (0.15, 0.0, 0.0, 0.15, 0.15), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "For computer data backup and physical data transfer, optical discs such as CDs and DVDs are gradually being replaced with faster, smaller solid-state devices, especially the USB flash drive. {'scores': (0.16, 0.0, 0.02, 0.16, 0.14), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "Write-once optical discs commonly have an organic dye (may also be a (Phthalocyanine) Azo dye, mainly used by Verbatim, or an oxonol dye, used by Fujifilm) recording layer between the substrate and the reflective layer. {'scores': (0.13, 0.0, 0.0, 0.13, 0.13), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      "Second-generation optical discs were for storing great amounts of data, including broadcast-quality digital video. {'scores': (0.15, 0.0, 0.02, 0.15, 0.13), 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      " ------------ imgs ------------- \n",
      " Minidiscs are magneto-optical discs used to store music. {'scores': (0.29, 0.0, 0.0, 0.29, 0.29), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/8/81/Memorex-minidisc.jpg/150px-Memorex-minidisc.jpg', 'link': 'https://en.wikipedia.org/wiki/Magneto-optical_disc', 'title': 'Magneto-optical drive'}\n",
      "Optical discs {'scores': (0.2, 0.0, 0.02, 0.2, 0.18000000000000002), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/5/5e/CDRom.png/111px-CDRom.png', 'link': 'https://en.wikipedia.org/wiki/Optical_disc_drive', 'title': 'Optical disc drive'}\n",
      " Assortment of flash drives {'scores': (0.17, 0.0, 0.0, 0.17, 0.17), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Custom_USB_Drives.JPG/220px-Custom_USB_Drives.JPG', 'link': 'https://en.wikipedia.org/wiki/Flash_drives', 'title': 'Flash drive'}\n",
      " The optical lens of a compact disc drive. {'scores': (0.12, 0.0, 0.0, 0.12, 0.12), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/2/22/CD_drive_lens.jpg/200px-CD_drive_lens.jpg', 'link': 'https://en.wikipedia.org/wiki/Optical_discs', 'title': 'Optical disc'}\n",
      " A Magneto-optical disc and the sector partition rectangles on its surface. {'scores': (0.11, 0.0, 0.0, 0.11, 0.11), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/5/53/MO-disk-surface.jpg/150px-MO-disk-surface.jpg', 'link': 'https://en.wikipedia.org/wiki/Magneto-optical_disc', 'title': 'Magneto-optical drive'}\n",
      " Two 2.5\" external USB hard drives {'scores': (0.11, 0.0, 0.0, 0.11, 0.11), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/7/76/WD_External_Hard_Drives_IMG_7899.jpg/220px-WD_External_Hard_Drives_IMG_7899.jpg', 'link': 'https://en.wikipedia.org/wiki/Hard_disk_drive', 'title': 'Hard disk drive'}\n",
      " A 130 mm 2.6GB magneto-optical disc. {'scores': (0.1, 0.0, 0.02, 0.1, 0.08), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/9/9e/2GB-MO-disk.jpg/150px-2GB-MO-disk.jpg', 'link': 'https://en.wikipedia.org/wiki/Magneto-optical_disc', 'title': 'Magneto-optical drive'}\n",
      " A 230 MB Fujitsu 90 mm magneto-optical disc. {'scores': (0.09, 0.0, 0.02, 0.09, 0.06999999999999999), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/a/a8/3point5_inch_MO_cartridge.jpg/150px-3point5_inch_MO_cartridge.jpg', 'link': 'https://en.wikipedia.org/wiki/Magneto-optical_disc', 'title': 'Magneto-optical drive'}\n",
      " From left to right: full-height 5.25″ drive, two half-height 5.25″ drives, and (sideways) a 3.5″ drive {'scores': (0.08, 0.0, 0.02, 0.08, 0.06), 'img': '//upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Drive_bays.jpg/350px-Drive_bays.jpg', 'link': 'https://en.wikipedia.org/wiki/Drive_bay', 'title': 'Drive bay'}\n"
     ]
    }
   ],
   "source": [
    "print(' ------------ snippets -------------')\n",
    "for k in list(sen2score.keys())[:10]:\n",
    "    print(k, sen2score[k])\n",
    "print(' ------------ imgs ------------- ')\n",
    "for k in list(cap2score.keys())[:10]:\n",
    "    print(k, cap2score[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Minidiscs are magneto-optical discs used to store music.',\n",
       " 'Optical discs',\n",
       " ' Assortment of flash drives',\n",
       " ' The optical lens of a compact disc drive.',\n",
       " ' A Magneto-optical disc and the sector partition rectangles on its surface.',\n",
       " ' Two 2.5\" external USB hard drives',\n",
       " ' A 130 mm 2.6GB magneto-optical disc.',\n",
       " ' A 230 MB Fujitsu 90 mm magneto-optical disc.',\n",
       " ' From left to right: full-height 5.25″ drive, two half-height 5.25″ drives, and (sideways) a 3.5″ drive']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap2score = dict(sorted(cap2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "list(cap2score.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
