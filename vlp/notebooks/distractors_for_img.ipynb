{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, os\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import copy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from itertools import tee\n",
    "import wikipedia\n",
    "import pylcs\n",
    "import string\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATIONS = set(string.punctuation)\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*|\\(|\\)|-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362367\n"
     ]
    }
   ],
   "source": [
    "img_meta = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data/img_metadata-Copy1.json\", \"r\"))\n",
    "print(len(img_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25467\n"
     ]
    }
   ],
   "source": [
    "img_dataset = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/dataset_J_split_update0721.json\", \"r\"))\n",
    "print(len(img_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['NUM', 'NOUN', 'ADJ', 'PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(A, B):\n",
    "    intersection = len(A.intersection(B))\n",
    "    union = len(A.union(B))\n",
    "    return round(intersection / (union+1e-7), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_from_page_for_img_data(title, page, keywords, answerwords):\n",
    "    try: \n",
    "        content = wikipedia.page(title, auto_suggest=False, redirect=True).content\n",
    "        paragraphs = content[:content.find('== References ==')].split('\\n')\n",
    "        \n",
    "    except: return {}\n",
    "    #records = []\n",
    "    sen2score = {}\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            #records.append(-999)\n",
    "            doc = nlp(p)\n",
    "            for s in doc.sents:\n",
    "                if len(s) < 10: \n",
    "                    continue\n",
    "                nouns_in_s = [t.text for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A > 0.06:\n",
    "                    sen2score[s.text] = {'scores': (IoU_Q, IoU_A, IoU_Q - IoU_A), 'link': page, 'title': title}\n",
    "                #records.append(round(IoU_Q, 2))\n",
    "    #print(records)\n",
    "\n",
    "    #records = []\n",
    "    for p in paragraphs:\n",
    "        if len(p.split()) >= 10:\n",
    "            #records.append(-999)\n",
    "            doc = nlp(p)\n",
    "            it1, it2 = tee(doc.sents)\n",
    "            next(it2, None)\n",
    "            for s1, s2 in zip(it1, it2):\n",
    "                if len(s1) < 5 or len(s2) < 5 or len(s1)+len(s2) > 70 or len(s1)+len(s2) < 10: \n",
    "                    continue \n",
    "                nouns_in_s = [t.text for s in [s1, s2] for t in s if (t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper()))]\n",
    "\n",
    "                IoU_Q = IoU(set(nouns_in_s), keywords)\n",
    "                IoU_A = IoU(set(nouns_in_s), answerwords)\n",
    "                if IoU_Q -  IoU_A >= 0.06:\n",
    "                    sen2score[\" \".join([s1.text, s2.text])] = {'scores': (IoU_Q, IoU_A, IoU_Q - IoU_A), 'link': page, 'title': title}\n",
    "                    #print(s)\n",
    "                #records.append(round(IoU_Q, 2))\n",
    "    #print(records)\n",
    "    #print(len(sen2score))\n",
    "    return sen2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_img_sample(k):\n",
    "    Q = img_dataset[str(k)]['Q'].replace('\"', '').replace('_', ' ')\n",
    "    doc = nlp(Q)\n",
    "    keywords = set([t.text for s in doc.sents for t in s if t.pos_ in ['NUM', 'PROPN', 'ADJ', 'NOUN'] or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    keywords = keywords - PUNCTUATIONS\n",
    "    keywords = set(sum([[w.capitalize(), w.lower()] for w in keywords], []))\n",
    "    \n",
    "    ### Extract noun chunks\n",
    "    proper_words = [t.text for s in doc.sents for t in s if t.pos_ in ['NUM', 'PROPN', 'ADJ'] or ((not t.is_sent_start) and t.text[0].isupper())]\n",
    "    chunks = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if any([n in proper_words for n in chunk.text.split()]):\n",
    "            chunks.add(chunk.text)\n",
    "    if not chunks: \n",
    "        chunks = chunks.union([c.text for c in doc.noun_chunks])\n",
    "        chunks = chunks.union([t.text for s in doc.sents for t in s if t.pos_ == 'PROPN' or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "    \n",
    "    A = img_dataset[str(k)]['A'].replace('\"', '')\n",
    "    doc = nlp(A)\n",
    "    answerwords = set([t.text for t in doc if t.pos_ in pos_list or ((not t.is_sent_start) and t.text[0].isupper())]) - keywords\n",
    "    answerwords = answerwords - PUNCTUATIONS\n",
    "    answerwords = set(sum([[w.capitalize(), w.lower()] for w in answerwords], []))\n",
    "    \n",
    "    return keywords, answerwords, Q, A, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What colors\n",
      "the Cheverny and Dicentra cucullaria flowers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7406b10a8aff4a4494dc02d575f1926f-0\" class=\"displacy\" width=\"1625\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">What</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">colors</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Cheverny</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Dicentra</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">cucullaria</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">flowers?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M905.0,441.5 L913.0,429.5 897.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-5\" stroke-width=\"2px\" d=\"M770,439.5 C770,264.5 1085.0,264.5 1085.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,441.5 L1093.0,429.5 1077.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7406b10a8aff4a4494dc02d575f1926f-0-7\" stroke-width=\"2px\" d=\"M420,439.5 C420,2.0 1450.0,2.0 1450.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7406b10a8aff4a4494dc02d575f1926f-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,441.5 L1458.0,429.5 1442.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"What colors are the Cheverny and Dicentra cucullaria flowers?\")\n",
    "for c in doc.noun_chunks:\n",
    "    print(c.text)\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 8\n",
      "cucullaria flowers\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.dep_ == 'amod' or token.dep_ == 'compound':\n",
    "        print(token.i, token.head.i)\n",
    "        print(doc[token.i: token.head.i+1].text if token.head.i > token.i else doc[token.head.i:token.i+1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given img_dataset indx & title, find sentences with word overlap with the question\n",
    "def find_sentences_from_indx_for_img(k, keywords, answerwords, chunks):\n",
    "    sen2score = {}\n",
    "    candidate_pages, updated_chunks = noun_chunk2candidate_page(chunks, k)\n",
    "    #print(\"num of candidate pages = {}\\n\".format(len(candidate_pages)))\n",
    "    for title in candidate_pages:\n",
    "        page = \"https://en.wikipedia.org/wiki/\" + \"_\".join(title.split())\n",
    "        sen2score.update(find_sentences_from_page_for_img_data(title, page, keywords, answerwords))\n",
    "    sen2score = dict(sorted(sen2score.items(), key=lambda x: x[1]['scores'][-1], reverse=True))\n",
    "    return sen2score, candidate_pages, updated_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_html_row_x_distractor_for_img(k, sen2score, word_lists, chunks, pages, colors=[\"(205, 245, 252)\", \"(255, 214, 222)\"]):\n",
    "    html = '<tr><td>{}.</td>'.format(k)\n",
    "    Q = img_dataset[str(k)]['Q'].replace('\"', '')\n",
    "    html += '<td>Q: {}<br><br>'.format(highlight_words(word_lists, chunks, colors, Q))\n",
    "    A = img_dataset[str(k)]['A'].replace('\"', '')\n",
    "    for gid in img_dataset[str(k)]['GoldIds']:\n",
    "        img = img_meta[str(int(gid))]\n",
    "        html += '<a href=\"{}\" target=\"_blank\"><img style=\"display:block; max-height:300px; max-width:100%;\" src = \"{}\"></a>'.format(img['page'], img['src'])\n",
    "        html += '<br>Title = {}<br>Description = {}<br><br>'.format(highlight_words(word_lists, [], colors, img['name'].replace(\"_\", \" \")), highlight_words(word_lists, [], colors, img['description'].replace(\"_\", \" \")))\n",
    "    html += 'A: {}<br><br>'.format(highlight_words(word_lists, [], colors, A))\n",
    "    html += '<span class=\"hid\" style=\"display: none\"><b>Relevant Wikipedia Pages: </b>{}</span>'.format(', '.join(pages))\n",
    "    html += '<br><button onclick=\"btn_click($(this));\">Toggle details</button></td><td>'\n",
    "    \n",
    "    for s in list(sen2score.keys())[:10]:\n",
    "        html += '{} --- {} '.format(highlight_words(word_lists, [], colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"  target=\"_blank\"> {}</a><br><br>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "    for s in list(sen2score.keys())[10:]:\n",
    "        html += '<span class=\"hid\" style=\"display: none\">{} --- {} '.format(highlight_words(word_lists, [], colors, s), str(sen2score[s]['scores']))\n",
    "        html += '<a href=\"{}\"  target=\"_blank\"> {}</a><br><br></span>'.format(sen2score[s]['link'], sen2score[s]['title'])\n",
    "        \n",
    "    html += '</td></tr>'\n",
    "    html += '<tr><td colspan=3><hr></td></tr>'\n",
    "    return html.encode('ascii', 'xmlcharrefreplace').decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words(word_lists, chunks, colors, sentence):\n",
    "    s = copy.deepcopy(sentence)\n",
    "    if \"\".join(chunks):\n",
    "        s = re.sub(r'\\s*(' + r'|'.join([re.escape(c) for c in chunks]) + r')\\s*', lambda m: '<span class=\"chunk\">{}</span>'.format(m.group()), s)\n",
    "    for word_list, color in zip(word_lists, colors):\n",
    "        if \"\".join(word_list): s = re.sub(r'\\b(' + r'|'.join(word_list) + r')\\b', lambda m: '<span style=\"background-color:rgb{}\">{}</span>'.format(color, m.group()), s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunk2candidate_page(chunks, k):\n",
    "    pages = set()\n",
    "    for chunk in chunks:\n",
    "        pages = pages.union(set(wikipedia.search(chunk)))\n",
    "    if len(pages) < 5:\n",
    "        print(k)\n",
    "        Q = img_dataset[str(k)]['Q'].replace('\"', '').replace('_', ' ')\n",
    "        doc = nlp(Q)\n",
    "        #more_chunks = set([c.text for c in doc.noun_chunks])\n",
    "        more_chunks = set()\n",
    "        more_chunks = more_chunks.union([t.text for s in doc.sents for t in s if t.pos_ == 'PROPN' or ((not t.is_sent_start) and t.text[0].isupper())])\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'amod' or token.dep_ == 'compound':\n",
    "                more_chunks.add(doc[token.i: token.head.i+1].text if token.head.i > token.i else doc[token.head.i:token.i+1].text)\n",
    "        more_chunks = more_chunks - chunks\n",
    "        print(Q)\n",
    "        print(\"More chunks: \", more_chunks)\n",
    "        more_pages = set()\n",
    "        for chunk in more_chunks:\n",
    "            more_pages = more_pages.union(wikipedia.search(chunk))\n",
    "        print('add {} more chunks, {} more pages'.format(len(more_chunks), len(more_pages)))\n",
    "        pages = pages.union(more_pages)\n",
    "        chunks = chunks.union(more_chunks)\n",
    "    return pages, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679\n",
      "Q =  Does Nathan's at Six Flags Great Adventure have its name displayed in more than one spot on the outside of its building?\n",
      "Keywords = {'six', 'More', 'Great', 'great', 'Six', 'building', 'spot', 'one', 'Nathan', 'Name', 'name', 'Flags', 'adventure', 'nathan', 'One', 'Building', 'more', 'flags', 'Spot', 'Adventure', 'outside', 'Outside'}\n",
      "A =  Yes\n",
      "answerwords = set()\n",
      "Noun chunks:  {'Nathan', 'Six Flags', 'Great Adventure', 'more than one spot'}\n",
      " \n",
      "680\n",
      "Q =  Does Hot Doug’s in Chicago have something attached to the building?\n",
      "Keywords = {'Doug', 'hot', 'Hot', 'Chicago', 'chicago', 'Building', 'doug', 'building'}\n",
      "A =  Yes\n",
      "answerwords = set()\n",
      "Noun chunks:  {'Hot Doug', 'Chicago'}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "### Mining + Save as json\n",
    "upd_img_data = {}\n",
    "try: upd_img_data = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_img_data/upd_img_data_{}.json\".format(1000), \"r\"))\n",
    "except: upd_img_data = {}\n",
    "for k in [678, 679, 680]: #random.sample(range(25467), 2):\n",
    "    if str(k) in upd_img_data: continue\n",
    "    #if k%1 == 0: json.dump(upd_img_data, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_img_data/upd_img_data.json\", \"w\"), indent=4)\n",
    "    upd_img_data[str(k)] = copy.deepcopy(img_dataset[str(k)])\n",
    "    upd_img_data[str(k)]['DistractorFacts'] = []\n",
    "    keywords, answerwords, Q, A, chunks = get_keywords_from_img_sample(k)\n",
    "    d, pages, chunks = find_sentences_from_indx_for_img(k, keywords, answerwords, chunks)\n",
    "    print(k)\n",
    "    print(\"Q = \", Q)\n",
    "    print(\"Keywords = {}\".format(keywords))\n",
    "    print(\"A = \", A)\n",
    "    print(\"answerwords = {}\".format(answerwords))\n",
    "    print(\"Noun chunks: \", chunks)\n",
    "    print(' ')\n",
    "    word_lists = [keywords, answerwords]\n",
    "    upd_img_data[str(k)]['word_lists'] = {\n",
    "        'keywords': \" || \".join(word_lists[0]),\n",
    "        'answerwords': \" || \".join(word_lists[1]),\n",
    "        'noun_chunks': \" || \".join(chunks)\n",
    "    }\n",
    "    \n",
    "    DistractorFacts_count = 0\n",
    "    for s in d:\n",
    "        if DistractorFacts_count >= 40: break\n",
    "        if len(s.split()) in range(22, 60):\n",
    "            upd_img_data[str(k)]['DistractorFacts'].append({\n",
    "                'title': d[s]['title'],\n",
    "                'scores': str(d[s]['scores']),\n",
    "                'fact': s,\n",
    "                'url': d[s]['link'] \n",
    "            })\n",
    "            DistractorFacts_count += 1\n",
    "json.dump(upd_img_data, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_img_data/upd_img_data_{}.json\".format(1000), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 files found\n",
      "24861 samples found\n"
     ]
    }
   ],
   "source": [
    "### check progress\n",
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_img_data/\"\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    finished_samples = []\n",
    "    files = os.listdir(path)\n",
    "    print(\"{} files found\".format(len(files)))\n",
    "    for f in files:\n",
    "        if not '.json' in f: continue\n",
    "        finished_samples.extend(list(json.load(open(os.path.join(path, f), \"r\")).keys()))\n",
    "else:\n",
    "    finished_samples.extend(list(json.load(open(path, \"r\")).keys()))\n",
    "print(\"{} samples found\".format(len(finished_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 143), (1, 128), (2, 114), (3, 105), (4, 100)]\n",
      "590\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_img_data/\"\n",
    "\n",
    "snippets_count = []\n",
    "\n",
    "no_snippet_k = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "data = {}\n",
    "for f in files:\n",
    "    if not '.json' in f: continue\n",
    "    x = json.load(open(os.path.join(path, f), \"r\"))\n",
    "    for k in x:\n",
    "        num_snippets = len(x[k]['DistractorFacts'])\n",
    "        if num_snippets == 0:\n",
    "            no_snippet_k.append(k)\n",
    "        snippets_count.append(num_snippets)\n",
    "    data.update(x)\n",
    "print(sorted(Counter(snippets_count).items())[:5])\n",
    "print(len([k for k in data if len(data[k]['DistractorFacts'])< 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 143), (1, 128), (2, 114), (3, 105), (4, 100), (5, 115), (6, 126), (7, 111), (8, 127), (9, 110), (10, 124), (11, 124), (12, 139), (13, 134), (14, 129), (15, 130), (16, 140), (17, 138), (18, 142), (19, 137), (20, 142), (21, 116), (22, 141), (23, 137), (24, 137), (25, 116), (26, 130), (27, 141), (28, 130), (29, 124), (30, 142), (31, 123), (32, 111), (33, 104), (34, 138), (35, 125), (36, 129), (37, 120), (38, 128), (39, 113), (40, 19457), (41, 11)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(snippets_count).items()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Create demo from json (dataset is divided into 8 chunks of size 3000 for parallel distractor mining)\n",
    "path = \"/home/yingshac/CYS/WebQnA/WebQnA_data_new/upd_img_data/\"\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    data = {}\n",
    "    files = os.listdir(path)\n",
    "    print(\"{} files found\".format(len(files)))\n",
    "    for f in files:\n",
    "        if not '.json' in f: continue\n",
    "        data.update(json.load(open(os.path.join(path, f), \"r\")))\n",
    "else:\n",
    "    data = json.load(open(path, \"r\"))\n",
    "print(\"{} samples found\".format(len(data)))\n",
    "\n",
    "html = '<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">'\n",
    "html += '<script src=\"https://code.jquery.com/jquery-3.2.1.min.js\" integrity=\"sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=\" crossorigin=\"anonymous\"></script>'\n",
    "html += '<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>'\n",
    "html += '<!DOCTYPE html><html><head><meta http-equiv=\"content-type\" content=\"text/html; chatset=\"UTF-8\"><body>'\n",
    "html += '<script>$(\"img\").on(\"error\", function(){console.log($(this).attr(\"src\"));});'\n",
    "html += 'function btn_click(btn){$(btn).parent().parent().find(\".hid\").toggle();}</script>'\n",
    "html += '<style>table {border-collapse: separate;border-spacing: 10px;}\\n'\n",
    "html += '.chunk {text-decoration: underline solid rgb(227, 123, 253) 3px;}\\n'\n",
    "html += 'button {background-color:white; border: 2px solid #4CAF50; color: black; padding: 0px 8px; text-align: center; display: inline-block; font-size: 14px; margin: 4px 2px; transition-duration: 0.4s; cursor: pointer; }'\n",
    "html += 'button:hover {background-color: #4CAF50;color: white;}'\n",
    "html += 'th {position: sticky; top: 0;background: FloralWhite;}</style>'\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=lightblue style=\"text-align: center;\"><th width=5%>Index</th><th width=35%>Q & Pos Facts</th><th width=60%>Neg Facts</th></tr>'\n",
    "\n",
    "for k in random.sample(list(data.keys()), 12):\n",
    "    word_lists = [l.split(\" || \") for l in [data[k]['word_lists']['keywords'], data[k]['word_lists']['answerwords']]]\n",
    "    sen2score = {}\n",
    "    for f in data[k]['DistractorFacts']:\n",
    "        sen2score[f['fact']] = {\n",
    "            'title': f['title'],\n",
    "            'scores': tuple(float(x) for x in f['scores'][1:-1].split(\",\")),\n",
    "            'link': f['url']\n",
    "        }\n",
    "    print(\"{} DistractorFacts in sample {}\".format(len(sen2score), k))\n",
    "    chunks = data[k]['word_lists']['noun_chunks'].split(\" || \")\n",
    "    pages = [sen2score[s]['title'] for s in sen2score]\n",
    "    html += add_html_row_x_distractor_for_img(k, sen2score, word_lists, chunks, pages, colors=[\"(193, 239, 253)\", \"(255, 214, 222)\"])\n",
    "    o = open('x_distractor_for_img_demo3.html', 'wt')\n",
    "\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_img_demo3.html', 'wt')\n",
    "\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20260\n",
      "Q =  Which monument has flowers around it: A monument in the Sea Garden of the town of Tsarevo, Bulgaria or Soldier statue. - Orczy garden, Budapest District VIII?\n",
      "Keywords = {'Orczy', 'Bulgaria', 'monument', 'Sea', 'Garden', 'statue', 'bulgaria', 'district', 'Town', 'Flowers', 'Soldier', 'budapest', 'town', 'sea', 'tsarevo', 'Statue', 'viii', 'Budapest', 'orczy', 'garden', 'flowers', 'soldier', 'District', 'Monument', 'Tsarevo', 'Viii'}\n",
      "A =  A monument in the Sea Garden of the town of Tsarevo, Bulgaria\n",
      "answerwords = set()\n",
      "Noun chunks:  {'the Sea Garden', 'Bulgaria', 'Tsarevo', 'Budapest District VIII', 'Soldier', 'Orczy garden'}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/yingshac/miniconda3/envs/py37/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24539\n",
      "Q =  What color is the rope lining the Isles at the The Old Library, Trinity College in Dublin?\n",
      "Keywords = {'The', 'library', 'Old', 'college', 'color', 'trinity', 'rope', 'dublin', 'the', 'Library', 'College', 'Isles', 'Color', 'old', 'Dublin', 'isles', 'Rope', 'Trinity'}\n",
      "A =  Green.\n",
      "answerwords = {'green', 'Green'}\n",
      "Noun chunks:  {'Trinity College', 'Dublin', 'the The Old Library', 'the Isles'}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "### Mining + Create demo\n",
    "html = '<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css\">'\n",
    "html += '<script src=\"https://code.jquery.com/jquery-3.2.1.min.js\" integrity=\"sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=\" crossorigin=\"anonymous\"></script>'\n",
    "html += '<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js\"></script>'\n",
    "html += '<!DOCTYPE html><html><head><meta http-equiv=\"content-type\" content=\"text/html; chatset=\"UTF-8\"><body>'\n",
    "html += '<script>$(\"img\").on(\"error\", function(){console.log($(this).attr(\"src\"));});'\n",
    "html += 'function btn_click(btn){$(btn).parent().parent().find(\".hid\").toggle();}</script>'\n",
    "html += '<style>table {border-collapse: separate;border-spacing: 10px;}\\n'\n",
    "html += '.chunk {text-decoration: underline solid rgb(227, 123, 253) 3px;}\\n'\n",
    "html += 'button {background-color:white; border: 2px solid #4CAF50; color: black; padding: 0px 8px; text-align: center; display: inline-block; font-size: 14px; margin: 4px 2px; transition-duration: 0.4s; cursor: pointer; }'\n",
    "html += 'button:hover {background-color: #4CAF50;color: white;}'\n",
    "html += 'th {position: sticky; top: 0;background: FloralWhite;}</style>'\n",
    "html += '<table border=\"0\" style=\"table-layout: fixed; width: 100%; word-break:break-word\">'\n",
    "html += '<tr bgcolor=lightblue style=\"text-align: center;\"><th width=5%>Index</th><th width=35%>Q & Pos Facts</th><th width=60%>Neg Facts</th></tr>'\n",
    "x = []\n",
    "for k in random.sample(list(img_dataset.keys()), 3):\n",
    "    if img_dataset[k]['Qcate'] == 'YesNo': continue\n",
    "    print(k)\n",
    "    keywords, answerwords, Q, A, chunks = get_keywords_from_img_sample(k)\n",
    "    print(\"Q = \", Q)\n",
    "    print(\"Keywords = {}\".format(keywords))\n",
    "    print(\"A = \", A)\n",
    "    print(\"answerwords = {}\".format(answerwords))\n",
    "    print(\"Noun chunks: \", chunks)\n",
    "    print(' ')\n",
    "    d, pages, chunks = find_sentences_from_indx_for_img(k, keywords, answerwords, chunks)\n",
    "    x.append(len(d))\n",
    "    \n",
    "    word_lists = [keywords, answerwords]\n",
    "    html += add_html_row_x_distractor_for_img(k, d, word_lists, chunks, pages, colors=[\"(193, 239, 253)\", \"(255, 214, 222)\"])\n",
    "    o = open('x_distractor_for_img_demo2.html', 'wt')\n",
    "    o.write(html)\n",
    "    o.close()\n",
    "html += '</table></body></html>'\n",
    "o = open('x_distractor_for_img_demo2.html', 'wt')\n",
    "o.write(html)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
