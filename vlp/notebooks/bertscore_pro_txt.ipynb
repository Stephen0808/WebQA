{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, time, copy\n",
    "import math\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from word2number import w2n\n",
    "import string, re\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\",\"textcat\",\"parser\"])\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/yingshac/CYS/WebQnA/VLP/BARTScore\")\n",
    "from bart_score import BARTScorer\n",
    "#bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_scorer_ParaBank = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    "bart_scorer_ParaBank.load(path='/home/yingshac/CYS/WebQnA/VLP/BARTScore/bart.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE = str.maketrans(dict.fromkeys(string.punctuation)) \n",
    "def normalize_text_for_bart(x):\n",
    "    return \" \".join(x.translate(TABLE).split())\n",
    "#def compute_bartscore(c, a, switch=False):\n",
    "    #if switch: score = np.exp(bart_scorer.score(c, a))\n",
    "    #else: score = np.exp(bart_scorer.score(a, c))\n",
    "    #return score\n",
    "def compute_bartscore_ParaBank(c, a, switch=False):\n",
    "    c_removepunc = [normalize_text_for_bart(x) for x in c]\n",
    "    a_removepunc = [normalize_text_for_bart(x) for x in a]\n",
    "    if switch: score = np.exp(bart_scorer_ParaBank.score(c_removepunc, a_removepunc))\n",
    "    else: score = np.exp(bart_scorer_ParaBank.score(a_removepunc, c_removepunc))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectNum(l):\n",
    "    result = []\n",
    "    for w in l:\n",
    "        try: result.append(str(int(w)))\n",
    "        except: pass\n",
    "    return result\n",
    "def toNum(word):\n",
    "    if word == 'point': return word\n",
    "    try: return w2n.word_to_num(word)\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def normalize_text(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text): # additional: converting numbers to digit form\n",
    "        return \" \".join([str(toNum(w)) for w in text.split()])\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation) - set(['.'])\n",
    "        text1 = \"\".join(ch for ch in text if ch not in exclude)\n",
    "        return re.sub(r\"\\.(?!\\d)\", \"\", text1) # remove '.' if it's not a decimal point\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def lemmatization(text):\n",
    "        return \" \".join([token.lemma_ for token in nlp(text)])\n",
    "\n",
    "    if len(s.strip()) == 1:\n",
    "        # accept article and punc if input is a single char\n",
    "        return white_space_fix(lower(s))\n",
    "    elif len(s.strip().split()) == 1: \n",
    "        # accept article if input is a single word\n",
    "        return lemmatization(white_space_fix(remove_punc(lower(s))))\n",
    "\n",
    "    return lemmatization(white_space_fix(remove_articles(remove_punc(lower(s)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA Eval (SQuAD style EM, F1)\n",
    "def compute_vqa_metrics(cands, a, exclude=\"\", domain=None):\n",
    "    if len(cands) == 0: return (0,0,0)\n",
    "    bow_a = normalize_text(a).split()\n",
    "    F1 = []\n",
    "    EM = 0\n",
    "    RE = []\n",
    "    PR = []\n",
    "    e = normalize_text(exclude).split()\n",
    "    for c in cands:\n",
    "        bow_c = [w for w in normalize_text(c).split() if not w in e]\n",
    "        if domain == {\"NUMBER\"}: bow_c = detectNum(bow_c)\n",
    "        elif domain is not None: \n",
    "            bow_c = list(domain.intersection(bow_c))\n",
    "            bow_a = list(domain.intersection(bow_a))\n",
    "        \n",
    "        #print(bow_c)\n",
    "        #print(bow_a)\n",
    "        if bow_c == bow_a:\n",
    "            EM = 1\n",
    "        common = Counter(bow_a) & Counter(bow_c)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return (0,0,0,0,0)\n",
    "        precision = 1.0 * num_same / len(bow_c)\n",
    "        recall = 1.0 * num_same / len(bow_a)\n",
    "        RE.append(recall)\n",
    "        PR.append(precision)\n",
    "\n",
    "        f1 = 2*precision*recall / (precision + recall + 1e-5)\n",
    "        F1.append(f1)\n",
    "    \n",
    "    PR_avg = np.mean(PR)\n",
    "    RE_avg = np.mean(RE)\n",
    "    F1_avg = np.mean(F1)\n",
    "    F1_max = np.max(F1)\n",
    "    return (F1_avg, F1_max, EM, RE_avg, PR_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'train': 17812, 'test': 4076, 'val': 2455})\n",
      "24343\n"
     ]
    }
   ],
   "source": [
    "txt_dataset = json.load(open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/txt_dataset_0823_clean_te.json\", \"r\"))\n",
    "\n",
    "print(Counter([txt_dataset[k]['split'] for k in txt_dataset]))\n",
    "print(len(set([txt_dataset[k]['Guid'] for k in txt_dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1366, 5: 1343, 4: 874, 3: 493})\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for k in txt_dataset:\n",
    "    if txt_dataset[k]['split'] == 'test':\n",
    "        x.append(len(txt_dataset[k]['A']))\n",
    "print(Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n",
      "3499\n",
      "3999\n",
      "4076\n",
      "defaultdict(<class 'int'>, {})\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "### Normalization, txt data\n",
    "bleu4_txt_clean = {}\n",
    "RE_txt_clean = {}\n",
    "F1_txt_clean = {}\n",
    "mul_txt_clean = {}\n",
    "\n",
    "drop = defaultdict(int)\n",
    "for k in txt_dataset:\n",
    "    if not 'test' in txt_dataset[k]['split']: continue\n",
    "    bleu4_txt_clean[k] = []\n",
    "    RE_txt_clean[k] = []\n",
    "    mul_txt_clean[k] = []\n",
    "    F1_txt_clean[k] = []\n",
    "    datum = txt_dataset[k]\n",
    "    Keywords_A = datum['Keywords_A'].replace('\"', \"\")\n",
    "    all_A = [a.replace('\"', \"\") for a in datum['A']]\n",
    "    scores = np.zeros((len(all_A), len(all_A)))\n",
    "    c_list = []\n",
    "    a_list = []\n",
    "    for i in range(len(all_A)):\n",
    "        for j in range(len(all_A)):\n",
    "            c_list.append(all_A[i])\n",
    "            a_list.append(all_A[j])\n",
    "    scores = compute_bartscore_ParaBank(c_list, a_list).reshape((len(all_A), len(all_A)))\n",
    "    for i in range(len(all_A)):\n",
    "        Q = datum['Q'].replace('\"', \"\")\n",
    "        C = [all_A[i]]\n",
    "        \n",
    "        score = np.max([min(1, scores[i][j]/scores[j][j]) for j in range(len(all_A)) if not i==j])\n",
    "        \n",
    "        F1_avg, F1_max, EM, RE_avg, PR_avg = compute_vqa_metrics(C, Keywords_A)\n",
    "        bleu4_txt_clean[k].append(score)\n",
    "        RE_txt_clean[k].append(RE_avg)\n",
    "        F1_txt_clean[k].append(F1_avg)\n",
    "        mul_txt_clean[k].append(RE_avg * score)\n",
    "        \n",
    "    if len(RE_txt_clean) % 500 == 499: print(len(RE_txt_clean))\n",
    "assert len(RE_txt_clean) == len(mul_txt_clean) == len(bleu4_txt_clean) == len(F1_txt_clean)\n",
    "print(len(RE_txt_clean))\n",
    "print(drop)\n",
    "print(np.sum(list(drop.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1366, 5: 1343, 4: 874, 3: 493})\n",
      "mean RE:  0.9400107298311824\n",
      "mean bleu4:  0.4834490687569886\n",
      "mean mul:  0.4562010155711136\n"
     ]
    }
   ],
   "source": [
    "#  Normalization\n",
    "print(Counter([len(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"mean RE: \", np.mean([np.mean(RE_txt_clean[k]) for k in RE_txt_clean]))\n",
    "print(\"mean bleu4: \", np.mean([np.mean(bleu4_txt_clean[k]) for k in bleu4_txt_clean]))\n",
    "print(\"mean mul: \", np.mean([np.mean(mul_txt_clean[k]) for k in mul_txt_clean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "999\n",
      "1499\n",
      "1999\n",
      "2499\n",
      "2999\n",
      "3499\n",
      "3999\n"
     ]
    }
   ],
   "source": [
    "# Cache normalization, txt data\n",
    "txt_dataset_0825_bartscorePB_te = copy.deepcopy(txt_dataset)\n",
    "count = 0\n",
    "for k in txt_dataset_0825_bartscorePB_te:\n",
    "    if not txt_dataset_0825_bartscorePB_te[k]['split'] == 'test': continue\n",
    "    a_list = [a.replace('\"', \"\") for a in txt_dataset_0825_bartscorePB_te[k]['A']]\n",
    "    scores = compute_bartscore_ParaBank(a_list, a_list)\n",
    "    txt_dataset_0825_bartscorePB_te[k]['bartscore_normalizer'] = json.dumps(list(scores))\n",
    "    count += 1\n",
    "    if count % 500 == 499: print(count)\n",
    "json.dump(txt_dataset_0825_bartscorePB_te, open(\"/home/yingshac/CYS/WebQnA/WebQnA_data_new/txt_dataset_0825_bartscorePB_te.json\", \"w\"),indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draft Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"bertscore\")\n",
    "def compute_bertscore(cands, a, model_type):\n",
    "    metric.add_batch(predictions = cands, references = [a]*len(cands))\n",
    "    score = metric.compute(model_type=model_type)\n",
    "    return np.mean(score['f1']), np.max(score['f1'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deno =  [0.3545 0.3307 0.3259 0.4168 0.35  ]\n",
      "[0.0041 0.0136 0.0089 0.0207 0.0121]\n",
      "[0.0108 0.014  0.0217 0.014  0.0091]\n"
     ]
    }
   ],
   "source": [
    "cands = [\"Meter\"]*5\n",
    "a = [\"Iambic foot is a sub part of a meter, the meter is the overall patterns in a verse, foot helps make that up.\",\n",
    "    \"Iambic foot can make up meter more than meter can make up iambic foot.\",\n",
    "    \"Meter makes up lambic foot more than lambic foot makes up meter.\",\n",
    "    \"An Iambic foot helps make up a Meter more than a meter helps make up an Iambic foot.\",\n",
    "    \"An iambic foot helps to make up a meter in poetry.\"]\n",
    "deno = compute_bartscore_ParaBank(a, a)\n",
    "print(\"deno = \", deno)\n",
    "print(compute_bartscore_ParaBank(a, cands)/deno)\n",
    "print(compute_bartscore_ParaBank(cands, a)/deno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deno =  [0.5157 0.5782 0.5817 0.4787 0.5187]\n",
      "[0.0038 0.0033 0.0033 0.0026 0.0039]\n",
      "[0.018  0.0142 0.022  0.0119 0.0246]\n"
     ]
    }
   ],
   "source": [
    "cands = [\"They are native to lakes .\"]*5\n",
    "a = [\"The Janaria and Labidochirus splendescens are both native to the Pacific Ocean\",\n",
    "    \"Janaria and Labidochirus splendescens are both native to the Pacific Ocean.\",\n",
    "    \"Janaria and Labidochirus splendescens are native to the Pacific Ocean.\",\n",
    "    \"The Janaria and Labidochirus splendescens native to the Pacific Ocean\",\n",
    "    \"The Janaria and Labidochirus splendescens are native to the Pacific Ocean.\"]\n",
    "deno = compute_bartscore_ParaBank(a, a)\n",
    "print(\"deno = \", deno)\n",
    "print(compute_bartscore_ParaBank(a, cands)/deno)\n",
    "print(compute_bartscore_ParaBank(cands, a)/deno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BARTScore",
   "language": "python",
   "name": "bartscore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
